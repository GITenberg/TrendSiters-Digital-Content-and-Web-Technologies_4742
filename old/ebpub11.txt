The Project Gutenberg Etext of E-Books and E-Publishing (TrendSiters
Digital Content And Web Technologies) by Sam Vaknin
#4 in our series by Sam Vaknin

** This is a COPYRIGHTED Project Gutenberg Etext, Details Below **
**     Please follow the copyright guidelines in this file.     **


Copyright (C) 2000 Copyright Lidija Rangelovska.

We encourage you to keep this file, exactly as it is, on your
own disk, thereby keeping an electronic path open for future
readers.  Please do not remove this header information.

This header should be the first thing seen when anyone starts to
view the etext. Do not change or edit it without written permission.
The words are carefully chosen to provide users with the
information they need to understand what they may and may not
do with the etext.


**Welcome To The World of Free Plain Vanilla Electronic Texts**

**Etexts Readable By Both Humans and By Computers, Since 1971**

*****These Etexts Are Prepared By Thousands of Volunteers!*****

Information on contacting Project Gutenberg to get etexts, and
further information, is included below.  We need your donations.

The Project Gutenberg Literary Archive Foundation is a 501(c)(3)
organization with EIN [Employee Identification Number] 64-6221541



Title: E-books and e-publishing

Author: Sam Vaknin

Release Date: December, 2003 [Etext #4742]
[This file was first posted on September 15, 2002]

Edition: 11

Language: English

Character set encoding: ASCII

The Project Gutenberg Etext of E-books and e-publishing by Sam Vaknin
*******This file should be named ebpub11.txt or ebpub11.zip******

Corrected EDITIONS of our etexts get a new NUMBER, ebpub12.txt

We are now trying to release all our etexts one year in advance
of the official release dates, leaving time for better editing.
Please be encouraged to tell us about any error or corrections,
even years after the official publication date.

Please note neither this listing nor its contents are final til
midnight of the last day of the month of any such announcement.
The official release date of all Project Gutenberg Etexts is at
Midnight, Central Time, of the last day of the stated month.  A
preliminary version may often be posted for suggestion, comment
and editing by those who wish to do so.

Most people start at our sites at:
http://gutenberg.net or
http://promo.net/pg

These Web sites include award-winning information about Project
Gutenberg, including how to donate, how to help produce our new
etexts, and how to subscribe to our email newsletter (free!).


Those of you who want to download any Etext before announcement
can get to them as follows, and just download by date.  This is
also a good way to get them instantly upon announcement, as the
indexes our cataloguers produce obviously take a while after an
announcement goes out in the Project Gutenberg Newsletter.

http://www.ibiblio.org/gutenberg/etext03 or
ftp://ftp.ibiblio.org/pub/docs/books/gutenberg/etext03

Or /etext02, 01, 00, 99, 98, 97, 96, 95, 94, 93, 92, 92, 91 or 90

Just search by the first five letters of the filename you want,
as it appears in our Newsletters.


Information about Project Gutenberg (one page)

We produce about two million dollars for each hour we work.  The
time it takes us, a rather conservative estimate, is fifty hours
to get any etext selected, entered, proofread, edited, copyright
searched and analyzed, the copyright letters written, etc.   Our
projected audience is one hundred million readers.  If the value
per text is nominally estimated at one dollar then we produce $2
million dollars per hour in 2001 as we release over 50 new Etext
files per month, or 500 more Etexts in 2000 for a total of 4000+
If they reach just 1-2% of the world's population then the total
should reach over 300 billion Etexts given away by year's end.

The Goal of Project Gutenberg is to Give Away One Trillion Etext
Files by December 31, 2001.  [10,000 x 100,000,000 = 1 Trillion]
This is ten thousand titles each to one hundred million readers,
which is only about 4% of the present number of computer users.

At our revised rates of production, we will reach only one-third
of that goal by the end of 2001, or about 4,000 Etexts.  We need
funding, as well as continued efforts by volunteers, to maintain
or increase our production and reach our goals.

The Project Gutenberg Literary Archive Foundation has been created
to secure a future for Project Gutenberg into the next millennium.

We need your donations more than ever!

As of January, 2002, contributions are being solicited from people
and organizations in: Alabama, Alaska, Arkansas, Connecticut, Delaware,
Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky,
Louisiana, Maine, Michigan, Missouri, Montana, Nebraska, Nevada, New
Jersey, New Mexico, New York, North Carolina, Oklahoma, Oregon,
Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee,
Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin,
and Wyoming.

As the requirements for other states are met, additions to this list
will be made and fund raising will begin in the additional states.
Please feel free to ask to check the status of your state.

In answer to various questions we have received on this:

We are constantly working on finishing the paperwork to legally
request donations in all 50 states.  If your state is not listed and
you would like to know if we have added it since the list you have,
just ask.

While we cannot solicit donations from people in states where we are
not yet registered, we know of no prohibition against accepting
donations from donors in these states who approach us with an offer to
donate.

International donations are accepted!  For more information
about donations, please view http://promo.net/pg/donation.html
We accept PayPal, as well as donation s via NetworkForGood.

Donation checks should be sent to:

Project Gutenberg Literary Archive Foundation
PMB 113
1739 University Ave.
Oxford, MS 38655-4109


The Project Gutenberg Literary Archive Foundation has been approved by
the US Internal Revenue Service as a 501(c)(3) organization with EIN
[Employee Identification Number] 64-622154.  Donations are
tax-deductible to the maximum extent permitted by law.  As fundraising
requirements for other states are met, additions to this list will be
made and fundraising will begin in the additional states.

We need your donations more than ever!

***

If you can't reach Project Gutenberg,
you can always email directly to:

Michael S. Hart <hart@pobox.com>

Prof. Hart will answer or forward your message.

We would prefer to send you information by email.


**Information prepared by the Project Gutenberg legal advisor**
(Three Pages)

***START** SMALL PRINT! for COPYRIGHT PROTECTED ETEXTS ***

TITLE AND COPYRIGHT NOTICE:

E-books and e-publishing, by Sam Vaknin
Copyright (C) 2000 Copyright Lidija Rangelovska.

This etext is distributed by Professor Michael S. Hart through the
Project Gutenberg Association (the "Project") under the "Project
Gutenberg" trademark and with the permission of the etext's
copyright owner.

Please do not use the "PROJECT GUTENBERG" trademark to market
any commercial products without permission.


LICENSE
You can (and are encouraged!) to copy and distribute this
Project Gutenberg-tm etext.  Since, unlike many other of the
Project's etexts, it is copyright protected, and since the
materials and methods you use will effect the Project's reputation,
your right to copy and distribute it is limited by the copyright
laws and by the conditions of this "Small Print!" statement.

  [A]  ALL COPIES: You may distribute copies of this etext
electronically or on any machine readable medium now known
or hereafter discovered so long as you:

     (1)  Honor the refund and replacement provisions of this
"Small Print!" statement; and

     (2)  Pay a royalty to the Foundation of 20% of the gross
profits you derive calculated using the method you already use
to calculate your applicable taxes.  If you don't derive
profits, no royalty is due.  Royalties are payable to "Project
Gutenberg Literary Archive Foundation" within the 60 days
following each date you prepare (or were legally required
to prepare) your annual (or equivalent periodic) tax return.

  [B]  EXACT AND MODIFIED COPIES: The copies you distribute
must either be exact copies of this etext, including this
Small Print statement, or can be in binary, compressed, mark-
up, or proprietary form (including any form resulting from
word processing or hypertext software), so long as *EITHER*:

     (1)  The etext, when displayed, is clearly readable, and
does *not* contain characters other than those intended by the
author of the work, although tilde (~), asterisk (*) and
underline (_) characters may be used to convey punctuation
intended by the author, and additional characters may be used
to indicate hypertext links; OR

     (2)  The etext is readily convertible by the reader at no
expense into plain ASCII, EBCDIC or equivalent form by the
program that displays the etext (as is the case, for instance,
with most word processors); OR

     (3)  You provide or agree to provide on request at no
additional cost, fee or expense, a copy of the etext in plain
ASCII.

LIMITED WARRANTY; DISCLAIMER OF DAMAGES
This etext may contain a "Defect" in the form of incomplete,
inaccurate or corrupt data, transcription errors, a copyright
or other infringement, a defective or damaged disk, computer
virus, or codes that damage or cannot be read by your
equipment.  But for the "Right of Replacement or Refund"
described below, the Project (and any other party you may
receive this etext from as a PROJECT GUTENBERG-tm etext)
disclaims all liability to you for damages, costs and
expenses, including legal fees, and YOU HAVE NO REMEDIES FOR
NEGLIGENCE OR UNDER STRICT LIABILITY, OR FOR BREACH OF
WARRANTY OR CONTRACT, INCLUDING BUT NOT LIMITED TO INDIRECT,
CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES, EVEN IF YOU
GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES.

If you discover a Defect in this etext within 90 days of
receiving it, you can receive a refund of the money (if any)
you paid for it by sending an explanatory note within that
time to the person you received it from.  If you received it
on a physical medium, you must return it with your note, and
such person may choose to alternatively give you a replacement
copy.  If you received it electronically, such person may
choose to alternatively give you a second opportunity to
receive it electronically.

THIS ETEXT IS OTHERWISE PROVIDED TO YOU "AS-IS".  NO OTHER
WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, ARE MADE TO YOU AS
TO THE ETEXT OR ANY MEDIUM IT MAY BE ON, INCLUDING BUT NOT
LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A
PARTICULAR PURPOSE.  Some states do not allow disclaimers of
implied warranties or the exclusion or limitation of
consequential damages, so the above disclaimers and exclusions
may not apply to you, and you may have other legal rights.

INDEMNITY
You will indemnify and hold Michael Hart and the Foundation,
and its trustees and agents, and any volunteers associated
with the production and distribution of Project Gutenberg-tm
texts harmless, from all liability, cost and expense, including
legal fees, that arise directly or indirectly from any of the
following that you do or cause:  [1] distribution of this etext,
[2] alteration, modification, or addition to the etext,
or [3] any Defect.

WHAT IF YOU *WANT* TO SEND MONEY EVEN IF YOU DON'T HAVE TO?
Project Gutenberg is dedicated to increasing the number of
public domain and licensed works that can be freely distributed
in machine readable form.

The Project gratefully accepts contributions of money, time,
public domain materials, or royalty free copyright licenses.
Money should be paid to the:
"Project Gutenberg Literary Archive Foundation."

If you are interested in contributing scanning equipment or
software or other items, please contact Michael Hart at:
hart@pobox.com


*SMALL PRINT! Ver.12.12.00 FOR COPYRIGHT PROTECTED ETEXTS*END*




TrendSiters
Digital Content
And Web Technologies


1st EDITION




Sam Vaknin, Ph.D.




Editing and Design:
Lidija Rangelovska




Lidija Rangelovska
A Narcissus Publications Imprint, Skopje 2002

Not for Sale! Non-commercial edition. 












(C) 2002 Copyright Lidija Rangelovska.
All rights reserved. This book, or any part thereof, may not 
be used or reproduced in any manner without written permission 
from:
Lidija Rangelovska  - write to:
palma@unet.com.mk or to
vaknin@link.com.mk


Visit the TrendSiters Web Site:
http://samvak.tripod.com/busiweb.html



ISBN: 9989-929-23-8


Created by:LIDIJA RANGELOVSKA
REPUBLIC OF MACEDONIA


Additional articles about Digital Content on the Web:
 
http://samvak.tripod.com/busiweb.html
 
Sam Vaknin's eBookWeb.org articles:
 
http://ebookweb.org.master.com/texis/master/search/?q=Vaknin
 
Sam Vaknin's "InternetContent" Author Archive:
 
http://www.internetcontent.net/AuthorProfile.asp?AuthorID=14
 
Essays dedicated to the new media, doing business on the web, 
digital content, its creation and distribution, e-publishing, 
e-books, digital reference, DRM technology, and other related 
issues.

http://samvak.tripod.com/internet.html

Visit Sam Vaknin's United Press International (UPI) Article 
Archive - Click HERE!
 
This letter constitutes a permission to reprint or mirror any 
and all of the materials mentioned or linked to herein subject 
to appropriate credit and linkback.
 
Every article published MUST include the author bio, including 
the link to the author's web site.
 
 
AUTHOR BIO:
Sam Vaknin is the author of Malignant Self Love - Narcissism 
Revisited and After the Rain - How the West Lost the East. He 
is a columnist for Central Europe Review and eBookWeb , a 
United Press International (UPI) Senior Business 
Correspondent, and the editor of mental health and Central 
East Europe categories in The Open Directory and Suite101 . 
Until recently, he served as the Economic Advisor to the 
Government of Macedonia. 
Visit Sam's Web site at http://samvak.tripod.com



The Articles (please scroll down to review them):
 
E-books and e-publishing

The Future of Electronic Publishing
 
I. The Disintermediation of Content

II. E(merging) Books 

III. Invasion of the Amazons 

IV. Revolt of the Scholars

V. The Kidnapping of Content

VI. The Miraculous Conversion

VII. The Medium and the Message

VIII. The Idea of Reference

IX. Will Content ever be Profitable?

X. Jamaican OverDrive - LDC's and LCD's

XI. An Embarrassment of Riches

XII. The Fall and Fall of p-Zines

XIII. The Internet and the Library

XIV. A Brief History of the Book

XV. The Affair of the Vanishing Content

XVI. Revolt of the Poor - The Demise of Intellectual Property

XVII. The Territorial Web

XVIII. The In-credible Web

XIX. Does Free Content Sell?

XX. Copyright and Free Online Scholarship

XXI. The Second Gutenberg

XXII. The E-book Evangelist



 
Web Technology and Trends
 
I. Bright Planet, Deep Web

II. The Seamless Internet

III. The Polyglottal Internet

IV. Deja Googled

V. Maps of Cyberspace

VI. The Universal Interface

VII. Internet Advertising - What Went Wrong?

VIII. The Economics of Spam

IX. Don't Blink - Interview with Jeffrey Harrow

X. The Case of the Compressed Image


The Internet and the Digital Divide

I. The Internet - A Medium or a Message?

II. The Internet in the Countries in Transition

III. Leapfrogging Transition

IV. The Selfish Net - The Semantic Web
 
Author: Sam Vaknin 
 
Contact Info: palma@unet.com.mk; vaknin@link.com.mk



 
E-BOOKS AND E-PUBLISHING

The Future of Electronic Publishing
First published by United Press International (UPI)
By: Sam Vaknin

UNESCO's somewhat arbitrary definition of "book" is: 
 
""Non-periodical printed publication of at least 49 pages 
excluding covers". 
 
The emergence of electronic publishing was supposed to change 
all that. Yet a bloodbath of unusual proportions has taken 
place in the last few months. Time Warner's iPublish 
and MightyWords (partly owned by Barnes and Noble) were the 
last in a string of resounding failures which cast in doubt 
the business model underlying digital content. Everything 
seemed to have gone wrong: the dot.coms dot bombed, venture 
capital dried up, competing standards fractured an already 
fragile marketplace, the hardware (e-book readers) was clunky 
and awkward, the software unwieldy, the e-books badly written 
or already in the public domain. 
 
Terrified by the inexorable process of disintermediation (the 
establishment of direct contact between author and readers, 
excluding publishers and bookstores) and by the ease with 
which digital content can be replicated - publishers resorted 
to draconian copyright protection measures (euphemistically 
known as "digital rights management"). This further alienated 
the few potential readers left. The opposite model of "viral" 
or "buzz" marketing (by encouraging the dissemination of free 
copies of the promoted book) was only marginally more 
successful. 
 
Moreover, e-publishing's delivery platform, the Internet, has 
been transformed beyond recognition since March 2000. 
 
From an open, somewhat anarchic, web of networked computers - 
it has evolved into a territorial, commercial, corporate 
extension of "brick and mortar" giants, subject to government 
regulation. It is less friendly towards independent (small) 
publishers, the backbone of e-publishing. Increasingly, it is 
expropriated by publishing and media behemoths. It is treated 
as a medium for cross promotion, supply chain management, and 
customer relations management. It offers only some minor 
synergies with non-cyberspace, real world, franchises and 
media properties. The likes of Disney and Bertelsmann have 
swung a full circle from considering the Internet to be the 
next big thing in New Media delivery - to frantic efforts to 
contain the red ink it oozed all over their otherwise 
impeccable balance sheets.
 
But were the now silent pundits right all the same? Is the 
future of publishing (and other media industries) inextricably 
intertwined with the Internet?
 
The answer depends on whether an old habit dies hard. 
Internet surfers are used to free content. They are very 
reluctant to pay for information (with precious few 
exceptions, like the "Wall Street Journal"'s electronic 
edition). Moreover, the Internet, with 3 billion pages listed 
in the Google search engine (and another 15 billion in 
"invisible" databases), provides many free substitutes to 
every information product, no matter how superior. Web based 
media companies (such as Salon and Britannica.com) have been 
experimenting with payment and pricing models. But this is 
besides the point. Whether in the form of subscription 
(Britannica), pay per view (Questia), pay to print (Fathom), 
sample and pay to buy the physical product (RealRead), or 
micropayments (Amazon) - the public refuses to cough up. 
 
Moreover, the advertising-subsidized free content Web site has 
died together with Web advertising. Geocities - a community of 
free hosted, ad-supported, Web sites purchased by Yahoo! - is 
now selectively shutting down Web sites (when they exceed a 
certain level of traffic) to convince their owners to revert 
to a monthly hosting fee model. With Lycos in trouble in 
Europe, Tripod may well follow suit shortly. Earlier this 
year, Microsoft has shut down ListBot (a host of discussion 
lists). Suite101 has stopped paying its editors (content 
authors) effective January 15th. About.com fired hundreds of 
category editors. With the ugly demise of Themestream, WebSeed 
is the only content aggregator which tries to buck the trend 
by relying (partly) on advertising revenue.
 
Paradoxically, e-publishing's main hope may lie with its 
ostensible adversary: the library. Unbelievably, e-publishers 
actually tried to limit the access of library patrons to e-
books (i.e., the lending of e-books to multiple patrons). But, 
libraries are not only repositories of knowledge and community 
centres. They are also dominant promoters of new knowledge 
technologies. They are already the largest buyers of e-books. 
Together with schools and other educational institutions, 
libraries can serve as decisive socialization agents and 
introduce generations of pupils, students, and readers to the 
possibilities and riches of e-publishing. Government use of e-
books (e.g., by the military) may have the same beneficial 
effect.
 
As standards converge (Adobe's Portable Document Format and 
Microsoft's MS Reader LIT format are likely to be the 
winners), as hardware improves and becomes ubiquitous (within 
multi-purpose devices or as standalone higher quality units), 
as content becomes more attractive (already many new titles 
are published in both print and electronic formats), as more 
versatile information taxonomies (like the Digital Object 
Identifier) are introduced, as the Internet becomes more 
gender-neutral, polyglot, and cosmopolitan - e-publishing is 
likely to recover and flourish. 
 
This renaissance will probably be aided by the gradual decline 
of print magazines and by a strengthening movement for free 
open source scholarly publishing. The publishing of periodical 
content and academic research (including, gradually, peer 
reviewed research) may be already shifting to the Web. Non-
fiction and textbooks will follow. Alternative models of 
pricing are already in evidence (author pays to publish, 
author pays to obtain peer review, publisher pays to publish, 
buy a physical product and gain access to enhanced online 
content, and so on). Web site rating agencies will help to 
discriminate between the credible and the in-credible. 
Publishing is moving - albeit kicking and screaming - online.





The Disintermediation of Content
By: Sam Vaknin
 
Are content brokers - publishers, distributors, and record 
companies - a thing of the past?
 
In one word: disintermediation
 
The gradual removal of layers of content brokering and 
intermediation - mainly in manufacturing marketing - is the 
continuation of a long term trend. Consider music for 
instance. Streaming audio on the internet ("soft radio"), or 
downloadable MP3 files may render the CD obsolete - but they 
were preceded by radio music broadcasts. But the novelty is 
that the Internet provides a venue for the marketing of niche 
products and reduces the barriers to entry previously imposed 
by the need to invest in costly "branding" campaigns and 
manufacturing and distribution activities.
 
This trend is also likely to restore the balance between 
artists and the commercial exploiters of their products. The 
very definition of "artist" will expand to encompass all 
creative people. One will seek to distinguish oneself, to 
"brand" oneself and to auction one's services, ideas, 
products, designs, experience, physique, or biography, etc. 
directly to end-users and consumers. This is a return to pre-
industrial times when artisans ruled the economic scene. Work 
stability will suffer and work mobility will increase in a 
landscape of shifting allegiances, head hunting, remote 
collaboration, and similar labour market trends.
 
But distributors, publishers, and record companies are not 
going to vanish. They are going to metamorphose. This is 
because they fulfil a few functions and provide a few services 
whose importance is only enhanced by the "free for all" 
Internet culture.
 
Content intermediaries grade content and separate the 
qualitative from the ephemeral and the atrocious. The deluge 
of self-published and vanity published e-books, music tracks 
and art works has generated few masterpieces and a lot of 
trash. The absence of judicious filtering has unjustly given a 
bad name to whole segments of the industry (e.g., small, or 
web-based publishers). Consumers - inundated, disappointed and 
exhausted - will pay a premium for content rating services. 
Though driven by crass commercial considerations, most 
publishers and record companies do apply certain quality 
standards routinely and thus are positioned to provide these 
rating services reliably.
 
Content brokers are relationship managers. Consider 
distributors: they provide instant access to centralized, 
continuously updated, "addressbooks" of clients (stores, 
consumers, media, etc.). This reduces the time to market and 
increases efficiency. It alters revenue models very 
substantially. Content creators can thus concentrate on what 
they do best: content creation, and reduce their overhead by 
outsourcing the functions of distribution and relationships 
management. The existence of central "relationship ledgers" 
yields synergies which can be applied to all the clients of 
the distributor. The distributor provides a single address 
that content re-sellers converge on and feed off. 
Distributors, publishers and record companies also provide 
logistical support: warehousing, consolidated sales reporting 
and transaction auditing, and a single, periodic payment.
 
Yet, having said all that, content intermediaries still over-
charge their clients (the content creators) for their 
services. This is especially true in an age of just-in-time 
inventory and digital distribution. Network effects mean that 
content brokers have to invest much less in marketing, 
branding and advertising once a product's first mover 
advantage is established. Economic laws of increasing, rather 
than diminishing, returns mean that every additional unit sold 
yields a HIGHER profit - rather than a declining one. The pie 
is getting bigger.
 
Hence, the meteoric increase in royalties publishers pay 
authors from sales of the electronic versions of their work 
(anywhere from Random House's 35% to 50% paid by smaller 
publishers). As this tectonic shift reverberates through the 
whole distribution chain, retail outlets are beginning to 
transact directly with content creators. The borders between 
the types of intermediaries are blurred. Barnes and Noble (the 
American bookstores chain) has, in effect, become a publisher. 
Many publishers have virtual storefronts. Many authors sell 
directly to their readers, acting as publishers. The 
introduction of "book ATMs" - POD (Print On Demand) machines, 
which will print  
every conceivable title in minutes, on the spot, in "book 
kiosks" - will give rise to a host of new intermediaries. 
Intermediation is not gone. It is here to stay because it is 
sorely needed. But it is in a state of flux. Old maxims break 
down. New modes of operation emerge. 
 
Functions are amalgamated, outsourced, dispensed with, or 
created from scratch. It is an exciting scene, full with 
opportunities.
 



 
 
E(merging) Books
By: Sam Vaknin
 
A novel re-definition through experimentation of the classical 
format of the book is emerging. 
Consider the now defunct BookTailor. It used to sell its book 
customization software mainly to travel agents - but such 
software is likely to conquer other niches (such as the legal 
and medical professions). It allows users to select bits and 
pieces from a library of e-books, combine them into a totally 
new tome and print and bind the latter on demand. The client 
can also choose to buy the end-product as an e-book. Consider 
what this simple business model does to entrenched and age old 
notions such as "original"  and "copies", copyright, and book 
identifiers. What is the "original" in this case? Is it the 
final, user-customized book - or its sources? And if no 
customized book is identical to any other - what happens to 
the intuitive notion of "copies"? Should BookTailor-generated 
books considered to be unique exemplars of one-copy print 
runs? If so, should each one receive a unique identifier (for 
instance, a unique ISBN)? Does the user possess any rights in 
the final product, composed and selected by him? What about 
the copyrights of the original authors? 
Or take BookCrossing.com. On the face of it, it presents no 
profound challenge to established publishing practices and to 
the modern concept of intellectual property. Members register 
their books, obtain a BCID (BookCrossing ID Number) and then 
give the book to someone, or simply leave it lying around for 
a total stranger to find. Henceforth, fate determines the 
chain of events. Eventual successive owners of the volume are 
supposed to report to BookCrossing (by e-mail) about the 
book's and their whereabouts, thereby generating moving plots 
and mapping the territory of literacy and bibliomania. This 
innocuous model subversively undermines the concept - legal 
and moral - of ownership. It also expropriates the book from 
the realm of passive, inert objects and transforms it into a 
catalyst of human interactions across time and space. In other 
words, it returns the book to its origins: a time capsule, a 
time machine and the embodiment of a historical narrative. 
E-books, hitherto, have largely been nothing but an ephemeral 
rendition of their print predecessors. But e-books are another 
medium altogether. They can and will provide a different 
reading experience.  Consider "hyperlinks within the e-book 
and without it - to web content, reference works, etc., 
embedded instant shopping and ordering links, divergent, user-
interactive, decision driven plotlines, interaction with other 
e-books (using Bluetooth or another wireless standard), 
collaborative authoring, gaming and community activities, 
automatically or periodically updated content, ,multimedia 
capabilities, database, Favourites and History Maintenance 
(records of reading habits, shopping habits, interaction with 
other readers, plot related decisions and much more), 
automatic and embedded audio conversion and translation 
capabilities, full wireless piconetworking and 
scatternetworking capabilities and more". 




INVASION OF THE AMAZONS
By: Sam Vaknin
 
The last few months have witnessed a bloodbath in tech stocks 
coupled with a frantic re-definition of the web and of every 
player in it (as far as content is concerned). 
 
This effort is three pronged:
 
Some companies are gambling on content distribution and the 
possession of the attendant digital infrastructure. 
MightyWords, for example, stealthily transformed itself from a 
"free-for-all-everyone-welcome" e-publisher to a distribution 
channel of choice works (mainly by midlist authors). It now 
aims to feed its content to content-starved web sites. In the 
process, it shed thousands of unfortunate authors who did not 
meet its (never stated) sales criteria. 
 
Others bet the farm on content creation and packaging. Bn.com 
invaded the digital publishing and POD (Print on Demand) 
businesses in a series of lightning purchases. It is now the 
largest e-book store by a wide margin.
 
But Amazon seemed to have got it right once more. The web's 
own virtual mall and the former darling of Wall Street has 
diversified into micropayments.
 
The Internet started as a free medium for free spirits. E-
commerce was once considered a dirty word. Web surfers became 
used to free content. Hence the (very low) glass ceiling on 
the price of content made available through the web - and the 
need to charge customers less than 1 US dollars to a few 
dollars per transaction ("micro-payments"). Various service 
providers (such as Pay-Pal) emerged, none became sufficiently 
dominant and all-pervasive to constitute a standard. Web 
merchants' ability to accept micropayments is crucial. E-
commerce (let alone m-commerce) will never take off without 
it.
 
Enter Amazon. Its "Honour System" is licenced to third party 
web sites (such as Bartleby.com and SatireWire). It allows 
people to donate money or effect micro-payments, apparently 
through its patented one-click system. As far as the web sites 
are concerned, there are two major drawbacks: all donations 
and payments are refundable within 30 days and Amazon charges 
them 15 cents per transaction plus 15(!) percent. By far the 
worst deal in town.
 
So, why the fuss?
 
Because of Amazon's customer list. This development emphasizes 
the growing realization that one's list of customers - 
properly data mined - is the greatest asset, greater even than 
original content and more important than distribution channels 
and digital right management or asset management applications. 
Merchants are willing to pay for access to this ever expanding 
virtual neighbourhood (even if they are not made privy to 
the customer information collected by Amazon). 



 
The Honour System looks suspiciously similar to the payment 
system designed by Amazon for Stephen King's serialized e-
novel, "The Plant". Interesting to note how the needs of 
authors and publishers are now in the driver's seat, helping 
to spur along innovations in business methods. 



 
 
 
Revolt of the Scholars
By: Sam Vaknin
 
http://www.realsci.com/
 
Scindex's Instant Publishing Service is about empowerment. The 
price of scholarly, peer-reviewed journals has skyrocketed in 
the last few years, often way out of the limited means of 
libraries, universities, individual scientists and scholars. A 
"scholarly divide" has opened between the haves (academic 
institutions with rich endowments and well-heeled 
corporations) and the haves not (all the others). 
Paradoxically, access to authoritative and authenticated 
knowledge has declined as the number of professional journals 
has proliferated. This is not to mention the long (and often 
crucial) delays in publishing research results and the shoddy 
work of many under-paid and over-worked peer reviewers.
 
The Internet was suppose to change all that. Originally, a 
computer network for the exchange of (restricted and open) 
research results among scientists and academics in 
participating institutions - it was supposed to provide 
instant publishing, instant access and instant gratification. 
It has delivered only partially. Preprints of academic papers 
are often placed online by their eager authors and subjected 
to peer scrutiny. But this haphazard publishing cottage 
industry did nothing to dethrone the print incumbents and 
their avaricious pricing. 
 
The major missing element is, of course, respectability. But 
there are others. No agreed upon content or knowledge 
classification method has emerged. Some web sites (such as 
Suite101) use the Dewey decimal system. Others invented and 
implemented systems of their making. Additionally, one click 
publishing technology (such as Webseed's or Blogger's) came to 
be identified strictly to non-scholarly material: personal 
reminiscences, correspondence, articles and news.
 
Enter Scindex and its Academic Resource Channel. Established 
by academics and software experts from Bulgaria, it epitomizes 
the tearing down of geographical barriers heralded by the 
Internet. But it does much more than that. Scindex is a whole, 
self-contained, stand-alone, instant self-publishing and self-
assembly system. Self-publishing systems do exist (for 
instance, Purdue University's) - but they incorporate only 
certain components. Scindex covers the whole range.
 
Having (freely) registered as a member, a scientist or a 
scholar can publish their papers, essays, research results, 
articles and comments online. They have to submit an abstract 
and use Sciendex's classification ("call") numbers and science 
descriptors, arranged in a massive directory available in the 
"RealSci Locator". The Locator can be also downloaded and used 
off-line and its is surprisingly user-friendly. The submission 
process itself is totally automated and very short.
 
The system includes a long series of thematic journals. These 
journals self-assemble, in accordance with the call numbers 
selected by the submitters. An article submitted with certain 
call numbers will automatically be included in the relevant 
journals. 
 
The fly in the ointment is the absence of peer review. As the 
system moves from beta to commercialization, Scindex intends 
to address this issue by introducing a system of incentives 
and inducements. Reviewers will be granted "credit points" to 
be applied against the (paid) publication of their own papers, 
for instance. 
 
Scindex is the model of things to come. Publishing becomes 
more and more automated and knowledge-orientated. Peer 
reviewed papers become more outlandishly expensive and 
irrelevant. Scientists and scholars are getting impatient and 
rebellious. The confluence of these three trends spells - at 
the least - the creation of a web based universe of 
parallel and alternative scholarly publishing. 



 
 
 
The Kidnapping of Content
By: Sam Vaknin
 
http://www.plagiarism.org and http://www.Turnitin.com
 
Latin kidnapped the word "plagion" from ancient Greek and it 
ended up in English as "plagiarism". It literally means "to 
kidnap" - most commonly, to misappropriate content and wrongly 
attribute it to oneself. It is a close kin of piracy. But 
while the software or content pirate does not bother to hide 
or alter the identity of the content's creator or the 
software's author - the plagiarist does. Plagiarism is, 
therefore, more pernicious than piracy.
 
Enter Turnit.com. An off-shoot of  www.iparadigms.com, it was 
established by a group of concerned (and commercially minded) 
scientists from UC Berkeley. 
 
Whereas digital rights and asset management systems are geared 
to prevent piracy - plagiarism.org and its commercial arm, 
Turnit.com, are the cyber equivalent of a law enforcement 
agency, acting after the fact to discover the culprits and 
uncover their misdeeds. This, they claim, is a first stage on 
the way to a plagiarism-free Internet-based academic community 
of both teachers and students, in which the educational 
potential of the Internet can be fully realized.
 
The problem is especially severe in academia. Various surveys 
have discovered that a staggering 80%(!) of US students cheat 
and that at least 30% plagiarize written material. The 
Internet only exacerbated this problem. More than 200 cheat-
sites have sprung up, with thousands of papers available on-
line and tens of thousands of satisfied plagiarists the world 
over. Some of these hubs - like cheater.com, cheatweb or 
cheathouse.com - make no bones about their offerings. Many of 
them are located outside the USA (in Germany, or Asia) and at 
least one offers papers in a few languages, Hebrew included.
 
The problem, though, is not limited to the ivory towers. E-
zines plagiarize. The print media plagiarize. Individual 
journalists plagiarize, many with abandon. Even advertising 
agencies and financial institutions plagiarize. The amount of 
material out there is so overwhelming that the plagiarist 
develops a (fairly justified) sense of immunity. The 
temptation is irresistible, the rewards big and the pressures 
of modern life great.
 
Some of the plagiarists are straightforward copiers. Others 
substitute words, add sentences, or combine two or more 
sources. This raises the question: "when should content be 
considered original and when - plagiarized?". Should the test 
for plagiarism be more stringent than the one applied by the 
Copyright Office? And what rights are implicitly granted by 
the material's genuine authors or publishers once they place 
the content on the Internet? Is the Web a public domain and, 
if yes, to what extent? These questions are not easily 
answered. Consider reports generated by users from a database. 


Are these reports copyrighted - and if so, by whom - by the 
database compiler or by the user who defined the parameters, 
without which the reports in question would have never been 
generated? What about "fair use" of text and works of art? In 
the USA, the backlash against digital content piracy and 
plagiarism has reached preposterous legal, litigious and 
technological nadirs. 
 
Plagiarism.org has developed a statistics-based technology 
(the "Document Source Analysis") which creates a "digital 
fingerprint" of every document in its database. Web crawlers 
are then unleashed to scour the Internet and find documents 
with the same fingerprint and a colour-coded report is 
generated. An instructor, teacher, or professor can then use 
the report to prove plagiarism and cheating. 
 
Piracy is often considered to be a form of viral marketing 
(even by software developers and publishers). The author's, 
publisher's, or software house's data are preserved intact in 
the cracked copy. Pirated copies of e-books often contribute 
to increased sales of the print versions. Crippled versions of 
software or pirated copies of software without its manuals, 
updates and support - often lead to the purchase of a licence. 
Not so with plagiarism. The identities of the author, editor, 
publisher and illustrator are deleted and replaced by the 
details of the plagiarist. And while piracy is discussed 
freely and fought vigorously - the discussion of plagiarism is 
still taboo and actively suppressed by image-conscious and 
endowment-weary academic institutions and media. It is an 
uphill struggle but plagiarism.org has taken the first 
resolute step.
 



 
 
The Miraculous Conversion
By: Sam Vaknin
 
http://www.ideavirus.com
 
 
The recent bloodbath among online content peddlers and digital 
media proselytisers can be traced to two deadly sins. The 
first was to assume that traffic equals sales. In other words, 
that a miraculous conversion will spontaneously occur among 
the hordes of visitors to a web site. It was taken as an 
article of faith that a certain percentage of this mass will 
inevitably and nigh hypnotically reach for their bulging 
pocketbooks and purchase content, however packaged. Moreover, 
ad revenues (more reasonably) were assumed to be closely 
correlated with "eyeballs". This myth led to an obsession with 
counters, page hits, impressions, unique visitors, statistics 
and demographics. 
 
It failed, however, to take into account the dwindling 
efficacy of what Seth Godin, in his brilliant essay 
("Unleashing the IdeaVirus"), calls "Interruption Marketing" - 
ads, banners, spam and fliers. It also ignored, at its peril, 
the ethos of free content and open source prevalent among the 
Internet opinion leaders, movers and shapers. These two 
neglected aspects of Internet hype and culture led to the 
trouncing of erstwhile promising web media companies while 
their business models were exposed as wishful thinking. 
 
The second mistake was to exclusively cater to the needs of a 
highly idiosyncratic group of people (Silicone Valley geeks 
and nerds). The assumption that the USA (let alone the rest of 
the world) is Silicone Valley writ large proved to be 
calamitous to the industry. 
 
In the 1970s and 1980s, evolutionary biologists like Richard 
Dawkins and Rupert Sheldrake developed models of cultural 
evolution. Dawkins' "meme" is a cultural element (like a 
behaviour or an idea) passed from one individual to another 
and from one generation to another not through biological -
genetic means - but by imitation. Sheldrake added the notion 
of contagion - "morphic resonance" - which causes behaviour 
patterns to suddenly emerged in whole populations. Physicists 
talked about sudden "phase transitions", the emergent results 
of a critical mass reached. A latter day thinker, Michael 
Gladwell, called it the "tipping point".
 
Seth Godin invented the concept of an "ideavirus" and an 
attendant marketing terminology. In a nutshell, he says, to 
use his own summation: 
 
"Marketing by interrupting people isn't cost-effective 
anymore. You can't afford to seek out people and send them 
unwanted marketing, in large groups and hope that some will 
send you money. Instead the future belongs to marketers who 
establish a foundation and process where interested people can 
market to each other. Ignite consumer networks and then get 
out of the way and let them talk."



 
This is sound advice with a shaky conclusion. The conversion 
from exposure to a marketing message (even from peers within a 
consumer network) - to an actual sale is a convoluted, multi-
layered, highly complex process. It is not a "black box", 
better left unattended to. It is the same deadly sin all over 
again - the belief in a miraculous conversion. And it is 
highly US-centric. People in other parts of the world interact 
entirely differently.
 
You can get them to visit and you get them to talk and you can 
get them to excite others. But to get them to buy - is a whole 
different ballgame. Dot.coms had better begin to study its 
rules.



 
 
 
The Medium and the Message  
By: Sam Vaknin
 
 
A debate is raging in e-publishing circles: should content be 
encrypted and protected (the Barnes and Noble or Digital goods 
model) - or should it be distributed freely and thus serve as 
a form of viral marketing (Seth Godin's "ideavirus")? 
Publishers fear that freely distributed and cost-free 
"cracked" e-books will cannibalize print books to oblivion. 
 
The more paranoid point at the music industry. It failed to 
co-opt the emerging peer-to-peer platforms (Napster) and to 
offer a viable digital assets management system with an 
equitable sharing of royalties. The results? A protracted 
legal battle and piracy run amok. "Publishers" - goes this 
creed - "are positioned to incorporate encryption and 
protection measures at the very inception of the digital 
publishing industry. They ought to learn the lesson." 
 
But this view ignores a vital difference between sound and 
text. In music, what matter are the song or the musical piece. 
The medium (or carrier, or packing) is marginal and 
interchangeable. A CD, an audio cassette, or an MP3 player are 
all fine, as far as the consumer is concerned. The listener 
bases his or her purchasing decisions on sound quality and the 
faithfulness of reproduction of the listening experience (for 
instance, in a concert hall). This is a very narrow, rational, 
measurable and quantifiable criterion. 
 
Not so with text. 
 
Content is only one element of many of equal footing 
underlying the decision to purchase a specific text-"carrier" 
(medium). Various media encapsulating IDENTICAL text will 
still fare differently. Hence the failure of CD-ROMs and e-
learning. People tend to consume content in other formats or 
media, even if it is fully available to them or even owned by 
them in one specific medium. People prefer to pay to listen to 
live lectures rather than read freely available online 
transcripts. Libraries buy print journals even when they have 
subscribed to the full text online versions of the very same 
publications. And consumers overwhelmingly prefer to purchase 
books in print rather than their e-versions. 
 
This is partly a question of the slow demise of old habits. E-
books have yet to develop the user-friendliness, platform-
independence, portability, browsability and many other 
attributes of this ingenious medium, the Gutenberg tome. But 
it also has to do with marketing psychology.  Where text (or 
text equivalents, such as speech) is concerned, the medium is 
at least as important as the message. And this will hold true 
even when e-books catch up with their print brethren 
technologically. 



 
There is no doubting that finally e-books will surpass print 
books as a medium and offer numerous options:  hyperlinks 
within the e-book and without it - to web content, reference 
works, etc., embedded instant shopping and ordering links, 
divergent, user-interactive, decision driven plotlines, 
interaction with other e-books (using Bluetooth or another 
wireless standard), collaborative authoring, gaming and 
community activities, automatically or periodically updated 
content, ,multimedia capabilities, database, Favourites and 
History Maintenance (records of reading habits, shopping 
habits, interaction with other readers, plot related decisions 
and much more), automatic and embedded audio conversion and 
translation capabilities, full wireless piconetworking and 
scatternetworking capabilities and more. 
 
The same textual content will be available in the future in 
various media. Ostensibly, consumers should gravitate to the 
feature-rich and much cheaper e-book. But they won't - because 
the medium is as important as the text message. It is not 
enough to own the same content, or to gain access to the same 
message. Ownership of the right medium does count. Print books 
offer connectivity within an historical context (tradition). 
E-books are cold and impersonal, alienated and detached. The 
printed word offers permanence. Digital text is ephemeral (as 
anyone whose writings perished in the recent dot.com bloodbath 
or Deja takeover by Google can attest). Printed volumes are a 
whole sensorium, a sensual experience - olfactory and tactile 
and visual. E-books are one dimensional in comparison. These 
are differences that cannot be overcome, not even with the 
advent of digital "ink" on digital "paper". They will keep the 
print book alive and publishers' revenues flowing. 
 
People buy printed matter not merely because of its content. 
If this were true e-books will have won the day. Print books 
are a packaged experience, the substance of life. People buy 
the medium as often and as much as they buy the message it 
encapsulates. It is impossible to compete with this mistique. 
Safe in this knowledge, publishers should let go and impose on 
e-books "encryption" and "protection" levels as rigorous as 
they do on the their print books. The latter are here to stay 
alongside the former. With the proper pricing and a modicum of 
trust, e-books may even end up promoting the old and trusted 
print versions.



  
 
The Idea of Reference
By: Sam Vaknin
 
http://www.britannica.com
 
There is no source of reference remotely as authoritative as 
the Encyclopaedia Britannica. There is no brand as venerable 
and as veteran as this mammoth labour of knowledge and ideas 
established in 1768. There is no better value for money. And, 
after a few sputters and bugs, it now comes in all shapes and 
sizes, including two CD-ROM versions (standard and deluxe) and 
an appealing and reader-friendly web site. So, why does it 
always appear to be on the brink of extinction?
 
The Britannica provides for an interesting study of the 
changing fortunes (and formats) of vendors of reference. As 
late as a decade ago, it was still selling in a leather-
imitation bound set of 32 volumes. As print encyclopaedias 
went, it was a daring innovator and a pioneer of hyperlinked-
like textual design. It sported a subject index, a lexical 
part and an alphabetically arranged series of in-depth essays 
authored by the best in every field of human erudition. 
 
When the CD-ROM erupted on the scene, the Britannica 
mismanaged the transition. As late as 1997, it was still 
selling a sordid text-only compact disc which included a part 
of the encyclopaedia. Only in 1998, did the Britannica switch 
to multimedia and added tables and graphs to the CD. Video and 
sound were to make their appearance even later. This error in 
trend analysis left the field wide open to the likes of 
Encarta and Grolier. The Britannica failed to grasp the 
irreversible shift from cumbersome print volumes to slender 
and freely searchable CD-ROMs. Reference was going digital and 
the Britannica's sales plummeted.
 
The Britannica was also late to cash on the web revolution - 
but, when it did, it became a world leader overnight. Its 
unbeatable brand was a decisive factor. A failed experiment 
with an annoying subscription model gave way to unrestricted 
access to the full contents of the Encyclopaedia and much more 
besides: specially commissioned articles, fora, an annotated 
internet guide, news in context, downloads and shopping. The 
site enjoys healthy traffic and the Britannica's CD-ROM 
interacts synergistically with its contents (through 
hyperlinks).
 
Yet, recently, the Britannica had to fire hundreds of workers 
(in its web division) and a return to a pay-for-content model 
is contemplated. What went wrong again? Internet advertising 
did. The Britannica's revenue model was based on monetizing 
eyeballs, to use a faddish refrain. When the perpetuum mobile 
of "advertisers pay for content and users get it free" 
crumbled - the Britannica found itself in familiar dire 
straits.
 
Is there a lesson to be learned from this arduous and 
convoluted tale? Are works of reference not self-supporting 
regardless of the revenue model (subscription, ad-based, 
print, CD-ROM)? This might well be the case. 
 
Classic works of reference - from Diderot to the Encarta - 
offered a series of advantages to their users:
 
1. Authority - Works of reference are authored by experts in 
their fields and peer-reviewed. This ensures both objectivity 
and accuracy.
 
2. Accessibility - Huge amounts of material were assembled 
under one "roof". This abolished the need to scour numerous 
sources of variable quality to obtain the data one needed.
 
3. Organization - This pile of knowledge was organized in a 
convenient and recognizable manner (alphabetically or by 
subject)
 
Moreover, authoring an encyclopaedia was such a daunting and 
expensive task that only states, academic institutions, or 
well-funded businesses were able to produce them. At any given 
period there was a dearth of reliable encyclopaedias, which 
exercised a monopoly on the dissemination of knowledge. 
Competitors were few and far between. The price of these tomes 
was, therefore, always exorbitant but people paid it to secure 
education for their children and a fount of knowledge at home. 
Hence the long gone phenomenon of "door to door encyclopaedia 
salesmen" and instalment plans.
 
Yet, all these advantages were eroded to fine dust by the 
Internet. The web offers a plethora of highly authoritative 
information authored and released by the leading names in 
every field of human knowledge and endeavour. The Internet, 
is, in effect, an encyclopaedia - far more detailed, far more 
authoritative, and far more comprehensive that any 
encyclopaedia can ever hope to be. The web is also fully 
accessible and fully searchable. What it lacks in organization 
it compensates in breadth and depth and recently emergent 
subject portals (directories such as Yahoo! or The Open 
Directory) have become the indices of the Internet. The 
aforementioned anti-competition barriers to entry are gone: 
web publishing is cheap and immediate. Technologies such as 
web communities, chat, and e-mail enable  
massive collaborative efforts. And, most important, the bulk 
of the Internet is free. Users pay only the communication 
costs.
 
The long-heralded transition from free content to fee-based 
information may revive the fortunes of online reference 
vendors. But as long as the Internet - with its 2,000,000,000 
(!) visible pages (and 5 times as many pages in its databases) 
- is free, encyclopaedias have little by way of a competitive 
advantage.



 
 
 
Will Content Ever be Profitable
By: Sam Vaknin
 
THE CURRENT WORRIES 
1. Content Suppliers 
The Ethos of Free Content 
Content Suppliers is the underprivileged sector of the 
Internet. They all lose money (even sites which offer basic, 
standardized goods - books, CDs), with the exception of sites 
proffering sex or tourism. No user seems to be grateful for 
the effort and resources invested in creating and distributing 
content. The recent breakdown of traditional roles (between 
publisher and author, record company and singer, etc.) and the 
direct access the creative artist is gaining to its paying 
public may change this attitude of ingratitude but hitherto 
there are scarce signs of that. Moreover, it is either quality 
of presentation (which only a publisher can afford) or 
ownership and (often shoddy) dissemination of content by the 
author. A really qualitative, fully commerce enabled site 
costs up to 5,000,000 USD, excluding site maintenance and 
customer and visitor services. Despite these heavy outlays, 
site designers are constantly criticized for lack of 
creativity or for too much creativity. More and more is asked 
of content purveyors and creators. They are exploited by 
intermediaries, hitchhikers and other parasites. This is all 
an off-shoot of the ethos of the Internet as a free content 
area. 
Most of the users like to surf (browse, visit sites) the net 
without reason or goal in mind. This makes it difficult to 
apply to the web traditional marketing techniques. 
What is the meaning of "targeted audiences" or "market shares" 
in this context? If a surfer visits sites which deal with 
aberrant sex and nuclear physics in the same session - what to 
make of it? 
Moreover, the public and legislative backlash against the 
gathering of surfer's data by Internet ad agencies and other 
web sites - has led to growing ignorance regarding the profile 
of Internet users, their demography, habits, preferences and 
dislikes. 
"Free" is a key word on the Internet : it used to belong to 
the US Government and to a bunch of universities. Users like 
information, with emphasis on news and data about new 
products. But they do not like to shop on the net - yet. Only 
38% of all surfers made a purchase during 1998. 
It would seem that users will not pay for content unless it is 
unavailable elsewhere or qualitatively rare or made rare. One 
way to "rarefy" content is to review and rate it. 



2. Quality-rated Content 
There is a long term trend of clutter-breaking website-rating 
and critique. It may have a limited influence on the 
consumption decisions of some users and on their willingness 
to pay for content. Browsers already sport "What's New" and 
"What's Hot" buttons. Most Search Engines and directories 
recommend specific sites. But users are still cautious. 
Studies discovered that no user, no matter how heavy, has 
consistently re-visited more than 200 sites, a minuscule 
number. Some recommendation services often produce random - at 
times, wrong - selections for their users. There are also 
concerns regarding privacy issues. The backlash against 
Amazon's "readers circles" is an example. Web Critics, who 
work today mainly for the printed press, publish their wares 
on the net and collaborate with intelligent software which 
hyperlinks to web sites, recommends them and refers users to 
them. Some web critics (guides) became identified with 
specific applications - really, expert systems -which 
incorporate their knowledge and experience. Most volunteer-
based directories (such as the "Open Directory" and the late 
"Go" directory) work this way. 
The flip side of the coin of content consumption is investment 
in content creation, marketing, distribution and maintenance. 
3. The Money 
Where is the capital needed to finance content likely to come 
from? 
Again, there are two schools: 
According to the first, sites will be financed through 
advertising -  and so will search engines and other 
applications accessed by users. 
Certain ASPs (Application Service Providers which rent out 
access to application software which resides on their servers) 
are considering this model. 
The recent collapse in online advertising rates and click-
through rates raised serious doubts regarding the validity and 
viability of this model. Marketing gurus, such as Seth Godin 
went as far as declaring "interruption marketing" (=ads and 
banners) dead. 
The second approach is simpler and allows for the existence of 
non-commercial content. 
It proposes to collect negligible sums (cents or fractions of 
cents) from every user for every visit ("micro-payments"). 
These accumulated cents will enable the site-owners to update 
and to maintain them and encourage entrepreneurs to develop 
new content and invest in it. Certain content aggregators 
(especially of digital textbooks) have adopted this model 
(Questia, Fathom). 
The adherents of the first school point to the 5 million USD 
invested in advertising during 1995 and to the 60 million or 
so invested during 1996. 
Its opponents point exactly at the same numbers : ridiculously 
small when contrasted with more conventional advertising 
modes. The potential of advertising on the Net is limited to 
1.5 billion USD annually in 1998, thundered the pessimists. 
The actual figure was double the prediction but still woefully 
small and inadequate to support the internet's content 
development. Compare these figures to the sale of Internet 
software (4 billion), Internet hardware (3 billion), Internet 
access provision (4.2 billion in 1995 alone!).
Even if online advertising were to be restored to its 
erstwhile glory days, other bottlenecks remain. Advertising 
encourages the consumer to interact and to initiate the 
delivery of a product to him. This - the delivery phase - is a 
slow and enervating epilogue to the exciting affair of 
ordering online. Too many consumers still complain of late 
delivery of the wrong or defective products. 
The solution may lie in the integration of advertising and 
content. The late Pointcast, for instance, integrated 
advertising into its news broadcasts, continuously streamed to 
the user's screen, even when inactive (it had an active screen 
saver and ticker in a "push technology"). Downloading of 
digital music, video and text (e-books) leads to the immediate 
gratification of consumers and increases the efficacy of 
advertising. 
Whatever the case may be, a uniform, agreed upon system of 
rating as a basis for charging advertisers, is sorely needed. 
There is also the question of what does the advertiser pay 
for?  The rates of many advertisers (Procter and Gamble, for 
instance) are based not on the number of hits or impressions 
(=entries, visits to a site). - but on the number of the times 
that their advertisement was hit (page views), or clicked 
through.  
.  
Finally, there is the paid subscription model - a flop to 
judge by the experience of the meagre number of sites of 
venerable and leading newspapers that are on a subscription 
basis. Dow Jones (Wall Street Journal) and The Economist. Only 
two. 
All this is not very promising. But one should never forget 
that the Internet is probably the closest thing we have to an 
efficient market. As consumers refuse to pay for content, 
investment will dry up and content will become scarce (through 
closures of web sites). As scarcity sets in, consumer may 
reconsider.  
Your article deals with the future of the Internet as a 
medium. Will it be able to support its content creation and 
distribution operations economically? 
If the Internet is a budding medium - then we should derive 
great benefit from a study of the history of its predecessors. 
The Future History of the Internet a Medium 
The internet is simply the latest in a series of networks 
which revolutionized our lives. A century before the internet, 
the telegraph, the railways, the radio and the telephone have 
been similarly heralded as "global" and transforming.  Every 
medium of communications goes through the same evolutionary 
cycle: 



Anarchy 
The Public Phase 
At this stage, the medium and the resources attached to it are 
very cheap, accessible, under no regulatory constraints. The 
public sector steps in : higher education institutions, 
religious institutions, government, not for profit 
organizations, non governmental organizations (NGOs), trade 
unions, etc. Bedevilled by limited financial resources, they 
regard the new medium as a cost effective way of disseminating 
their messages. 
The Internet was not exempt from this phase which ended only a 
few years ago. It started with a complete computer anarchy 
manifested in ad hoc networks, local networks, networks of 
organizations (mainly universities and organs of the 
government such as DARPA, a part of the defence establishment, 
in the USA). Non commercial entities jumped on the bandwagon 
and started sewing these networks together (an activity fully 
subsidized by government funds). The result was a globe 
encompassing network of academic institutions. The American 
Pentagon established the network of all networks, the ARPANET. 
Other government departments joined the fray, headed by the 
National Science Foundation (NSF) which withdrew only lately 
from the Internet. 
The Internet (with a different name) became semi-public 
property - with access granted to the chosen few. 
Radio took precisely this course. Radio transmissions started 
in the USA in 1920. Those were anarchic broadcasts with no 
discernible regularity. Non commercial organizations and not 
for profit organizations began their own broadcasts and even 
created radio broadcasting infrastructure (albeit of the cheap 
and local kind)dedicated to their audiences. Trade unions, 
certain educational institutions and religious groups 
commenced "public radio" broadcasts. 
The Commercial Phase 
When the users (e.g., listeners in the case of the radio, or 
owners of PCs and modems in the case of the Internet) reach a 
critical mass - the business sector is alerted. In the name of 
capitalist ideology (another religion, really) it demands 
"privatization" of the medium. This harps on very sensitive 
strings in every Western soul: the efficient allocation of 
resources which is the result of competition. Corruption and 
inefficiency are intuitively associated with the public sector 
("Other People's Money" - OPM). This, together with the 
ulterior motives of members of the ruling political echelons 
(the infamous American Paranoia), a lack of variety and of 
catering to the tastes and interests of certain audiences and 
the automatic equation of private enterprise with democracy 
lead to a privatization of the young medium. 
The end result is the same: the private sector takes over the 
medium from "below" (makes offers to the owners or operators 
of the medium that they cannot possibly refuse) - or from 
"above" (successful lobbying in the corridors of power leads 
to the appropriate legislation and the medium is 
"privatized"). Every privatization - especially that of a 
medium - provokes public opposition. There are (usually 
founded) suspicions that the interests of the public are 
compromised and sacrificed on the altar of commercialization 
and rating. 


Fears of monopolization and cartelization of the medium are 
evoked - and proven correct in due course. Otherwise, there is 
fear of the concentration of control of the medium in a few 
hands. All these things do happen - but the pace is so slow 
that the initial fears are forgotten and public attention 
reverts to fresher issues. 
A new Communications Act was enacted in the USA in 1934. It 
was meant to transform radio frequencies into a national 
resource to be sold to the private sector which was supposed 
to use it to transmit radio signals to receivers. In other 
words : the radio was passed on to private and commercial 
hands. Public radio was doomed to be marginalized. 
The American administration withdrew from its last major 
involvement in the Internet in April 1995, when the NSF ceased 
to finance some of the networks and, thus, privatized its 
hitherto heavy involvement in the net. 
A new Communications Act was legislated in 1996. It permitted 
"organized anarchy". It allowed media operators to invade each 
other's territories. Phone companies were allowed to transmit 
video and cable companies were allowed to transmit telephony, 
for instance. This was all phased over a long period of time - 
still, it was a revolution whose magnitude is difficult to 
gauge and whose consequences defy imagination. It carries an 
equally momentous price tag - official censorship. "Voluntary 
censorship", to be sure, somewhat toothless standardization 
and enforcement authorities, to be sure - still, a censorship 
with its own institutions to boot. The private sector reacted 
by threatening litigation - but, beneath the surface it is 
caving in to pressure and temptation, constructing its own 
censorship codes both in the cable and in the internet media. 
Institutionalization 
This phase is the next in the Internet's history, though, it 
seems, few realize it. 
It is characterized by enhanced activities of legislation. 
Legislators, on all levels, discover the medium and lurch at 
it passionately. Resources which were considered "free", 
suddenly are transformed to "national treasures not to be 
dispensed with cheaply, casually and with frivolity". 
It is conceivable that certain parts of the Internet will be 
"nationalized" (for instance, in the form of a licensing 
requirement) and tendered to the private sector. Legislation 
will be enacted which will deal with permitted and disallowed 
content (obscenity ? incitement ? racial or gender bias ?) No 
medium in the USA (not to mention the wide world) has eschewed 
such legislation. There are sure to be demands to allocate 
time (or space, or software, or content, or hardware) to 
"minorities", to "public affairs", to "community business". 
This is a tax that the business sector will have to pay to 
fend off the eager legislator and his nuisance value. 
All this is bound to lead to a monopolization of hosts and 
servers. The important broadcast channels will diminish in 
number and be subjected to severe content restrictions. Sites 
which will refuse to succumb to these requirements - will be 
deleted or neutralized. Content guidelines (euphemism for 
censorship) exist, even as we write, in all major content 
providers (CompuServe, AOL, Yahoo!-Geocities, Tripod, 
Prodigy). 



The Bloodbath 
This is the phase of consolidation. The number of players is 
severely reduced. The number of browser types will settle on 
2-3 (Netscape, Microsoft and Opera?). Networks will merge to 
form privately owned mega-networks. Servers will merge to form 
hyper-servers run on supercomputers in "server farms". The 
number of ISPs will be considerably cut.  50 companies ruled 
the greater part of the media markets in the USA in 1983. The 
number in 1995 was 18. At the end of the century they will 
number 6. 
This is the stage when companies - fighting for financial 
survival - strive to acquire as many users/listeners/viewers 
as possible. The programming is shallowed to the lowest (and 
widest) common denominator. Shallow programming dominates as 
long as the bloodbath proceeds. 
From Rags to Riches 
Tough competition produces four processes: 
     1. A Major Drop in Hardware Prices 
This happens in every medium but it doubly applies to a 
computer-dependent medium, such as the Internet. Computer 
technology seems to abide by "Moore's Law" which says that the 
number of transistors which can be put on a chip doubles every 
18 months. As a result of this miniaturization, computing 
power quadruples every 18 months and an exponential series 
ensues. Organic-biological-DNA computers, quantum computers, 
chaos computers - prompted by vast profits and spawned by 
inventive genius will ensure the continued applicability of 
Moore's Law. 
The Internet is also subject to "Metcalf's Law". 
It says that when we connect N computers to a network - we get 
an increase of N to the second power in its computing 
processing power. And these N computers are more powerful 
every year, according to Moore's Law. The growth of computing 
powers in networks is a multiple of the effects of the two 
laws. More and more computers with ever increasing computing 
power get connected and create an exponential 16 times growth 
in the network's computing power every 18 months. 
     2. Content related Fees 
This was prevalent in the Net until recently. Even potentially 
commercial software can still be downloaded for free. In many 
countries television viewers still pay for television 
broadcasts - but in the USA and many other countries in the 
West, the basic package of television channels comes free of 
charge. 
As users / consumers form a habit of using (or consuming) the 
software - it is commercialized and begins to carry a price 
tag. This is what happened with the advent of cable television 
: contents are sold for subscription or per usage (Pay Per 
View - PPV) fees. 
Gradually, this is what will happen to most of the sites and 
software on the Net. Those which survive will begin to collect 
usage fees, access fees, subscription fees, downloading fees 
and other, appropriately named, fees. These fees are bound to 
be low - but it is the principle that counts. Even a few cents 
per transaction may accumulate to hefty sums with the traffic 
which characterizes some web sites on the Net (or, at least 
its more popular locales). 
     3. Increased User Friendliness 
As long as the computer is less user friendly and less 
reliable (predictable) than television - less of a black box - 
its potential (and its future) is limited. Television attracts 
3.5 billion users daily. The Internet stands to attract - 
under the  
most exuberant scenario - less than one tenth of this number 
of people. The only reasons for this disparity are (the lack 
of) user friendliness and reliability. Even browsers, among 
the most user friendly applications ever -are not sufficiently 
so. The user still needs to know how to use a keyboard and 
must possess some basic acquaintance with the operating 
system.  The more mature the medium, the more friendly it 
becomes. Finally, it will be operated using speech or common 
language. There will be room left for user "hunches" and built 
in flexible responses.
     4. Social Taxes 
Sooner or later, the business sector has to mollify the God of 
public opinion with offerings of political and social nature. 
The Internet is an affluent, educated, yuppie medium. It 
requires literacy and numeracy, live interest in information 
and  
its various uses (scientific, commercial, other), a lot of 
resources (free time, money to invest in hardware, software 
and connect time). It empowers - and thus deepens the divide 
between the haves and have-nots, the developed and the 
developing world, the knowing and the ignorant, the computer 
illiterate. 
In short: the Internet is an elitist medium. Publicly, this is 
an unhealthy posture. "Internetophobia" is already 
discernible. People (and politicians) talk about how unsafe 
the Internet is and about its possible uses for racial, sexist 
and pornographic purposes. The wider public is in a state of 
awe. 
So, site builders and owners will do well to begin to improve 
their image: provide free access to schools and community 
centres, bankroll internet literacy classes, freely distribute 
contents and software to educational institutions, collaborate 
with researchers and social scientists and engineers. In 
short: encourage the view that the Internet is a medium 
catering to the needs of the community and the 
underprivileged, a mostly altruist endeavour. This also 
happens to make good business sense by educating and 
conditioning a future generation of users. He who visited a 
site when a student, free of charge - will pay to do so when 
made an executive. Such a user will also pass on the 
information within and without his organization. This is 
called media exposure. The future will, no doubt, will be 
witness to public Internet terminals, subsidized ISP accounts, 
free Internet classes and an alternative "non-commercial, 
public" approach to the Net. This may prove to be one more 
source of revenue to content creators and distributors. 


 
  
Jamaican Overdrive - LDC's and LCD's
By: Sam Vaknin
 
OverDrive - an e-commerce, software conversion and e-
publishing applications leader - has just expanded an e-book 
technology centre by adding 200 e-book editors. This happened 
in Montego Bay, Jamaica - one of the less privileged spots on 
earth. The centre now provides a vertical e-publishing service 
- from manuscript editing to conversion to Quark (for POD), 
Adobe, and MS Reader ebook formats. Thus, it is not confined 
to the classic sweatshop cum production centre so common in 
Less Developed Countries (LDC's). It is a full fledged 
operation with access to cutting edge technology. 
The Jamaican OverDrive is the harbinger of things to come and 
the outcome of a confluence of a few trends. 
First, there is the insatiable appetite big publishers (such 
as McGraw-Hill, Random House, and Harper Collins) have 
developed to converting their hitherto inertial backlists into 
e-books. Gone are the days when e-books were perceived as 
merely a novel form of packaging. Publishers understood the 
cash potential this new distribution channel offers and the 
value added to stale print tomes in the conversion process. 
This epiphany is especially manifest in education and textbook 
publishing. 
Then there is the maturation of industry standards, readers 
and audiences. Both the supply side (title lists) and the 
demand side (readership) have increased. Giants like Microsoft 
have successfully entered the fray with new e-book reader 
applications, clearer fonts, and massive marketing. Retailers 
- such as Barnes and Noble - opened their gates to e-books. A 
host of independent publishers make good use of the 
negligible-cost distribution channel that the Internet is. 
Competition and positioning are already fierce - a good sign. 
The Internet used to be an English, affluent middle-class, 
white collar, male phenomenon. It has long lost these 
attributes. The digital divides that opened up with the early 
adoption of the Net by academe and business - are narrowing. 
Already there are more women than men users and English is the 
language of less than half of all web sites. The wireless Net 
will grant developing countries the chance to catch up. 
Astute entrepreneurs are bound to take advantage of the 
business-friendly profile of the manpower and investment-
hungry governments of some developing countries. It is not 
uncommon to find a mastery of English, a college degree in the 
sciences, readiness to work outlandish hours at a fraction of 
wages in Germany or the USA - all combined in one employee in 
these deprived countries. India has sprouted a whole industry 
based on these competitive endowments. 
Here is how Steve Potash, OverDrive's CEO, explains his daring 
move in OverDrive's press release dated May 22, 2001: 
"Everyone we are partnering with in the US and worldwide has 
been very excited and delighted by the tremendous success and 
quality of eBook production from OverDrive Jamaica. Jamaica 
has tremendous untapped talent in its young people. Jamaica is 
the largest English-speaking nation in the Caribbean and their 
educational and technical programs provide us with a wealth of 
quality candidates for careers in electronic publishing. We 
could not have had this success without the support and 
responsiveness of the Jamaican government and its agencies. At 
every stage the agencies assisted us in opening our technology 
centre and staffing it with trained and competent eBook 
professionals. OverDrive Jamaica will be pioneering many of 
the advances for extending books, reference materials, 
textbooks, literature and journals into new digital channels - 
and will shortly become the foremost centre for eBook 
automation serving both US and international markets". 
Druanne Martin, OverDrive's Director of publishing services 
elaborates: 
""With Jamaica and Cleveland, Ohio sharing the same time zone 
(EST), we have our US and Jamaican production teams in sync. 
Jamaica provides a beautiful and warm climate, literally, for 
us to build long-term partnerships and to invite our 
publishing and content clients to come and visit their books 
in production". 
The Jamaican Minister of Industry, Commerce and Technology, 
the Hon. Phillip Paulwell reciprocates: 
"We are proud that OverDrive has selected Jamaica to extend 
its leadership in eBook technology. OverDrive is benefiting 
from the investments Jamaica has made in developing the needed 
infrastructure for IT companies to locate and build skilled 
workforces here." 
There is nothing new in outsourcing back office work 
(insurance claims processing, air ticket reservations, medical 
records maintenance) to third world countries, such as (the 
notable example) India. Research and Development is routinely 
farmed out to aspiring first world countries such as Israel 
and Ireland. But OverDrive's Jamaican facility is an example 
of something more sophisticated and more durable. Western 
firms are discovering the immense pools of skills, talent, 
innovation, and top notch scientific and other education often 
offered even by the poorest of nations. These multinationals 
entrust the locals now with more than keyboarding and 
responding to customer queries using fake names. The Jamaican 
venture is a business partnership. In a way, it is a topsy-
turvy world. Digital animation is produced in India and 
consumed in the States. The low compensation of scientists 
attracts the technology and R&D arms of the likes of General 
Electric to Asia and Intel to Israel. In other words, there 
are budding signs of a reversing brain drain - from West to 
East. 
E-publishing is at the forefront of software engineering, e-
consumerism, intellectual property technologies, payment 
systems, conversion applications, the mobile Internet, and, 
basically, every important trend in network and computing and 
digital content. Its migration to warmer and cheaper climates 
may be inevitable. OverDrive sounds happy enough. 



 
An Ambarrassment of Riches 
By: Sam Vaknin
 
http://www.doi.org/  
  
The Internet is too rich. Even powerful and sophisticated 
search engines, such as Google, return a lot of trash, dead 
ends, and Error 404's in response to the most well-defined 
query, Boolean operators and all. Directories created by human 
editors - such as Yahoo! or the Open Directory Project - are 
often overwhelmed by the amount of material out there. Like 
the legendary blob, the Internet is clearly out of 
classificatory control. Some web sites - like Suite101 - have 
introduced the old and tried Dewey subject classification 
system successfully used in non-virtual libraries for more 
than a century. Books - both print and electronic - (actually, 
their publishers) get assigned an ISBN (International Standard 
Book Number) by national agencies. Periodical publications 
(magazines, newsletters, bulletins) sport an ISSN 
(International Serial Standard Number). National libraries 
dole out CIP's (Cataloguing in Publication numbers), which 
help lesser outfits to catalogue the book upon arrival. But 
the emergence of new book formats, independent publishing, and 
self publishing has strained this already creaking system to 
its limits. In short: the whole thing is fast developing into 
an awful mess. 
Resolution is one solution. 
Resolution is the linking of identifiers to content. An 
identifier can be a word, or a phrase. RealNames implemented 
this approach and its proprietary software is now incorporated 
in most browsers. The user types a word, brand name, phrase, 
or code, and gets re-directed to a web site with the 
appropriate content. The only snag: RealNames identifiers are 
for sale. Thus, its identifiers are not guaranteed to lead to 
the best, only, or relevant resource. Similar systems are 
available in many languages. Nexet, for example, provides such 
a resolution service in Hebrew. 
The Association of American Publishers (APA) has an Enabling 
Technologies Committee. Fittingly, at the Frankfurt Book Fair 
of 1997, it announced the DOI (Digital Object Identifier) 
initiative. An International DOI Foundation (IDF) was set up 
and invited all publishers - American and non-American alike - 
to apply for a unique DOI prefix. DOI is actually a private 
case of a larger system of "handles" developed by the CNRI 
(Corporation for National Research Initiatives). Their "Handle 
Resolver" is a browser plug-in software, which re-directs 
their handles to URL's or other pieces of data, or content. 
Without the Resolver, typing in the handle simply directs the 
user to a few proxy servers, which "understand" the handle 
protocols. 



The interesting (and new) feature of the system is its ability 
to resolve to MULTIPLE locations (URL's, or data, or content). 
The same identifier can resolve to a Universe of inter-related 
information (effectively, to a mini-library). The content thus 
resolved need not be limited to text. Multiple resolution 
works with audio, images, and even video. 
The IDF's press release is worth some extensive quoting: 
"Imagine you're the manager of an Internet company reading a 
story online in the "Wall Street Journal" written by Stacey E. 
Bressler, a co-author of Communities of Commerce, and at the 
end of the story there is a link to purchase options for the 
book.  
Now imagine you are an online retailer, a syndicator or a 
reporter for an online news service and you are reading a 
review in "Publishers Weekly" about Communities of Commerce 
and you run across a link to related resources. 
And imagine you are in Buenos Aires, and in an online 
publication you encounter a link to "D-Lib Magazine", an 
electronic journal produced in Washington, D.C. which offers 
you locale-specific choices for downloading an article.  
The above examples demonstrate how multiple resolution can 
present you with a list of links from within an electronic 
document or page. The links beneath the labels - URLs and 
email addresses - would all be stored in the DOI System, and 
multiple resolution means any or all of those links can be 
displayed for you to select from in one menu. Any combination 
of links to related resources can be included in these menus.  
Capable of providing much richer experiences then single 
resolution to a URL, Multiple Resolution operates on the 
premise that content, not its location, is identified. In 
other words, where content and related resources reside is 
secondary information. Multiple Resolution enables content 
owners and distributors to identify their intellectual 
property with bound collections of related resources at a 
hyperlink's point of departure, instead of requiring a user to 
leave the page to go to a new location for further 
information.  
A content owner controls and manages all the related resources 
in each of these menus and can determine which information is 
accessible to each business partner within the supply chain. 
When an administrator changes any facet of this information, 
the change is simultaneous on all internal networks and the 
Internet. A DOI is a permanent identifier, analogous to a 
telephone number for life, so tomorrow and years from now a 
user can locate the product and related resources wherever 
they may have been moved or archived to." 
The IDF provides a limited, text-only, online demonstration. 
When sweeping with the cursor over a linked item, a pop-down 
menu of options is presented. These options are pre-defined 
and customized by the content creators and owners. In the 
first example above (book purchase options) the DOI resolves 
to retail outlets (categorized by book formats), information 
about the title and the author, digital rights management 
information (permissions), and more. The DOI server generates 
this information in "real time", "on the fly". But it is the 
author, or (more often) the publisher that choose the 
information, its modes of presentation, selections, and 
marketing and sales data. The ingenuity is in the fact that 
the DOI server's files and records can be updated, replaced, 
or deleted. It does not affect the resolution path - only the 
content resolved to. 
Which brings us to e-publishing. 
The DOI Foundation has unveiled the DOI-EB (EB stands for e-
books) Initiative in the Book Expo America Show 2001, to, in 
their words:
"Determine requirements with respect to the application of 
unique identifiers to eBooks 
Develop proofs-of-concept for the use of DOIs with eBooks 
Develop technical demonstrations, possibly including a 
prototype eBook Registration Agency."
It is backed by a few major publishers, such as McGraw-Hill, 
Random House, Pearson, and Wiley.
This ostensibly modest agenda conceals a revolutionary and 
ambitious attempt to unambiguously identify the origin of 
digital content (in this case, e-books) and link a universe of 
information to each and every ID number. Aware of competing 
efforts underway, the DOI Foundation is actively courting the 
likes of "indecs" (Interoperability of Data in E-Commerce 
System) and OeBF (Open e-Book). Companies ,like Enpia Systems 
of South Korea (a DOI Registration Agency), have already 
implemented a DOI-cum-indecs system. On November 2000, the 
APA's (American Publishers' Association) Open E-book 
Publishing Standards Initiative has recommended to use DOI as 
the primary identification system for e-books' metadata. The 
MPEG (Motion Pictures Experts Group) is said to be considering 
DOI seriously in its efforts to come up with numbering and 
metadata standards for digital videos. A DOI can be expressed 
as a URN (Universal Resource Name - IETF's syntax for generic 
resources) and is compatible with OpenURL (a syntax for 
embedding parameters such as identifiers and metadata in 
links). Shortly, a "Namespace Dictionary" is to be published. 
It will encompass 800 metadata elements and will tackle e-
books, journals, audio, and video. A working group was started 
to develop a "services definition" interface (i.e., to allow 
web-enabled systems, especially e-commerce and m-commerce 
systems, to deploy DOI).
The DOI, in other words, is designed to be all-inclusive and 
all-pervasive. Each DOI number is made of a prefix, specific 
to a publisher, and a suffix, which could end up painlessly 
assimilating the ISBN and ISSN (or any other numbering and 
database) system. 
Thus, a DOI can be assigned to every e-book based on its ISBN 
and to every part (chapter, section, or page) of every e-book. 
This flexibility could support Pay Per View models (such as 
Questia's or Fathom's), POD (Print On Demand), and academic 
"course packs", which comprise material from many textbooks, 
whether on digital media or downloadable. The DOI, in other 
words, can underlie D-CMS (Digital Content Management Systems) 
and Electronic Catalogue ID Management Systems.
Moreover, the DOI is a paradigm shift (though, conceptually, 
it was preceded by the likes of the UPC code and the ISO's 
HyTime multimedia standard). It blurs the borders between 
types of digital content. Imagine an e-novel with the video 
version of the novel, the sound track, still photographs, a 
tourist guide, an audio book, and other digital content 
embedded in it. Each content type and each segment of each 
content type can be identified and tagged separately and, 
thus, sold separately - yet all under the umbrella of the same 
DOI! The nightmare of DRM (digital rights management) may be 
finally over. 



But the DOI is much more than a sophisticated tagging 
technology. It comes with multiple resolution (see 
"Embarrassment of Riches - Part I"). In other words, as 
opposed to the URL (Universal Resource Locator) - it is 
generated dynamically, "on the fly", by the user, and is not 
"hard coded" into the web page. This is because the DOI 
identifies content - not its location. And while the URL 
resolves to a single web page - the DOI resolves to a lot more 
in the form of publisher-controlled (ONIX-XML) "metadata" in a 
pop-up (Javascript or other) screen. The metadata include 
everything from the author's name through the book's title, 
edition, blurbs, sample chapters, other promotional material, 
links to related products, a rights and permissions profile, 
e-mail contacts, and active links to retailers' web pages. 
Thus, every book-related web page becomes a full fledged book 
retailing gateway. The "anchor document" (in which the DOI is 
embedded) remains uncluttered. ONIX 2.0 may contain standard 
metadata fields and extensions specific to e-publishing and e-
books.
This latter feature - the ability to link to the systems of 
retailers, distributors, and other types of vendors - is the 
"barcode" function of the DOI. Like barcode technology, it 
helps to automate the supply chain, and update the inventory, 
ordering, billing and invoicing, accounting, and re-ordering 
databases and functions. Besides tracking content use and 
distribution, the DOI allows to seamlessly integrate hitherto 
disparate e-commerce technologies and facilitate 
interoperability among DRM systems.
The resolution itself can take place in the client's browser 
(using a software plug-in), in a proxy server, or in a 
central, dynamic server. Resolving from the client's PC, e-
book reader, or PDA has the advantage of being able to respond 
to the user's specific condition (location, time of day, 
etc.). No plug-in is required when a proxy server HTTP is used 
- but then the DOI becomes just another URL, embedded in the 
page when it is created and not resolved when the user clicks 
on it. The most user-friendly solution is, probably, for a 
central server to look up values in response to a user's 
prompt and serve her with cascading menus or links. 
Admittedly, in this option, the resolution tables (what DOI 
links to what URL's and to what content) is not really 
dynamic. It changes only with every server update and is 
static between updates. But this is a minor inconvenience. As 
it is, users are likely to respond with some trepidation to 
the need to install plug-ins and to the avalanche of 
information their single, innocuous, mouse click generates.
The DOI Foundation has compiled this impressive list of 
benefits - and beneficiaries:
"Publishers to enable cross referencing to related 
information, control over metadata, viral distribution and 
sales, easy access to content, sale of granular content 
Consumers to increase value for time and money, and purchase 
options 
Distributors to facilitate sale and distribution of materials 
as well as user needs 
Retailers to build related materials on their sites, heighten 
consumer usability and copyright protection 
Conversion Houses/Wholesaler Repositories to increase access 
to and use of metadata 
DRM Vendors/Rights Clearing Houses to enable interoperability 
and use of standards 
Data Aggregators to enable compilation of primary and 
secondary content and print on demand 
Trade Associations facilitate dialog on social level and 
attend to legal and technical perspectives pertaining to 
multiple versions of electronic content 
eBbook software Developers to enable management of personal 
collections of eBooks including purchase receipt information 
as reference for quick return to retailer 
Content Management System Vendors to enable internal synching 
with external usage 
Syndicators to drive sales to retailers, add value to retail 
online store/sales, and increase sales for publishers"
The DOI is assigned to publishers by Registration Agencies (of 
which there are currently three - CrossRef and Content 
Directions in the States and the aforementioned Enpia Systems 
in Asia). It is already widely used to cross reference almost 
5,000 periodicals with a database of 3,000,000 citations. The 
price is steep - it costs a publisher $200 to get a prefix and 
submit DOI's to the registry. But as Registration Agencies 
proliferate, competition is bound to slash these prices 
precipitously.



 
The Fall and Fall of the P-Zine  
   
By: Sam Vaknin
http://home.wuliweb.com/index.shtml 
http://www.pshares.org/  
  
The circulation of print magazines has declined precipitously 
in the last 24 months. This dissolution of subscriber bases 
has accelerated dramatically as economic recession set in. But 
a diminishing wealth effect is only partly to blame. The 
managements of printed periodicals - from dailies to 
quarterlies - failed miserably to grasp the Internet's 
potential and  potential threat. They were fooled by the lack 
of convenient and cheap e-reading devices into believing that 
old habits die hard. They do - but magazine reading is not 
habit forming. Readers' loyalties are fickle and shift 
according to content and price. The Web offers cornucopial and 
niche-targeted content - free of charge or very cheaply. This 
is hard to beat and is getting harder by the day as natural 
selection among dot.bombs spares only quality content 
providers.  
Consider Ploughshares, the Literary Journal. 
It is a venerable, not for profit, print journal published by 
Emerson College, now marking its 30th anniversary. It recently 
inaugurated its web sibling. The project consumed three years 
and $125,000 (grant from the Wallace-Reader's Digest Funds). 
Every title Ploughshares has ever published was indexed (over 
18,000 journal pages digitized). In all, the "website will 
offer free access to over 2,750 poems and short stories from 
past and current issues." 
The more than 2000 (!) authors ever published in Ploughshares 
will each maintain a personal web page comprising biographical 
notes, press releases, new books and events announcements and 
links to other web sites. This is the Yahoo! formula. Content 
generated by the authors will thus transform Ploughshares into 
a leading literary portal. 
 
But Ploughshares did not stop at this standard features. A 
"bookshelf" will link to book reviews contributed online (and 
augmented by the magazine's own prestigious offerings). An 
annotated bookstore is just a step away (though Ploughshares' 
web site does not include one hitherto). The next best thing 
is a rights-management application used by  the journal's 
authors to grant online publishing permissions for their work 
to third parties. 
 
No print literary magazine can beat this one stop shop. So, 
how can print publications defend themselves? 
By being creative and by not conceding defeat is how. 
Consider WuliWeb's example of thinking outside the printed 
box. 
It is a simple online application which enables its users to 
"send, save and share material from print publications". 
Participating magazines and newspapers print "WuliCodes" on 
their (physical) pages and WuliWeb subscribers barcode-scan, 
or manually enter them into their online "Content Manager" via 
keyboard, PDA, pager, cell phone, or fixed phone (using a 
PIN). The service is free (paid for by the magazine publishers 
and advertisers) and, according to WuliWeb, offers these 
advantages to its users: 
"Once you choose to use WuliWeb's free service, you will no 
longer have to laboriously "tear and share" print articles or 
ads that you want to archive or share with colleagues or 
friends. You will be able to store material sourced from print 
publications permanently in your own secure, electronic files, 
and you can share this material instantly with any number of 
people. Magazine and Newspaper Publishers will now have the 
ability to distribute their online content more widely and to 
offer a richer experience to their readers. Advertisers will 
be able to deploy dynamic and media-rich content to 
attract and convert customers, and will be able to communicate 
more completely with their customers." 
Links to the shared material are stored in WuliWeb's central 
database and users gain access to them by signing up for a 
(free) WuliWeb account. Thus, the user's mailbox is 
unencumbered by huge downloads. Moreover, WuliWeb allows for a 
keywords-based search of articles saved.  
Perhaps the only serious drawback is that WuliWeb provides its 
users only with LINKS to content stored on publishers' web 
sites. It is a directory service - not a full text database. 
This creates dependence. Links may get broken. Whole web sites 
vanish. Magazines and their publishers go under. All the more 
reason for publishers to adopt this service and make it their 
own.



 
 
The Internet and the Library   
   
By: Sam Vaknin
"In this digital age, the custodians of published works are at 
the center of a global copyright controversy that casts them 
as villains simply for doing their job: letting people borrow 
books for free." 
(ZDNet quoted by "Publisher's Lunch on July 13, 2001) 
It is amazing that the traditional archivists of human 
knowledge - the libraries - failed so spectacularly to ride 
the tiger of the Internet, that epitome and apex of knowledge 
creation and distribution. At first, libraries, the inertial 
repositories of printed matter, were overwhelmed by the rapid 
pace of technology and by the ephemeral and anarchic content 
it spawned. They were reduced to providing access to dull card 
catalogues and unimaginative collections of web links. The 
more daring added online exhibits and digitized collections. A 
typical library web site is still comprised of static 
representations of the library's physical assets and a few 
quasi-interactive services.  
This tendency - by both publishers and libraries - to 
inadequately and inappropriately pour old wine into new 
vessels is what caused the recent furor over e-books.  
 
The lending of e-books to patrons appears to be a natural 
extension of the classical role of libraries: physical book 
lending. Libraries sought also to extend their archival 
functions to e-books. But librarians failed to grasp the 
essential and substantive differences between the two formats. 
E-books can be easily, stealthily, and cheaply copied, for 
instance. Copyright violations are a real and present danger 
with e-books. Moreover, e-books are not a tangible product. 
"Lending" an e-book - is tantamount to copying an e-book. In 
other words, e-books are not books at all. They are software 
products. Libraries have pioneered digital collections (as 
they have other information technologies throughout history) 
and are still the main promoters of e-publishing. But now they 
are at risk of becoming piracy portals.  
Solutions are, appropriately, being borrowed from the software 
industry. NetLibrary has lately granted multiple user licences 
to a university library system. Such licences allow for 
unlimited access and are priced according to the number of the 
library's patrons, or the number of its reading devices and 
terminals. Another possibility is to implement the shareware 
model - a trial period followed by a purchase option or an 
expiration, a-la Rosetta's expiring e-book.  
 
Distributor Baker & Taylor have unveiled at the recent ALA a 
prototype e-book distribution system jointly developed  by 
ibooks and Digital Owl. It will be sold to libraries by B&T's 
Informata division and Reciprocal.


 
 
The annual subscription for use of the digital library 
comprises "a catalog of digital content, brandable pages and 
web based tools for each participating library to customize 
for their patrons. Patrons of participating libraries will 
then be able to browse digital content online, or download and 
check out the content they are most interested in. Content may 
be checked out for an extended period of time set by each 
library, including checking out eBooks from home." Still, it 
seems that B&T's approach is heavily influenced by software 
licencing ("one copy one use"). 
 
But, there is an underlying, fundamental incompatibility 
between the Internet and the library. They are competitors. 
One vitiates the other. Free Internet access and e-book 
reading devices in libraries notwithstanding - the Internet, 
unless harnessed and integrated by libraries, threatens their 
very existence by depriving them of patrons. Libraries, in 
turn, threaten the budding software industry we, misleadingly, 
call "e-publishing".  
There are major operational and philosophical differences 
between physical and virtual libraries. The former are based 
on the tried and proven technology of print. The latter on the 
chaos we know as cyberspace and on user-averse technologies 
developed by geeks and nerds, rather than by marketers, users, 
and librarians. 
Physical libraries enjoy great advantages, not the least being 
their habit-forming head start (2,500 years of first mover 
advantage). Libraries are hubs of social interaction and 
entertainment (the way cinemas used to be). Libraries have 
catered to users' reference needs in reference centres for 
centuries (and, lately, through Selective Dissemination of 
Information, or SDI). The war is by no means decided. 
"Progress" may yet consist of the assimilation of hi-tech 
gadgets by lo-tech libraries. It may turn out to be 
convergence at its best, as librarians become computer savvy - 
and computer types create knowledge and disseminate it.



 
 
A Brief History of the Book  
 
By: Sam Vaknin
"The free communication of thought and opinion is one of the 
most precious rights of man; every citizen may therefore 
speak, write and print freely." 
(French National Assembly, 1789) 
I. What is a Book? 
UNESCO's arbitrary and ungrounded definition of "book" is: 
""Non-periodical printed publication of at least 49 pages 
excluding covers". 
But a book, above all else, is a medium. It encapsulates 
information (of one kind or another) and conveys it across 
time and space. Moreover, as opposed to common opinion, it is 
- and has always been - a rigidly formal affair. Even the 
latest "innovations" are nothing but ancient wine in sparkling 
new bottles. 
Consider the scrolling protocol. Our eyes and brains are 
limited readers-decoders. There is only that much that the eye 
can encompass and the brain interpret. Hence the need to 
segment data into cognitively digestible chunks. There are two 
forms of scrolling - lateral and vertical. The papyrus, the 
broadsheet newspaper, and the computer screen are three 
examples of the vertical scroll - from top to bottom or vice 
versa. The e-book, the microfilm, the vellum, and the print 
book are instances of the lateral scroll - from left to right 
(or from right to left, in the Semitic languages).  
In many respects, audio books are much more revolutionary than 
e-books. They do not employ visual symbols (all other types of 
books do), or a straightforward scrolling method. E-books, on 
the other hand, are a throwback to the days of the papyrus.  
The text cannot be opened at any point in a series of 
connected pages and the content is carried only on one side of 
the (electronic) "leaf". Parchment, by comparison, was multi-
paged, easily browseable, and printed on both sides of the 
leaf. It led to a revolution in publishing and to the print 
book. All these advances are now being reversed by the e-book. 
Luckily, the e-book retains one innovation of the parchment - 
the hypertext. Early Jewish and Christian texts (as well as 
Roman legal scholarship) was written on parchment (and later 
printed) and included numerous inter-textual links. The 
Talmud, for example, is made of a main text (the Mishna) which 
hyperlinks on the same page to numerous interpretations 
(exegesis) offered by scholars throughout generations of 
Jewish learning.   
Another distinguishing feature of books is portability (or 
mobility). Books on papyrus, vellum, paper, or PDA - are all 
transportable. In other words, the replication of the book's 
message is achieved by passing it along and no loss is 
incurred thereby (i.e., there is no physical metamorphosis of 
the message). 


The book is like a perpetuum mobile. It spreads its content 
virally by being circulated and is not diminished or altered 
by it. Physically, it is eroded, of course - but it can be 
copied faithfully. It is permanent.  
Not so the e-book or the CD-ROM. Both are dependent on devices 
(readers or drives, respectively). Both are technology-
specific and format-specific. Changes in technology - both in 
hardware and in software - are liable to render many e-books 
unreadable. And portability is hampered by battery life, 
lighting conditions, or the availability of appropriate 
infrastructure (e.g., of electricity).  
II. The Constant Content Revolution 
Every generation applies the same age-old principles to new 
"content-containers". Every such transmutation yields a great 
surge in the creation of content and its dissemination. The 
incunabula (the first printed books) made knowledge accessible 
(sometimes in the vernacular) to scholars and laymen alike and 
liberated books from the scriptoria and "libraries" of 
monasteries. The printing press technology shattered the 
content monopoly. In 50 years (1450-1500), the number of books 
in Europe surged from a few thousand to more than 9 million! 
And, as McLuhan has noted, it shifted the emphasis from the 
oral mode of content distribution (i.e., "communication") to 
the visual mode. 
E-books are threatening to do the same. "Book ATMs" will 
provide Print on Demand (POD) services to faraway places. 
People in remote corners of the earth will be able to select 
from publishing backlists and front lists comprising millions 
of titles. Millions of authors are now able to realize their 
dream to have their work published cheaply and without 
editorial barriers to entry. The e-book is the Internet's 
prodigal son. The latter is the ideal distribution channel of 
the former. The monopoly of the big publishing houses on 
everything written - from romance to scholarly journals - is a 
thing of the past. In a way, it is ironic. Publishing, in its 
earliest forms, was a revolt against the writing (letters) 
monopoly of the priestly classes. It flourished in non-
theocratic societies such as Rome, or China - and languished 
where religion reigned (such as in Sumeria, Egypt, the Islamic 
world, and Medieval Europe). 
With e-books, content will once more become a collaborative 
effort, as it has been well into the Middle Ages. Authors and 
audience used to interact (remember Socrates) to generate 
knowledge, information, and narratives. Interactive e-books, 
multimedia, discussion lists, and collective authorship 
efforts restore this great tradition. Moreover, as in the not 
so distant past, authors are yet again the publishers and 
sellers of their work. The distinctions between these 
functions is very recent. E-books and POD partially help to 
restore the pre-modern state of affairs. Up until the 20th 
century, some books first appeared as a series of pamphlets 
(often published in daily papers or magazines) or were sold by 
subscription. Serialized e-books resort to these erstwhile 
marketing ploys. E-books may also help restore the balance 
between best-sellers and midlist authors and between fiction 
and textbooks. E-books are best suited to cater to niche 
markets, hitherto neglected by all major publishers. 



III. Literature for the Millions 
E-books are the quintessential "literature for the millions". 
They are cheaper than even paperbacks. John Bell (competing 
with Dr. Johnson) published "The Poets of Great Britain" in 
1777-83. Each of the 109 volumes cost six shillings (compared 
to the usual guinea or more). The Railway Library of novels 
(1,300 volumes) costs 1 shilling apiece only eight decades 
later. The price continued to dive throughout the next century 
and a half. E-books and POD are likely to do unto paperbacks 
what these reprints did to originals. Some reprint libraries 
specialized in public domain works, very much like the bulk of 
e-book offering nowadays. 
The plunge in book prices, the lowering of barriers to entry 
due to new technologies and plentiful credit, the 
proliferation of publishers, and the cutthroat competition 
among booksellers was such that price regulation (cartel) had 
to be introduced. Net publisher prices, trade discounts, list 
prices were all anti-competitive inventions of the 19th 
century, mainly in Europe. They were accompanied by the rise 
of trade associations, publishers organizations, literary 
agents, author contracts, royalties agreements, mass 
marketing, and standardized copyrights.  
The sale of print books over the Internet can be 
conceptualized as the continuation of mail order catalogues by 
virtual means. But e-books are different. They are detrimental 
to all these cosy arrangements. Legally, an e-book may not be 
considered to constitute a "book" at all. Existing contracts 
between authors and publishers may not cover e-books. The 
serious price competition they offer to more traditional forms 
of publishing may end up pushing the whole industry to re-
define itself. Rights may have to be re-assigned, revenues re-
distributed, contractual relationships re-thought. Moreover, 
e-books have hitherto been to print books what paperbacks are 
to hardcovers - re-formatted renditions. But more and more 
authors are publishing their books primarily or exclusively as 
e-books. E-books thus threaten hardcovers and paperbacks 
alike. They are not merely a new format. They are a new mode 
of publishing. 
Every technological innovation was bitterly resisted by 
Luddite printers and publishers: stereotyping, the iron press, 
the application of steam power, mechanical typecasting and 
typesetting, new methods of reproducing illustrations, cloth 
bindings, machine-made paper, ready-bound books, paperbacks, 
book clubs, and book tokens. Without exception, they relented 
and adopted the new technologies to their considerable 
commercial advantage. It is no surprise, therefore, that 
publishers were hesitant to adopt the Internet, POD, and e-
publishing technologies. The surprise lies in the relative 
haste with which they came to adopt it, egged on by authors 
and booksellers. 
IV. Intellectual Pirates and Intellectual Property 
Despite the technological breakthroughs that coalesced to form 
the modern printing press - printed books in the 17th and 18th 
centuries were derided by their contemporaries as inferior to 
their laboriously hand-made antecedents and to the incunabula. 
One is reminded of the current complaints about the new media 
(Internet, e-books), its shoddy workmanship, shabby 
appearance, and the rampant piracy. 


The first decades following the invention of the printing 
press, were, as the Encyclopedia Britannica puts it "a 
restless, highly competitive free for all ... (with) enormous 
vitality and variety (often leading to) careless work".  
There were egregious acts of piracy - for instance, the 
illicit copying of the Aldine Latin "pocket books", or the 
all-pervasive piracy in England in the 17th century (a direct 
result of over-regulation and coercive copyright monopolies). 
Shakespeare's work was published by notorious pirates and 
infringers of emerging intellectual property rights. Later, 
the American colonies became the world's centre of 
industrialized and systematic book piracy. Confronted with 
abundant and cheap pirated foreign books, local authors 
resorted to freelancing in magazines and lecture tours in a 
vain effort to make ends meet. 
Pirates and unlicenced - and, therefore, subversive - 
publishers were prosecuted under a variety of monopoly and 
libel laws (and, later, under national security and obscenity 
laws). There was little or no difference between royal and 
"democratic" governments. They all acted ruthlessly to 
preserve their control of publishing. John Milton wrote his 
passionate plea against censorship, Areopagitica, in response 
to the 1643 licencing ordinance passed by Parliament. The 
revolutionary Copyright Act of 1709 in England established the 
rights of authors and publishers to reap the commercial fruits 
of their endeavours exclusively, though only for a prescribed 
period of time. 
V. As Readership Expanded 
The battle between industrial-commercial publishers (fortified 
by ever more potent technologies) and the arts and 
craftsmanship crowd never ceased and it is raging now as 
fiercely as ever in numerous discussion lists, fora, tomes, 
and conferences. William Morris started the "private press" 
movement in England in the 19th century to counter what he 
regarded as the callous commercialization of book publishing. 
When the printing press was invented, it was put to commercial 
use by private entrepreneurs (traders) of the day. Established 
"publishers" (monasteries), with a few exceptions (e.g., in 
Augsburg, Germany and in Subiaco, Italy) shunned it and 
regarded it as a major threat to culture and civilization. 
Their attacks on printing read like the litanies against self-
publishing or corporate-controlled publishing today.  
But, as readership expanded (women and the poor became 
increasingly literate), market forces reacted. The number of 
publishers multiplied relentlessly. At the beginning of the 
19th century, innovative lithographic and offset processes 
allowed publishers in the West to add illustrations (at first, 
black and white and then in color), tables, detailed maps and 
anatomical charts, and other graphics to their books. Battles 
fought between publishers-librarians over formats (book sizes) 
and fonts (Gothic versus Roman) were ultimately decided by 
consumer preferences. Multimedia was born. The e-book will, 
probably, undergo a similar transition from being the static 
digital rendition of a print edition - to being a lively, 
colorful, interactive and commercially enabled creature.  
The commercial lending library and, later, the free library 
were two additional reactions to increasing demand. As early 
as the 18th century, publishers and booksellers expressed the 
fear that libraries will cannibalize their trade. Two 
centuries of accumulated experience demonstrate that the 
opposite has happened. Libraries have enhanced book sales and 
have become a major market in their own right. 
VI. The State of Subversion 
Publishing has always been a social pursuit and depended 
heavily on social developments, such as the spread of literacy 
and the liberation of minorities (especially, of women). As 
every new format matures, it is subjected to regulation from 
within and from without. E-books (and, by extension, digital 
content on the Web) will be no exception. Hence the recurrent 
and current attempts at regulation.  
Every new variant of content packaging was labeled as 
"dangerous" at its inception. The Church (formerly the largest 
publisher of bibles and other religious and "earthly" texts 
and the upholder and protector of reading in the Dark Ages) 
castigated and censored the printing of "heretical" books 
(especially the vernacular bibles of the Reformation) and 
restored the Inquisition for the specific purpose of 
controlling book publishing. In 1559, it published the Index 
Librorum Prohibitorum ("Index of Prohibited Books"). A few 
(mainly Dutch) publishers even went to the stake (a habit 
worth reviving, some current authors would say...). European 
rulers issued proclamations against "naughty printed books" 
(of heresy and sedition). The printing of books was subject to 
licencing by the Privy Council in England. The very concept of 
copyright arose out of the forced registration of books in the 
register of the English Stationer's Company (a royal 
instrument of influence and intrigue). Such obligatory 
registration granted the publisher the right to exclusively 
copy the registered book (often, a class of books) for a 
number of years - but politically restricted printable 
content, often by force. Freedom of the press and free speech 
are still distant dreams in many corners of the earth. The 
Digital Millennium Copyright Act (DMCA), the V-chip and other 
privacy invading, dissemination inhibiting, and censorship 
imposing measures perpetuate a veteran if not so venerable 
tradition.  
VII. The More it Changes 
The more it changes, the more it stays the same. If the 
history of the book teaches us anything it is that there are 
no limits to the ingenuity with which publishers, authors, and 
booksellers, re-invent old practices. Technological and 
marketing innovations are invariably perceived as threats - 
only to be adopted later as articles of faith. Publishing 
faces the same issues and challenges it faced five hundred 
years ago and responds to them in much the same way. Yet, 
every generation believes its experiences to be unique and 
unprecedented. It is this denial of the past that casts a 
shadow over the future. Books have been with us since the dawn 
of civilization, millennia ago. In many ways, books constitute 
our civilization. Their traits are its traits: resilience, 
adaptation, flexibility, self re-invention, wealth, 
communication. We would do well to accept that our most 
familiar artifacts - books - will never cease to amaze us. 



 
The Affair of the Vanishing Content

By: Sam Vaknin

http://www.archive.org/ 
"Digitized information, especially on the Internet, has such 
rapid turnover these days that total loss is the norm. 
Civilization is developing severe amnesia as a result; indeed 
it may have become too amnesiac already to notice the problem 
properly."
(Stewart Brand, President, The Long Now Foundation )
Thousands of articles and essays posted by hundreds of authors 
were lost forever when themestream.com surprisingly shut its 
virtual gates. A sizable portion of the 1960 census, recorded 
on UNIVAC II-A tapes, is now inaccessible. Web hosts crash 
daily, erasing in the process valuable content. Access to web 
sites is often suspended - or blocked altogether - because of 
a real (or imagined) violation by the webmaster of the host's 
Terms of Service (TOS). Millions of other web sites - the 
results of collective, multi-annual, transcontinental efforts 
- contain unique stores of information in the form of 
databases, articles, discussion threads, and links to other 
web sites. Consider "Central Europe Review". Its archives 
comprise more than 2500 articles and essays about every 
conceivable aspect of Central and Eastern Europe and the 
Balkan. It is one of countless such collections.
Similar and much larger treasures have perished since the dawn 
of the digital age in the 1920's. Very few early radio and TV 
programs have survived, for instance. The current "digital 
dark age" can be compared only to the one which followed the 
torching of the Library of Alexandria. The more accessible and 
abundant the information available to us - the more devalued 
and common it becomes and the less institutional and cultural 
memory we seem to possess. In the battle between paper and 
screen, the former has won formidably. Newspaper archives, 
dating back to the 1700's are now being digitized - testifying 
to the endurance, resilience, and longevity of paper.
Enter the "Internet Libraries", or Digital Archival 
Repositories (DAR). These are libraries that provide free 
access to  digital materials replicated across multiple 
servers ("safety in redundancy"). They contain Web pages, 
television programming, films, e-books, archives of discussion 
lists, etc. Such materials can help linguists trace the 
development of language, journalists conduct research, 
scholars compare notes, students learn, and teachers teach. 
The Internet's evolution mirrors closely the social and 
cultural history of North America at the end of the 20th 
century. If not preserved, our understanding of who we are and 
where we are going will be severely hampered. The clues to our 
future lie ensconced in our past. It is the only guarantee 
against repeating the mistakes of our predecessors. Long gone 
Web pages cached by the likes of Google and Alexa constitute 
the first tier of such archival undertaking. 
The Stanford Archival Vault (SAV) in Stanford University 
assigns a numerical handle to every digital "object" (record) 
in a repository. 


The handle is the clever numerical result of a mathematical 
formula whose input is the number of information bits in the 
original object being deposited. This allows to track and 
uniquely identify records across multiple repositories. It 
also prevents tampering. SAV also offers application layers. 
These allow programmers to develop digital archive software 
and permit users to change the "view" (the interface) of an 
archive and thus to mine data. Its "reliability layer" 
verifies the completeness and accuracy of digital 
repositories.
The Internet Archive, a leading digital depository, in its own 
words:
"...is working to prevent the Internet -- a new medium with 
major historical significance -- and other "born-digital" 
materials from disappearing into the past. Collaborating with 
institutions including the Library of Congress and the 
Smithsonian, we are working to permanently preserve a record 
of public material."
Data storage is the first phase. It is not as simple as it 
sounds. The proliferation of formats of digital content has 
made it necessary to develop a standard for archiving Internet 
objects. The size of the digitized collections must pose a 
serious challenge as far as timely retrieval is concerned. 
Interoperability issues (numerous formats and readers) 
probably requires software and hardware plug-ins to render a 
smooth and transparent user interface.
Moreover, as time passes, digital data, stored on magnetic 
media, tend to deteriorate. It must be copied to newer media 
every 10 years or so ("migration"). Advances in hardware and 
software applications render many of the digital records 
indecipherable (try reading your word processing files from 
1981, stored on 5.25" floppies!). Special emulators of older 
hardware and software must be used to decode ancient data 
files. And, to ameliorate the impact of inevitable natural 
disasters, accidents, bankruptcies of publishers, and 
politically motivated destruction of data - multiple copies 
and redundant systems and archives must be maintained. As time 
passes, data formatting "dictionaries" will be needed. Data 
preservation is hardly useful if the data cannot be searched, 
retrieved, extracted, and researched. And, as "The Economist" 
put it ("The Economist Technology Quarterly, September 22nd, 
2001), without a "Rosetta Stone" of data formats, future 
deciphering of stored the data might prove to be an 
insurmountable obstacle.
Last, but by no means least, Internet libraries are Internet 
based. They themselves are as ephemeral as the historical 
record they aim to preserve. This tenuous cyber existence goes 
a long way towards explaining why our paperless offices 
consume much more paper than ever before. 




Revolt of the Poor - The Demise of Intellectual Property
By: Sam Vaknin
Three years ago I published a book of short stories in Israel. 
The publishing house belongs to Israel's leading (and 
exceedingly wealthy) newspaper. I signed a contract which 
stated that I am entitled to receive 8% of the income from the 
sales of the book after commissions payable to distributors, 
shops, etc. A few months later (1997), I won the coveted Prize 
of the Ministry of Education (for short prose). The prize 
money (a few thousand DMs) was snatched by the publishing 
house on the legal grounds that all the money generated by the 
book belongs to them because they own the copyright. 
In the mythology generated by capitalism to pacify the masses, 
the myth of intellectual property stands out. It goes like 
this : if the rights to intellectual property were not defined 
and enforced, commercial entrepreneurs would not have taken on 
the risks associated with publishing books, recording records, 
and preparing multimedia products. As a result, creative 
people will have suffered because they will have found no way 
to make their works accessible to the public. Ultimately, it 
is the public which pays the price of piracy, goes the 
refrain. 
But this is factually untrue. In the USA there is a very 
limited group of authors who actually live by their pen. Only 
select musicians eke out a living from their noisy vocation 
(most of them rock stars who own their labels - George Michael 
had to fight Sony to do just that) and very few actors come 
close to deriving subsistence level income from their 
profession. All these can no longer be thought of as mostly 
creative people. Forced to defend their intellectual property 
rights and the interests of Big Money, Madonna, Michael 
Jackson, Schwarzenegger and Grisham are businessmen at least 
as much as they are artists. 
Economically and rationally, we should expect that the 
costlier a work of art is to produce and the narrower its 
market - the more emphasized its intellectual property rights. 
Consider a publishing house. 
A book which costs 50,000 DM to produce with a potential 
audience of 1000 purchasers (certain academic texts are like 
this) - would have to be priced at a a minimum of 100 DM to 
recoup only the direct costs. If illegally copied (thereby 
shrinking the potential market as some people will prefer to 
buy the cheaper illegal copies) - its price would have to go 
up prohibitively to recoup costs, thus driving out potential 
buyers. The story is different if a book costs 10,000 DM to 
produce and is priced at 20 DM a copy with a potential 
readership of 1,000,000 readers. Piracy (illegal copying) 
should in this case be more readily tolerated as a marginal 
phenomenon. 
This is the theory. But the facts are tellingly different. The 
less the cost of production (brought down by digital 
technologies) - the fiercer the battle against piracy. The 
bigger the market - the more pressure is applied to clamp down 
on samizdat entrepreneurs. 
Governments, from China to Macedonia, are introducing 
intellectual property laws (under pressure from rich world 
countries) and enforcing them belatedly. But where one factory 
is closed on shore (as has been the case in mainland China) - 
two sprout off shore (as is the case in Hong Kong and in 
Bulgaria). 
But this defies logic : the market today is global, the costs 
of production are lower (with the exception of the music and 
film industries), the marketing channels more numerous (half 
of the income of movie studios emanates from video cassette 
sales), the speedy recouping of the investment virtually 
guaranteed. Moreover, piracy thrives in very poor markets in 
which the population would anyhow not have paid the legal 
price. The illegal product is inferior to the legal copy (it 
comes with no literature, warranties or support). So why 
should the big manufacturers, publishing houses, record 
companies, software companies and fashion houses worry? 
The answer lurks in history. Intellectual property is a 
relatively new notion. In the near past, no one considered 
knowledge or the fruits of creativity (art, design) as 
'patentable', or as someone's 'property'. The artist was but a 
mere channel through which divine grace flowed. Texts, 
discoveries, inventions, works of art and music, designs - all 
belonged to the community and could be replicated freely. 
True, the chosen ones, the conduits, were honoured but were 
rarely financially rewarded. They were commissioned to produce 
their works of art and were salaried, in most cases. Only with 
the advent of the Industrial Revolution were the embryonic 
precursors of intellectual property introduced but they were 
still limited to industrial designs and processes, mainly as 
embedded in machinery. The patent was born. The more massive 
the market, the more sophisticated the sales and marketing 
techniques, the bigger the financial stakes - the larger 
loomed the issue of intellectual property. It spread from 
machinery to designs, processes, books, newspapers, any 
printed matter, works of art and music, films (which, at their 
beginning were not considered art), software, software 
embedded in hardware, processes, business methods, and even 
unto genetic material. 
Intellectual property rights - despite their noble title - are 
less about the intellect and more about property. This is Big 
Money : the markets in intellectual property outweigh the 
total industrial production in the world. The aim is to secure 
a monopoly on a specific work. This is an especially grave 
matter in academic publishing where small- circulation 
magazines do not allow their content to be quoted or published 
even for non-commercial purposes. The monopolists of knowledge 
and intellectual products cannot allow competition anywhere in 
the world - because theirs is a world market. A pirate in 
Skopje is in direct competition with Bill Gates. When he sells 
a pirated Microsoft product - he is depriving Microsoft not 
only of its income, but of a client (=future income), of its 
monopolistic status (cheap copies can be smuggled into other 
markets), and of its competition-deterring image (a major 
monopoly preserving asset). This is a threat which Microsoft 
cannot tolerate. Hence its efforts to eradicate piracy - 
successful in China and an utter failure in legally-relaxed 
Russia. 
But what Microsoft fails to understand is that the problem 
lies with its pricing policy - not with the pirates. When 
faced with a global marketplace, a company can adopt one of 
two policies: either to adjust the price of its products to a 
world average of purchasing power - or to use discretionary 
differential pricing (as pharmaceutical companies were forced 
to do in Brazil and South Africa). A Macedonian with an 
average monthly income of 160 USD clearly cannot afford to buy 
the Encyclopaedia Encarta Deluxe. In America, 50 USD is the 
income generated in 4 hours of an average job. 


In Macedonian terms, therefore, the Encarta is 20 times more 
expensive. Either the price should be lowered in the 
Macedonian market - or an average world price should be fixed 
which will reflect an average global purchasing power. 
Something must be done about it not only from the economic 
point of view. Intellectual products are very price sensitive 
and highly elastic. Lower prices will be more than compensated 
for by a much higher sales volume. There is no other way to 
explain the pirate industries : evidently, at the right price 
a lot of people are willing to buy these products. High prices 
are an implicit trade-off favouring small, elite, select, rich 
world clientele. This raises a moral issue : are the children 
of Macedonia less worthy of education and access to the latest 
in human knowledge and creation ? 
Two developments threaten the future of intellectual property 
rights. One is the Internet. Academics, fed up with the 
monopolistic practices of professional publications - already 
publish on the web in big numbers. I published a few book on 
the Internet and they can be freely downloaded by anyone who 
has a computer or a modem. The full text of electronic 
magazines, trade journals, billboards, professional 
publications, and thousands of books is available online. 
Hackers even made sites available from which it is possible to 
download whole software and multimedia products. It is very 
easy and cheap to publish on the Internet, the barriers to 
entry are virtually nil. Web pages are hosted free of charge, 
and authoring and publishing software tools are incorporated 
in most word processors and browser applications. As the 
Internet acquires more impressive sound and video capabilities 
it will proceed to threaten the monopoly of the record 
companies, the movie studios and so on. 
The second development is also technological. The oft-
vindicated Moore's law predicts the doubling of computer 
memory capacity every 18 months. But memory is only one aspect 
of computing power. Another is the rapid simultaneous advance 
on all technological fronts. Miniaturization and concurrent 
empowerment by software tools have made it possible for 
individuals to emulate much larger scale organizations 
successfully. A single person, sitting at home with 5000 USD 
worth of equipment can fully compete with the best products of 
the best printing houses anywhere. CD-ROMs can be written on, 
stamped and copied in house. A complete music studio with the 
latest in digital technology has been condensed to the 
dimensions of a single chip. This will lead to personal 
publishing, personal music recording, and the to the 
digitization of plastic art. But this is only one side of the 
story. 
The relative advantage of the intellectual property 
corporation does not consist exclusively in its technological 
prowess. Rather it lies in its vast pool of capital, its 
marketing clout, market positioning, sales organization, and 
distribution network. 
Nowadays, anyone can print a visually impressive book, using 
the above-mentioned cheap equipment. But in an age of 
information glut, it is the marketing, the media campaign, the 
distribution, and the sales that determine the economic 
outcome. 
This advantage, however, is also being eroded. 
First, there is a psychological shift, a reaction to the 
commercialization of intellect and spirit. Creative people are 
repelled by what they regard as an oligarchic establishment of 
institutionalized, lowest common denominator art and they are 
fighting back. 
Secondly, the Internet is a huge (200 million people), truly 
cosmopolitan market, with its own marketing channels freely 
available to all. Even by default, with a minimum investment, 
the likelihood of being seen by surprisingly large numbers of 
consumers is high.
I published one book the traditional way - and another on the 
Internet. In 50 months, I have received 6500 written responses 
regarding my electronic book. Well over 500,000 people read it 
(my Link Exchange meter registered c. 2,000,000 impressions 
since November 1998). It is a textbook (in psychopathology) - 
and 500,000 readers is a lot for this kind of publication. I 
am so satisfied that I am not sure that I will ever consider a 
traditional publisher again. Indeed, my last book was 
published in the very same way. 
The demise of intellectual property has lately become 
abundantly clear. The old intellectual property industries are 
fighting tooth and nail to preserve their monopolies (patents, 
trademarks, copyright) and their cost advantages in 
manufacturing and marketing.
But they are faced with three inexorable processes which are 
likely to render their efforts vain:
The Newspaper Packaging 
Print newspapers offer package deals of cheap content 
subsidized by advertising. In other words, the advertisers pay 
for content formation and generation and the reader has no 
choice but be exposed to commercial messages as he or she 
studies the content. 
This model - adopted earlier by radio and television - rules 
the internet now and will rule the wireless internet in the 
future. Content will be made available free of all pecuniary 
charges. The consumer will pay by providing his personal data 
(demographic data, consumption patterns and preferences and so 
on) and by being exposed to advertising. Subscription based 
models are bound to fail. 
Thus, content creators will benefit only by sharing in the 
advertising cake. They will find it increasingly difficult to 
implement the old models of royalties paid for access or of 
ownership of intellectual property.
Disintermediation 
A lot of ink has been spilt regarding this important trend. 
The removal of layers of brokering and intermediation - mainly 
on the manufacturing and marketing levels - is a historic 
development (though the continuation of a long term trend). 
Consider music for instance. Streaming audio on the internet 
or downloadable MP3 files will render the CD obsolete. The 
internet also provides a venue for the marketing of niche 
products and reduces the barriers to entry previously imposed 
by the need to engage in costly marketing ("branding") 
campaigns and manufacturing activities. 
This trend is also likely to restore the balance between 
artist and the commercial exploiters of his product. The very 
definition of "artist" will expand to include all creative 
people. One will seek to distinguish oneself, to "brand" 
oneself and to auction off one's services, ideas, products, 
designs, experience, etc. 


This is a return to pre-industrial times when artisans ruled 
the economic scene. Work stability will vanish and work 
mobility will increase in a landscape of shifting allegiances, 
head hunting, remote collaboration and similar labour market 
trends.
Market Fragmentation 
In a fragmented market with a myriad of mutually exclusive 
market niches, consumer preferences and marketing and sales 
channels - economies of scale in manufacturing and 
distribution are meaningless. Narrowcasting replaces 
broadcasting, mass customization replaces mass production, a 
network of shifting affiliations replaces the rigid owned-
branch system. The decentralized, intrapreneurship-based 
corporation is a late response to these trends. The mega-
corporation of the future is more likely to act as a 
collective of start-ups than as a homogeneous, uniform (and, 
to conspiracy theorists, sinister) juggernaut it once was. 


The Territorial Web
By: Sam Vaknin
 
The Net was supposed to dissolve anachronistic national 
borders and cultural boundaries. It was expected to vitiate 
distance - both physical and mental. It was hailed as the 
invention that will unify Mankind and harmonize (though not 
homogenize) civilizations, east and west.
Yet, this was not to be. As dot.coms bombed, their more 
veteran and more experienced brick and mortar rivals took over 
the Net, transforming it in the process into a giant content 
delivery, marketing, supply chain management, and customer 
relationship management platform. This evolution all but 
demolished the non-local nature of the early Internet. It has 
also brought it into the remit of existing national laws.
Moreover, governments throughout the world have become more 
assertive in exercising territorial jurisdiction over the 
hitherto ostensibly extraterritorial Net. A French court has 
prohibited Yahoo! from making certain content on its Web sites 
available to French citizens. An American court advised Yahoo! 
to ignore this decision. A Russian programmer was arrested by 
the FBI for offering a decryption software for sale in Russia 
(where it is perfectly legal). Governments from China to Saudi 
Arabia filter Web content regularly. Following the September 
11 attacks, restrictive anti-terrorist legislation the world 
over targeted cyberspace.
But the real territorialization of the Internet - the 
redrawing of its internal contours and the withdrawal of its 
libertarian foundations - is more pernicious, all-pervasive, 
quotidian, and surreptitiously gradual. This is not the 
outcome of legal revolutions and court-driven evolution. It is 
piecemeal, quiet, unnoticed, often inadvertent and unintended. 
It is an "afterthought" rather than a premeditated "plot". It 
happens e-tailer by e-tailer, one Web site after the other, 
like the spread of a virus.
Consider these two - by no means exhaustive - examples. 
Amazon and Geocities (now, Yahoo!Geocities) are two Internet 
establishments, two gigantic communities of users that, 
between them, represent a sizable chunk of all the activity on 
the Internet. 
It has long been impossible for a non-US publisher to sell its 
wares (books, for instance) through Amazon or to Amazon 
directly. Amazon works exclusively with US publishers and 
distributors. To collaborate with Amazon - one of the members 
of a duopoly as far as B2C e-commerce goes - a non-US 
publisher (no matter how substantial) has to work with a US 
distributor and thus forgo a large portion of its revenues 
(payable to the distributor as commissions). Moreover, said 
publisher cannot even open a ZShop (Amazon's version of mom 
and pop store). One has to be a US resident to do so. Amazon 
is closed to the outside world, despite its (false) global 
image. It sells all over the world - but it only buys 
American.
This discriminatory behaviour is partly profit-motivated. It 
is logistically easier and cheaper to deal only with US 
businesses. But Barnes and Noble works directly with foreign 
publishers and they preceded Amazon in the book business by 
decades. 



Yahoo!Geocities has lately instituted a new policy. It limits 
the size of downloads from the free home pages of members of 
its community. If the downloaded content from a given home 
page exceeds 3 Gb (extrapolated based on hourly usage) - the 
"offending" member's page is shut down for an hour. The member 
is then prompted to pay a monthly subscription fee for a 
Premium Service in order avoid a recurrence of this 
unfortunate event. This "marketing drive" is intended to 
compensate Yahoo!Geocities for a precipitous drop in online 
advertising revenues.
The "Premium" package includes "Premium Mail". But only US 
citizens or residents can subscribe to it. And, you guessed it 
right, without the Premium Mail component, one cannot complete 
the subscription process. Though not stated explicitly 
anywhere, the Premium services are closed to the outside world 
and are the exclusive reserve of Americans. One can get around 
this virtual ethnic cleansing by providing false data while 
registering, but this is besides the point.
The Internet is a reflection of the outside world. As 
economies contract, unemployment soars, personal safety 
vanishes, the social fabric disintegrates, and consumption 
slumps - countries tend to isolate themselves politically, 
react aggressively, and protect their national economies. 
Protectionism, unilateralism, and isolationism are scourges 
the Internet was supposed to be immune to. Little did we know.



The In-Credible Web
By: Sam Vaknin
http://www.webcredibility.org/ 
 
People are conditioned to trust written words, not to mention 
images. "I read it in the paper" or "As seen on TV" are worn 
out but still effective cliches. The Internet combines both 
the written and the seen. It is both a textual and a visual 
(and audio) medium. Do people trust Internet content? Is the 
incredible Internet - credible?
In the "brick and mortar" world, credibility is associated 
with brands. A brand, in effect, guarantees the quality and 
specifications of a product (think McDonald's hamburgers), its 
performance (think Palm), level of service and commitment to 
customer care (Amazon), variety, or price (Wal-Mart). Brands 
are sustained and enhanced by advertising campaigns. The 
content or sales pitch of specific ads are often less 
important than the message conveyed by the very existence of a 
campaign: "This company is rich enough (read: stable, 
reliable, trustworthy, here to stay) to spend millions on 
advertising".  
The Internet has very few brands (Yahoo!, Amazon) - and some 
of them are tarnished. Some "old media" brands have entered 
the fray (Barnes and Noble, The Wall Street Journal, the 
Britannica) - hitherto without much success. The overwhelming 
bulk of Web content is created or disseminated by small time 
entrepreneurs and monomaniacs. 
So, how does one establish or acquire credibility in such a 
diffuse and anarchic medium?
Enter Stanford University's "Web Credibility Project".
They define themselves thus:
"Our goal is to understand what leads people to believe what 
they find on the Web. We hope this knowledge will enhance Web 
site design and promote future research on Web credibility. As 
part of this ongoing project we are:
*	Performing quantitative research on Web credibility. 
*	Collecting all public information on Web credibility. 
*	Acting as a clearinghouse for this information. 
*	Facilitating research and discussion about Web 
credibility. 
*	Helping designers create credible Web sites." 


*	Examples of current projects:
 
Timeliness: How does having out-of-date content affect the 
credibility of a Web site?
 
Interaction: How does having a personalized interaction with 
a Web site affect its credibility?
 
Negative Content: How does displaying negative content 
associated with a branded web site affect the credibility of 
the brand?
It is useful to confine ourselves to this definition of trust:
"The subjective belief, perception, or conviction that 
information provided is true, factual, and objective, and that 
commitments undertaken, explicitly, or implicitly, will be 
honoured fully and in a timely manner".
Such perception, belief, or conviction are based on:
*	Past experience in general (with spam, with merchants, or 
providers, with a similar product category, with the same 
type of content, etc.) and personal proclivity to trust 
or to distrust 
*	Experience with the specific merchant or provider 
(whether personal or gleaned from other people's feedback 
- reviews, complaints, and opinions) 
There is little that a merchant can do about the former. The 
latter is, expectedly, influenced by:
*	Professionalism (as evident in Web site design, e-
commerce facilities, user-friendliness, navigability, 
links to other relevant Web pages, links from other Web 
sites, ease and speed of download, updated content, 
proofreading, domain name which matches the company's 
name, availability, multilingualism, etc.) 
*	Trustworthiness (lack of bias, good intentions, 
truthfulness, thoroughness, objectivity, expertise and 
author credentials, knowledgeable sources and treatment, 
citations and bibliography), and what the authors of the 
research call "Real World Feel" (physical address, 
phone/fax numbers, non-Web e-mail address, photos of 
facilities and staff, audio recording, ownership by a not 
for profit organization, URL ending with ORG). 
*	Commercial Web sites are less trusted. Cluttered ads, 
paid subscriptions, e-commerce enabled forms - all reduce 
the site's credibility! This is especially true if the 
entire site is a one, big ad and when it is hard to 
distinguish ads from  content. 
*	Track record (how veteran is the merchant, past financial 
performance, credit history, brand name recognition, 
lists of customers, etc.) 
*	Selection (how many products are carried, how often is 
inventory refreshed, etc.) 
*	Advertising (is the company's business sufficiently 
lucrative to support a campaign?) 
*	Service (good service indicates a reassuring readiness to 
sacrifice the bottom line to cater to the customer's 
legitimate concerns, feedback forms, live support, etc.) 
*	Full disclosure of rates, prices, privacy policy, 
security issues, etc. 
*	Feedback from other users (opinions, reviews, comments, 
FAQs, support groups, etc.) 
*	Site rating and certification by trustworthy agencies 
(like the Better Business Bureau - BBB, VeriSign, TRUSTe) 
- or awards won (from credible and reputable 
organizations). Links from other, well-known and 
believable Web sites. 
The Credibility Web discovered that trust in e-commerce is 
also influenced by idiosyncratic factors. Certain domain names 
(org) are more trusted than others (com). Too many ads, broken 
links, typos, outdated or old content - all diminish trust. In 
the absence of proven markers and behavioral guidelines, 
people seem to resort to extrapolation ("if they can't 
maintain their own Web site ...") and stereotypes (e.g., NGO's 
are more trustworthy than corporations).
As Web sites proliferate (Google indexes well over 3 billion 
now) and Web authoring becomes a routine task - the noise to 
signal ratio of garbage to useful information is bound to 
deteriorate. Search engines already incorporate crude measures 
of credibility in their rankings (e.g., the number of links 
from external Web sites). But, to remain useful, search 
engines (and Web directories) would do well to rate Web 
content more comprehensively and thoroughly. They should rank 
Web sites by  authoritativeness, reliability, and objectivity, 
for instance. 
Research shows that 75% of all respondents resort to the 
Internet as a primary information provider. The inundation of 
irrelevant material caused most surfers to confine their 
surfing to 10 Web sites (the equivalent of "anchors" in 
shopping malls), which they deem reliable, timely, accurate, 
objective, authoritative, and credible. The rest of the 
Internet gets the leftovers.  This worrying trend can be 
reversed only through the emergence of independent and 
commercially-viable rating agencies. Web sites (at least the 
business ones) should be willing to pay for credible rating to 
enhance their stickiness and attract monetizable "eyeballs". 
In the absence of such third party accreditation, the Internet 
risks both irrelevance and disrepute.


Does Free Content - Sell?
By: Sam Vaknin

The answer is: no one knows. Many self-styled "gurus" and 
"pundits" - authors of voluminous tomes they sell to the 
gullible - pretend to know. But their "expertise" is an 
admixture of guesswork, superstitions, anecdotal "evidence" 
and hearsay. The sad truth is that no methodical, long term, 
and systematic research has been attempted in the nascent 
field of e-publishing and, more broadly, digital content on 
the Web. So, no one knows to say for sure whether free content 
sells, when, or how.
There are two schools - apparently equally informed by the 
dearth of hard data. One is the "viral school". Its vocal 
proponents claim that the dissemination of free content fuels 
sales by creating "buzz" (word of mouth marketing driven by 
influential communicators). The "intellectual property" school 
roughly says that free content cannibalizes paid content 
mainly because it conditions potential consumers to expect 
free information. Free content also often serves as a 
substitute (imperfect but sufficient) to paid content.
Experience - though patchy - confusingly seems to points both 
ways. Views and prejudices tend to converge around this 
consensus: whether free content sells or not depends on a few 
variables. They are:
(1) The nature of the information. People are generally 
willing to pay for specific or customized information, 
tailored to their idiosyncratic needs, provided in a timely 
manner, and by authorities in the field. The more general and 
"featureless" the information, the more reluctant people are 
to dip into their pockets (probably because there are many 
free substitutes).
(2) The nature of the audience. The more targeted the 
information, the more it caters to the needs of a unique, or 
specific group, the more often it has to be updated 
("maintained"), the less indiscriminately applicable it is, 
and especially if it deals with money, health, sex, or 
relationships - the more valuable it is and the more people 
are willing to pay for it. The less computer savvy users - 
unable to find free alternatives - are more willing to pay.
(3) Time dependent parameters. The more the content is linked 
to "hot" topics, "burning" issues, trends, fads, buzzwords, 
and "developments" - the more likely it is to sell regardless 
of the availability of free alternatives.
(4) The "U" curve. People pay for content if the free 
information available to them is either (a) insufficient or 
(b) overwhelming. People will buy a book if the author's Web 
site provides only a few tantalizing excerpts. But they are 
equally likely to buy the book if its entire full text content 
is available online and overwhelms them. Packaged and indexed 
information carries a premium over the same information in 
bulk. Consumer willingness to pay for content seems to decline 
if the amount of content provided falls between these two 
extremes. They feel sated and the need to acquire further 
information vanishes. Additionally, free content must really 
be free. People resent having to pay for free content, even if 
the currency is their personal data.
(5) Frills and bonuses. There seems to be a weak, albeit 
positive link between willingness to pay for content and 
"members only" or "buyers only" frills, free add-ons, bonuses, 
and free maintenance. Free subscriptions, discount vouchers 
for additional products, volume discounts, add-on, or 
"piggyback" products - all seem to encourage sales. 
Qualitative free content is often perceived by consumers to be 
a BONUS - hence its enhancing effect on sales.
(6) Credibility. The credibility and positive track record of 
both content creator and vendor are crucial factors. This is 
where testimonials and reviews come in. But their effect is 
particularly strong if the potential consumer finds himself in 
agreement with them. In other words, the motivating effect of 
a testimonial or a review is amplified when the customer can 
actually browse the content and form his or her own opinion. 
Free content encourages a latent dialog between the potential 
consumer and actual consumers (through their reviews and 
testimonials). 
(7) Money back warranties or guarantees. These are really 
forms of free content. The consumer is safe in the knowledge 
that he can always return the already consumed content and get 
his money back. In other words, it is the consumer who decides 
whether to transform the content from free to paid by not 
exercising the money back guarantee. 
(8) Relative pricing. Information available on the Web is 
assumed to be inherently inferior and consumers expect pricing 
to reflect this "fact". Free content is perceived to be even 
more shoddy. The coupling of free ("cheap", "gimcrack") 
content with paid content serves to enhance the RELATIVE VALUE 
of the paid content (and the price people are willing to pay 
for it). It is like pairing a medium height person with a 
midget - the former would look taller by comparison. 
(9) Price rigidity. Free content reduces the price elasticity 
of paid content. Normally, the cheaper the content - the more 
it sells. But the availability of free content alters this 
simple function. Paid content cannot be too cheap or it will 
come to resemble the free alternative ("shoddy", "dubious"). 
But free content is also a substitute (however partial and 
imperfect) to paid content. Thus, paid content cannot be 
priced too high - or people will prefer the free alternative. 
Free content, in other words, limits both the downside and the 
upside of the price of paid content. 
There are many other factors which determine the interaction 
of free and paid content. Culture plays an important role as 
do the law and technology. But as long as the field is not 
subject to a research agenda the best we can do is observe, 
collate - and guess. 
This article is, of course, free content...:o))


Copyright Law and Free Online Scholarship
An Interview with Peter Suber
By: Sam Vaknin
Also published by United Press International (UPI) 

The battle between owners of content and its users extends to 
all corners of the publishing world. Following a brief period 
of enthusing about "synergies", most media companies, content 
aggregators, content providers - movie and recording studios, 
publishers, news organizations - came to view the digitization 
of content as a threat rather than an opportunity. In an 
effort to protect their intellectual property rights, 
publishing and recording corporations have fostered the 
radicalization of copyright law (mainly in the DMCA - the 
Digital Millennium Copyright Act). They have also retarded the 
fair use of copyrighted material and the rights and 
traditional privileges enjoyed by content users. This was 
achieved mainly by incorporating "rights management" or "asset 
management" technologies into readers of digital records (such 
as e-books). These technologies prevented users from copying 
the files they purchased, from converting them to audio, from 
lending them to others (as they would a print book), and from 
reading them on more than one device.
 
Consider, for instance, scholarly publishing. It is in the 
throes of a protracted crisis. 
 
The price of scholarly, peer-reviewed journals has skyrocketed 
in the last three decades, often way out of the limited means 
of libraries, universities, individual scientists and 
scholars. A "scholarly divide" has opened between the haves 
(the negligible minority of academic institutions with rich 
endowments and well-heeled corporations) and the haves not 
(all the others). Paradoxically, due to rising costs, access 
to authoritative and authenticated knowledge has declined as 
the number of professional journals has proliferated. This is 
not to mention the long (and often crucial) delays in 
publishing research results and the shoddy work of many under-
paid and over-worked peer reviewers.
 
The Internet was suppose to change all that. Originally, a 
computer network for the exchange of (restricted and open) 
research results among scientists and academics in 
participating institutions - it was supposed to provide 
instant publishing, instant access, and instant gratification. 
It has delivered only partially. Preprints of academic papers 
are often placed online by their eager authors and subjected 
to peer scrutiny. But this haphazard publishing cottage 
industry did nothing to dethrone the print incumbents and 
their avaricious pricing. 
 
Peter Suber has both a Ph.D. in philosophy and a J.D. He is a 
professor of philosophy at Earlham College, where he also 
teaches law and computer science. This qualifies him uniquely 
to tackle the issue of free online scholarship, which cannot 
be divorced from the legal intricacies of copyright law.  In 
the last 11 months, he has been writing and publishing the 
weekly the Free Online Scholarship (FOS) Newsletter.


 
  
Apart from writing the FOS Newsletter, Suber is working to 
realize FOS on several fronts. He is a consultant to the Open 
Society Institute on FOS issues. He is the general editor of 
the Web's foremost philosophy search engine Hippias and co-
editor of Noesis, both available online free of charge. He 
serves on the Committee on Philosophy and Computers of the 
American Philosophical Association. He is on the board of 
governors of the International Consortium for the Advancement 
of Academic Publishing. With Tony Beavers, He is working on 
software to collect, index, and search the literature at 
distributed online journal sites and text archives.  
 
Q: In "Revolt of the Poor", I wrote: "If the rights to 
intellectual property were not defined and enforced, 
commercial entrepreneurs would not have taken on the risks 
associated with publishing books, recording records, and 
preparing multimedia products. As a result, creative people 
will have suffered because they will have found no way to make 
their works accessible to the public. Ultimately, it is the 
public which pays the price of piracy." Is there any proven 
connection between the enforcement (or even the existence) of 
intellectual property rights - and the preponderance of 
creativity and/or of media entrepreneurship (publishing, 
etc.)? 

A:  I don't have the relevant expertise to answer for music, 
software, general literature, or even scholarly books.  But 
for scholarly journal articles (the main focus of the FOS 
movement), there seems to be very little or no connection 
between copyright and the productivity and creativity of 
authors.  I say this for two reasons.  First, scholarly 
authors tend to transfer copyright in their articles to the 
journals that publish them.  (Most scholars don't realize that 
they could probably negotiate a different arrangement, but 
that's another issue.)  For most journal articles, then, 
copyright protects publishers, not authors.  But this hasn't 
stopped scholars from writing journal articles.  Second, 
authors of scholarly journal articles are not paid for them, 
whether they transfer copyright or not.  Authors consent to 
this practice and willingly submit their articles to journals 
that don't pay for submissions.  Scholarly authors are paid by 
their institutions, not by readers, which frees them from the 
market in deciding what to write.  They are rewarded by making 
a contribution to knowledge and advancing their own careers, 
not by cash.  Hence, the "unauthorized copying" prohibited by 
copyright law doesn't deprive these authors of money, but only 
readers.  Copyright law (at least when used in the traditional 
way to restrict access to paying customers) gets in the way.  
Widespread copying with or without permission would give 
authors of journal articles more readers and more impact, 
without depriving them of any revenue.  But copyright law 
generally prohibits this kind of copying.  Even though this 
limit on free distribution is contrary to their interests, it 
clearly hasn't deterred authors from writing more articles.  
 
Having said that, let me add that the FOS movement doesn't 
need to abolish or even reform copyright law.  If authors of 
scholarly journal articles retain the copyright to their 
articles (transferring only, say, the right of first print 
publication, and perhaps some other rights), then authors can 
consent to widespread copying and finally let copyright 
advance their interests rather than those of publishers.  In 
particular, authors could consent to put their writings on the 
internet without any financial, legal, or technical barriers 
to access.  This is what the FOS movement is trying to 
achieve, and it can all happen within the boundaries of 
existing copyright law.  
 
Q: Could you describe the crisis in scholarly publishing? 
 
A: The main problem is that the prices of journals (both print 
and online journals) have risen faster than inflation and 
faster than library budgets for three decades.  Libraries cope 
by canceling subscriptions, or by taking from their book 
budgets to enlarge their serials (journal) budgets, or both.  
One result is that even researchers at the wealthiest 
institutions do not have access to all the journals they need 
for their research.  Or, from the other end of the author-
reader relationship, authors of journal articles cannot reach 
all the readers who would benefit from the results of their 
research.  When research is slowed and obstructed in this way, 
so are all the benefits of research, such as new medicines. 
 
Another way to put the underlying economic problem is that the 
huge savings that can be achieved by publishing to the 
internet haven't yet done anything to bring down the costs of 
scholarly journals.  One reason is that most journals still 
have print editions whose costs are unaffected by the internet 
revolution.  Another reason is that the online editions of 
most journals use expensive software to permit access to 
paying subscribers and block access to everyone else.  The 
internet is only a revolutionary medium of nearly costless 
dissemination for those who don't manage subscription lists 
and don't try to distinguish between authorized and 
unauthorized readers.   
 
There are other dimensions to the scholarly publishing 
crisis.  One is that journal publishers (like software 
publishers) are moving beyond copyright law to licensing 
contracts give them even more protection.  Publishers don't 
let libraries "buy" or "own" copies of electronic journals, 
but only "license" them.  As a result, libraries aren't 
assured that they have long-term access rights to these 
journals, they have diminished rights to lend their copies, 
and their patrons have diminished fair-use rights.  They are 
getting much less and paying much more. 
 
If there were no alternative, that would be one thing.  But 
there is an alternative to the near monopoly concentration in 
the scholarly publishing industry.  There is an alternative to 
harsh licensing contracts.  And above all, the internet gives 
us an alternative method of dissemination that widens 
distribution and lowers cost at the same time.  Even if there 
were no crisis, the opportunity afforded by the internet would 
be too beautiful to ignore.  Given the crisis, it's 
inexcusable.  
  
Q: What is Free Online Scholarship and how can it be 
reconciled with rights to intellectual property? Can the 
current revenue models of publishers be replaced with viable 
alternative revenue models - and, if yes, which are they? What 
the risks of abuse of FOS? Is FOS an instance of a larger 
"free content" movement (Napster, etc.)? If so, can Free 
Online Content principles be applied to music, books, and 
film. for instance? 
 
A: Free online scholarship is scientific and scholarly 
literature which is made available free of charge on the 
internet.  The FOS movement singles out this body of 
literature not because it is useful (because other kinds of 
literature are useful too), but because it has the relevant 
peculiarity that its authors don't expect to be paid.  If 
authors want to make money from their works, we don't 
criticize or pressure them.  But when authors consent to do 
without royalties, then there's no reason not to make their 
writings freely available on the internet.  When the 
literature is as useful as research articles are, then free 
online access is a public good  worth every effort to realize. 
 
Once we understand that the scope of the FOS movement is 
limited to works that authors consent to give away, or to 
publish without payment, then we can understand why this 
movement is completely compatible with intellectual property 
rights.  When authors write articles, they are the copyright 
holders.  A growing number of journals will use their peer 
review process to vet and validate articles, and ultimately 
publish them, without demanding that authors give up copyright 
--and we hope to launch more journals with this enlightened 
policy.  If the authors of peer-reviewed articles holds the 
copyright to them, then they have the right to decide whether 
to make access free or restricted.  If they choose to make it 
free and open, that is their right, not an infringement of 
their right.  The FOS movement is about using copyright to 
authorize free and open access, not about piracy that creates 
free access without the consent of the copyright holder. 
 
This movement has nothing interesting in common with the 
movement created by Napster.  The all-important difference is 
that researchers give away their journal articles and 
musicians don't give away their music.  We work entirely 
within the consent of the copyright holder. 
 
Q:  The major missing element seems to be 
perceived respectability. But there are others. No agreed upon 
content or knowledge classification method has emerged. Some 
web sites (such as Suite101) use the Dewey decimal system. 
Others invented and implemented systems of their making. 
Additionally, one click publishing technology (such as 
Webseed's or Blogger's) came to be identified strictly with 
non-scholarly material: personal reminiscences, 
correspondence, articles, and news. Above all, no feasible 
alternative revenue models seem to have emerged.
 
A: Regarding respectability:  There is a growing number of 
free online *peer-reviewed* journals, and growing number of 
highly respected academics willing to serve on their editorial 
boards.  As measured by impact (citations) or informal 
prestige, some online journals surpass many print journals.  
It's true that print journals still have greater impact and 
prestige than online journals, but only if we average the two 
classes.  The factors that create respectability are medium-
independent, and can  easily belong to online journals.  A 
growing number of online journals are as respectable as any 
print journal. BMJ (formerly called the British Medical 
Journal) is eminently respectable.  It offers 100% of its 
print copy online free of charge.  There are other examples in 
every field. 
 
My view is that the lack of an agreed upon classification 
method is not a problem.  That's a long conversation.  But 
it's not true that the need for such a classification method 
is widely felt.  Indexing and organization are desirable, but 
there is free and priced software to index and organize any 
online content in any way that users want.  This software will 
only get better as time goes on. 
 
It's not true that no feasible alternative revenue models have 
emerged.  FOS doesn't depend on volunteer labor.  The general 
revenue model is to pay for outgoing articles (dissemination) 
rather than incoming articles (access).  There are many 
variations on the theme, depending on who pays.  But it's 
perfectly feasible to regard the costs of dissemination as 
part of the cost of research, to be paid by the grant that 
funds the research --for example.  (This is just one variation 
on the theme.)  BioMed Central is a *for-profit* provider of 
FOS implementing one variation on this theme.


 
 
In a general introduction to the FOS movement I'm writing for 
another journal, I'm putting it this way.  The economic 
feasibility of FOS is no more mysterious than the economic 
feasibility of Public TV.  Donors pay the costs of 
dissemination so that it will be free for everyone.  For that 
matter, it's no more mysterious than the economics of 
commercial TV, which is identical except that advertisers are 
among the donors.  There are many successful and sustainable 
examples in our economy in which some people pay to make a 
good free for everyone rather than pay only for their own 
private access or consumption. 
  
Q. Can you summarize for us the major developments and trends 
in FOS?  
 
A: Here are some trends in the FOS movement: 
 
A growing number of disciplines have free online preprint 
archives. Every discipline now has a growing number of free 
online peer-reviewed journals. A growing number of 
universities have free online archives for faculty research 
papers. Journal publishers are experimenting with ways to 
offer more of their content online, some of it free of 
charge.  They are also experimenting with different ways to 
fund the costs of the online content. More journal publishers 
are allowing authors to put their published papers online free 
of charge e.g. on their own home pages.  It is increasingly 
common to see journal editors rebel against journal publishers 
that refuse to lower subscription prices or widen online 
access.  They rebel by resigning and launching new journals on 
the same topics and usually gather the same subscribers and a 
superior "impact factor" very quickly. More scholars and 
researchers are demanding that journals offer free online 
access to their contents.  The Public Library of Science open 
letter has so far gathered more than 29,000 signatures from 
175 countries. More online repositories of digital articles 
are participating in the Open Archives Initiative, and more 
scholars and task forces are endorsing it.  It is the emerging 
standard for making separate archives "interoperable" --for 
example, searchable as if they were one. More serious, 
feasible solutions are emerging to the problem of long-term 
preservation of digital content. More journals and special 
initiatives are seeking ways to provide developing countries 
with free online access to scientific and scholarly 
literature. More software tools exist to automate the 
operation of online journals (hence, to keep costs low).  Just 
about all tasks can now be automated except editorial judgment 
(which shouldn't be, of course).  More hiring and tenure 
committees are giving weight to peer-reviewed publications 
without regard to the medium of publication (print or 
electronic). More journal publishers are seeking ways to 
accommodate the scholarly demand for online access (though not 
always to accommodate the demand for free online access). The 
serials pricing crisis which has long alarmed and mobilized 
librarians is starting to alarm and mobilize university 
administrators and faculty. Copyright law is changing from a 
balance between publishers and readers toward a severe 
imbalance favoring publishers.  (See next question below.) 
 
The recent Budapest Open Access Initiative (BOAI) is promising 
for several reasons.  It brings together FOS proponents from 
many disciplines and nations, FOS initiatives from many 
fronts, and foundations with serious resources to help advance 
the cause.  These foundations are led by George Soros' Open 
Society Institute, which convened the meeting that gave birth 
to the BOAI.


 
 
One thing I like about the BOAI is its friendliness.  It 
doesn't demand that journals or publishers join the cause or 
face sanctions.  It offers to help them make the transition if 
they are willing to do so.  But if they aren't willing, it 
simply says it will pursue the cause without their help.  The 
BOAI doesn't demand any changes from publishers, markets, or 
legislation, and doesn't criticize anyone for not joining.  It 
articulates two strategies that scholars can pursue on their 
own.  One is self-archiving, by which scholars deposit their 
papers in institutional or disciplinary archives.  (These 
archives are interoperable, or they cooperate with one 
another, by virtue of their compliance with the standards of 
the Open Archives Initiative.)  The second is the launch of a 
new generation of journals that are committed to making their 
contents freely accessible online.  
 
The long-term economic sustainability of free online 
scholarship is not a problem.  We know this because creating 
open online access to this literature costs much less than 
traditional forms of dissemination and much less than the 
money currently spent on journal subscriptions.  The only 
problem is the transition from here to there.  The BOAI is 
especially promising because it understands this and mobilizes 
the financial resources to help make the transition possible 
for existing journals that would like to change their business 
model, new journals that need to establish themselves, and 
universities that don't yet participate in self-archiving.  In 
this sense the BOAI is not just a statement of principles or 
ideals, but a serious and effective plan to achieve this very 
important public good. 
  
Q. Copyright laws are being revamped the world over (but 
mainly in the USA). What would be the impact of the likes of 
the DMCA on scholarship and on the economics of publishing? 
 
A. The DMCA has several harmful consequences for scholarship.  
First, it prevents some scientists who happen to specialize in 
encryption and data security from publishing their research.  
Edward Felten of Princeton has so far been unable to get a 
court to declare that he has a First Amendment right to 
publish his research on certain methods of copy protection.  
Taken at face value, the DMCA would punish Felten for 
publishing his research.  Until courts settle the question 
whether the relevant sections of the DMCA are constitutional, 
the free expression rights of scholars like Felten will be 
chilled.  And of course if the question is resolved in favor 
of the DMCA, then the free expression rights of scholars like 
Felten will be repealed.  Second, it prevents some computer 
scientists from publishing their research in the form of 
source code, the technical language of their field.  While 
some courts have held that source code is protected as a kind 
of speech, other courts are giving it a low level of 
protection in order to give effect to DMCA prohibitions on 
certain kinds of software.  Third, it supports strong copy-
protection schemes that deprive readers of their fair-use 
rights.  For the same reason, it deprives purchasers of 
digital content of the right to bypass copy protection in 
order to make personal back-up copies or to keep the content 
readable when they move to a new computer.  For the same 
reason, it prevents libraries from taking necessary measures 
to assure the long-term access and preservation of digital 
literature. The DMCA is even worse for software developers and 
consumers than it is for scholars. This week Felten dropped 
his appeal.  So currently no court is even considering his 
question whether scholars have a First Amendment right to 
publish their research, or whether the anti-circumvention 
clause of the DMCA (which seems to prohibit Felten from 
publishing) is unconstitutional.

Note that the FOS movement has no problem with the strong 
protection of intellectual property, which is at the heart of 
the DMCA.  That's not the problem.  The problem is the way the 
DMCA upsets a long-standing (and constitutionally mandated) 
balance between publishers and readers and gives nearly 
everything to publishers.  
 
Because internet content crosses national boundaries, one 
nation will often want to enforce the copyright judgments of 
its own courts, interpreting its own laws, in another 
country.  Worldwide developments in parallel to the DMCA, like 
the still evolving Hague Convention on Jurisdiction and 
Foreign Judgments, are giving effect to these desires.  The 
problem is that these efforts, like the DMCA, put intellectual 
property rights above free speech rights.  The same rules that 
let a nation enforce a copyright judgment beyond its own 
boundaries also let it enforce a censorship judgment beyond 
its own boundaries.  Until recently, the border-crossing 
potential of the internet was a feature; now it's a bug.  
Until recently, it subjected less-free nations to the free 
speech of the most-free nations.  New developments threaten to 
subject the most-free nations to the censorship rules of the 
least-free nations.  In the name of copyright enforcement, 
worldwide speech rights are sinking to the lowest standard in 
use anywhere. 
 
Another development in copyright law that harms scholarship is 
the extension of copyright terms, even retroactively.  The 
Sonny Bono Copyright Extension Act (1998) retroactively added 
20 years to existing copyrights.  This harms scholarship by 
greatly delaying the transition of copyrighted works into the 
public domain.  By shrinking the public domain, it shrinks the 
number of modern classics that volunteers can lawfully 
digitize and make freely available on the internet.  For the 
same reason, it tilts the balance of copyright law even 
further in the direction of publishers and against the 
interests of readers and researchers.  Those who have looked 
into it believe that the Bono Act was motivated to protect the 
Disney copyright on Mickey Mouse, which would have expired in 
2003.  If so, this is a grotesque inversion of values.  The 
Uruguay Round Agreements Act (1994) is even worse, and can 
remove works from the public domain and retroactively grant 
them copyrights.   
 
In short, whatever harms the rights and interests of readers 
harms scholarship and research, and recent trends in copyright 
law increasingly favor the rights and interests of publishers 
over those of readers.  Copyright law is increasingly hostile 
to fair-use rights, the first sale doctrine, limited terms, 
and the public domain.   
  
Q. To summarize: is the Internet a boon or a bane as far as 
publishing and scholarly exchange are concerned? It would seem 
that its existence brought about the RETARDATION of users' 
rights - rather than the user empowerment everyone was hoping 
for. 
  
A. The Internet is an unprecedented boon to scholarly 
publishing.  The only problem is that we have barely begun to 
realize its full potential, including its potential to make 
scholarly literature freely available to everyone with an 
internet connection.  We may never take full advantage of the 
ways it can transform scholarly research and publication.  
That requires an endless approximation process, deep 
imagination, and time.  But if we could just take advantage of 
the opportunity it affords for free online research 
literature, then the internet will have a greater beneficial 
impact on research and education than lending libraries or the 
Gutenberg press.


The Second Gutenberg
Interview with Michael Hart
By: Sam Vaknin
Also published by United Press International (UPI)

"Michael Hart, founder of Project Gutenberg is a visionary who 
was quite ahead of his time. In fact, it may still be several 
years before his dream of universally-available literature 
comes true. Nevertheless, Michael's efforts have inspired 
thousands of people around the world who now share his vision. 
The progress of Project Gutenberg has been slower than many 
hoped, but it has definitely helped to push forward the great 
eBook  dream which I share. Unfortunately, the technology, 
infrastructure, and market are lagging way behind Michael's 
vision, a common hazard of being a pioneer." - says Glenn 
Sanders, Director of eBookWeb.org.
Michael S. Hart is a Professor of Electronic Text at 
Benedictine University (Illinois, U.S.A.) and a former 
Visiting Scientist at Carnegie Mellon University was a Fellow 
of the Internet Archive for the year 2000. He founded Project 
Gutenberg in 1971 and is currently its Executive Coordinator.
In more ways than one, he is the father of e-publishing and e-
books. He pioneered not only the dissemination of electronic 
texts - but also some of the working models that underpinned 
the Internet until the dot.com crash two years ago. 
The ethos of the early Internet owes a lot to Hart. He created 
a mass movement of volunteers, remote-collaborating on a 
project of free access to content. There is no better 
encapsulation of the gist of the Net. And PG books can be 
replicated at no cost - a precursor of viral and buzz 
marketing.
Project Gutenberg is, by now, an integral part of the myth and 
history of our networked world. It is a worldwide library 
created and maintained by a small army of dedicated volunteers 
who scan, proofread, and upload dozens of new e-texts every 
week. Most of these texts are in the public domain.
But a few are copyrighted - with permission to store the work 
granted by authors and publishers or other copyright holders. 
There are many imitators and copycats - but only one Project 
Gutenberg, in scope, perseverance, dedication, and 
thoroughness.
As copyright expires, thousands of works are added monthly to 
the public domain and can be freely replicated and 
distributed. Most of these books are out of print and saved by 
the Project from obscurity and ultimate oblivion. 
The recurrent extension of copyright terms by Congress hampers 
this work by restricting the growth of the public domain or 
even by removing texts from it. It benefits very few copyright 
holders at the expense of universal access to literature and 
knowledge.
Hart mourns the rapidly dwindling public domain: 
"In the USA, no copyrights will expire from now to 2019!!! It 
is even much worse in many other countries, where they 
actually removed 20 years from the public domain. Books that 
had been legal to publish all of a sudden were not. Friends 
told me that in Italy, for example, all the great Italian 
operas that had entered the public domain are no longer there. 
. . 
Same goes for the United Kingdom. Germany increased their 
copyright term to more than 70 years back in the 1960's. It is 
a domino effect. Australia is the only country I know of that 
has officially stated they will not extend the copyright term 
by 20 years to more than 70."
Hart is a visionary and a pioneer. Such vocations carry a 
heavy price tag in recurrent frustration and cumulative 
exhaustion. Hart may be tired, but he does not sound bitter. 
He is still a fount of brilliant ideas, thought provoking 
insights, exuberant optimism, and titillating predictions. 
Three decades of constant battle ended in partial victory - 
but Hart is as energetic as ever, straining at the next, 
seemingly implausible target. "A million books to a billion 
people in all corners of the globe."
Inevitably, he sometimes feels cornered. "They" figure in many 
of his statements - the cynical and avaricious establishment 
that will sacrifice anything to secure the diminishing returns 
of a few more copies sold. In the Project's life time, the 
period of copyright has been extended from an average of 30 
years to an inane 95 years.
Moreover, no notice of renewal is required in order to enjoy 
the copyright extensions.
This protectionism hinders the spread of literacy, deprives 
the masses of much needed knowledge, discriminates against the 
poor, and, ultimately, undermines democracy - believes Hart.
Q. Project "Gutenberg" is a self-conscious name. In which ways 
is the Project comparable to Gutenberg's revolution?
A. When I chose the name, the major factor in mind was that 
publishing e-Books would change the map of literacy and 
education as much as did the Gutenberg Press which reduced the 
price of books to 1/400th their previous price tag. From the 
equivalent of the cost of an average family farm, books became 
so inexpensive that you could see a wagonload of them in the 
weekend marketplace in small villages at prices that even 
these people could afford.
My second choice was Project Alexandria. The major difference 
is that the Alexandrians *collect* e-Books, while the 
Gutenbergers *produce* e-Books.
Another way our Project compares to Gutenberg's revolution is 
that copyright laws were created to stop both.
When we only had a dozen e-Books online, the price of putting 
one on a computer was about 1/400th the price of a paperback. 
But obviously with 100 gigabyte drives coming down to $100, 
the price of putting e-Books on computers has fallen so low as 
to be literally "too cheap to meter." Those who like to meter 
everything on the cash scale are incredibly upset about 
Project Gutenberg. 
Project Gutenberg is the first example of a "paradigm shift" 
from "Limited Distribution" to "Unlimited Distribution", now 
touted as "The Information Age". However, you should be aware 
that this is the 4th such Information Age. 
Each such phase has been stifled by making it illegal to use 
new technologies to copy texts. In 1710, the Statute of Anne 
copyright made it illegal for any but members of the ancient 
Stationers' Guild to use a Gutenberg Press. Then, in 1909, the 
US doubled the term of all copyrights to eliminate "reprint 
houses" who were using the new steam and electric powered 
presses to compete with the old boy publishing network.
The third Information Age came in 1976 when the US increased 
the copyright term to 75 years and eliminated the requirement 
to file copyright renewals, to stifle changes brought on by 
Xerox machines. In 1998, the US extended the copyright term 
yet again, to 95 years, to eliminate publication via the 
Internet.
Q. The concept of e-texts or e-books back in 1971 was novel. 
What made you think of this particular use for the $100 
million in spare computer time you were given by the 
University of Illinois?
A. What allowed me to think of this particular use for 
computers so long before anyone else did is the same thing 
that allows every other inventor to create their inventions: 
being at the right place, at the right time, with the right 
background.
As Lermontov said in The Red Shoes: "Not even the greatest 
magician in the world can pull a rabbit out of a hat if there 
isn't already a rabbit in it."
I owe this background to my parents, and to my brother. I grew 
up in a house full of books and electronics, so the idea of 
combining the two was obviously not as great a leap as it 
would have been for someone else. I repaired my Dad's hi-fi 
the first time when I was in the second grade, and was also 
the kid who adjusted everyone's TV and antennas when they were 
so new everyone was scared of them. 
I have always had a knack for electronics, and built and 
rebuilt radios and other electronics all my life, even though 
I never read an electronics book or manuals. . .it was just 
natural.
Let me tell you a story about how the Project started:
I happened to stop at our local IGA grocery store on the way. 
We were just coming up on the American Bicentennial and they 
put faux parchment historical documents in with the groceries. 
So, as I fumbled through my backpack for something to eat, I 
found the US Declaration of Independence and had a light bulb 
moment.
I thought for a while to see if I could figure out anything I 
could do with the computer that would be more important than 
typing in the Declaration of Independence, something that 
would still be there 100 years later, but couldn't come up 
with anything, and so Project Gutenberg was born.
You have to remember that the Internet had just gone 
transcontinental and this was one of the very first computers 
on it. Somehow I had envisioned the Net in my mind very much 
as it would become 30 years later.
I envisioned sending the Declaration of Independence to 
everyone on the Net. . .all 100 of them. . .which would have 
crashed the whole thing, but luckily Fred Ranck stopped me, 
and we just posted a notice in what would later become 
comp.gen
I think about 6 out of the 100 users at the time downloaded 
it. . . .
Q. Between 1971 and 1993 you produced 100 e-texts. And then, 
in less than 9 years, an additional few thousand. What 
happened?
A. People rarely understand the power of doubling something 
every so often.
In 1991 we were doing one e-Book per month. This was totally 
revolutionary at the time. People kept predicting that we 
couldn't continue, but we were planning on doubling production 
every year, which we did for most years. We are now adding 200 
e-texts a month.
Q. Can you give us some current download statistics?
A. As for stats, this is pretty much impossible since we don't 
directly control any but one or two of what I presume are 
hundreds of sites around the world that have our files up for 
download. What I can tell you is that the one site we have the 
most control of gives away over a million e-Books per month.
Q. The Internet is often castigated as an English-language, 
affluent people's toy. PG includes predominantly English 
language, Western world, texts. Do you intend to make it more 
multicultural and multilingual?
A. I encourage all languages as hard as I possibly can.
So far we have English, Latin, French, Italian, German, 
Spanish, Chinese, Japanese, Swedish, Danish, Welsh, 
Portuguese, Old Dutch, Bulgarian, Dutch/Flemish, Greek, 
Hebrew. We have texts in Old French, Polish, Russian, 
Romanian, and Farsi in progress.
I wonder if we should count mathematics as a language?
I was surprised at how many people were interested when we 
first uploaded Pi to a million places. . .
Q. Why are stand-alone images (e.g., films, photographs) and 
sound excluded or rare?
A. We have tried some, but haven't received much feedback. 
Still, we will continue to experiment with all formats.
Also, these files are total hogs for drives and bandwidth.
Our short movie of the lunar landing is twice as big as 
Shakespeare and the Bible combined in uncompressed format. 
It's only a couple minutes long, and low-resolution. Think how 
big a whole movie would be, even not at hi-resolution. It 
would take up a couple CD- ROMs. . . .
Q. PG now makes files available as DOC/RTF and HTML - as well 
as plain vanilla ASCII. Yet, plain text delivery seemed to 
have been a basic tenet of the Project. What made you change 
your mind?
A. We're willing to post in all kinds of file formats, but the 
only format everyone can read is Plain Vanilla ASCII, so we 
always try to include that. PG has been available on CDs for 
years.
Q. The failure of the advertising-sponsored revenue model 
forces Internet-based content generators and aggregators to 
charge for their wares. Will PG continue to be free - and, if 
so, how will it finance itself? Example: who is paying for the 
hosting and bandwidth now?
A. It's all volunteer. . . . And the number of sites continues 
to grow, and to reach more and more regions around the world 
for easier local access.
Actually, all the hosting, bandwidth, etc. are voluntary, too. 
However, we desperately need donations to do copyright 
research, cataloging, to hire librarians and Library and 
Information Science professors, to support the Project 
Gutenberg spin-offs in other languages and countries, not to 
mention mundane things such as phone and utility bills, 
computers, drives, backups, etc. We need volunteers equally 
desperately. 
Volunteering is perhaps the only way for one person to work 
for a week or a month on a book and get it to a hundred 
million people. . . .
Q. The reaction to e-books fluctuates wildly between euphoria 
and gloom.
A. This is only the commercial point of view. . . They want to 
take it over or sink it to the bottom. . .There are no other 
commercial perspectives. Between 1500-1550, thanks to the 
Gutenberg Press, more books were printed than in all of 
history previous to Gutenberg. I have hopes like that for e-
Books. . . .
Q. Some say that e-books are doomed, having miserably failed 
to capture the public's imagination and devotion. Others 
predict a future of ubiquitous, ATM-printed, e-books, replete 
with olfactory, tactile, audio, and 3-D effects. What is your 
scenario?
A. The main trouble with these predictions is not only that 
they are made solely with the commercial aspects in mind, but 
that they are made by an assortment of people from pre-e-Book 
generations, who have no idea that you could use the same 
gizmo to play MP3s as to read or listen to e-Books.
The younger generations have no doubt about e-Books.
It's only the dinosaurs that have no idea what's going on. We 
are still getting email stating that not one person is ever 
going to read books from computers!
Who will be the more well-read - those who can carry at most a 
dozen books with them, or those who have a PDA in their pocket 
with a hundred or more e-Books in it?
Who will look up more quotations in context? Who will use the 
dictionary more often? Who will look up geographical 
information more often?
These are all things I do with my little antique PDA and the 
new ones are already a dozen times more powerful.
I want to tell you the story of when I first realized that 
Project Gutenberg was going to work. It was about 10 years 
before we published our 2,000th E-text. We had only about a 
dozen e-books online. At the beginning of 1989 there were only 
80,000 host computers in the entire Internet - though by 
October that year the number had doubled.
I was on the phone one day, with the Executive Director of 
Common Knowledge, a project to put the Library of Congress 
catalogs into public domain MARC (Machine Accessible Record 
Catalog) records. During the conversation, there was this huge 
noise. She dropped the phone and ran off. She was back in a 
minute, and laughing her head off, she told me:
Her son had been playing around with her computer, and found 
this copy of Project Gutenberg's "Alice in Wonderland" and had 
started to read it. He mentioned this at school, and a few of 
the kids followed him home to see it. The next day even more 
kids followed.  Eventually the number of kids grew so great 
that they were hanging off this huge oak chair.
Eventually this oak chair had so many kids all over it, 
reading "Alice in Wonderland"...that it literally separated 
into all its parts and kids went tumbling in all 
directions....At that very moment, in 1989, I realized that E-
books were going to succeed, no matter what any of a number of 
adults thought. To the next generation, this will be how they 
remember Alice in Wonderland, just as my memory of it was a 
golden inscribed red leather edition my family used to read 
from together.
Four years later, in 1993, there were still under 100 Project 
Gutenberg e-Books.
A neighbor dropped by to talk to me one day and in the course 
of the conversation mentioned he had read the Project 
Gutenberg Alice in Wonderland. I had no idea his interests 
even included computers. He had found a few errors. I hurried 
home to correct them and to put the new edition online. 
At first I was in happy shock just because I could improve our 
edition, but then it occurred to me that perhaps the more 
important aspect was that someone I knew had downloaded Alice 
all on his own, then read the entire book from "cover to 
cover" on his computer thus putting paid to the naysayers who 
said no one my age would read e-Books.
There are lots of stories like this: professors who tell me 
their students will not read paper textbooks, Texas preparing 
for all textbooks to be e-Books. . . .
Q. PG is a prime example of two phenomena characteristic to 
the early Internet: collaborative efforts and volunteering. 
With the crass commercialization of the Net - will people 
continue to volunteer and collaborate - or will corporate, 
brick and mortar, behemoths take over?
A. Well, the commercialization of the Web started in 1994, and 
that didn't wipe us out. It took us 30 years to do our first 
5,000 e-Books, and I'll bet you a pizza that it will only take 
30 months to do our second 5,000!!! Then we write up a 
schedule for 1,000,000!!!!!!!
Q. In other words: PG is the reification of the spirit of the 
Internet.
A. Definitely. . .So was "Ask Dr. Internet", another of my 
personas. . .
Q. Should the Internet change dramatically - what will happen 
to PG? Will you ever consider going commercial, for instance? 
If not, how do you plan to adapt?
A. Why should we go commercial. . .that just invites a 
downfall if the money goes away. Which they would love to 
happen -and would probably encourage it. It's hard to kill off 
something that doesn't have a physical plant or a budget. . 
.and cannot be bought. We will adapt by doing the entire 
public domain, including graphics, music, movies, sculpture, 
paintings, photographs, etc. . . .
Q. PG makes obscure and inaccessible texts as well as seminal 
works - easily and globally available. Doesn't this lead to an 
embarrassment of riches or to confusion? In other words: all 
PG e-texts are "equal". It is a "democratic" system. There is 
no "text rating", historical context, peer review, quality 
control, censorship ...
A. This is because I am not a very bossy boss. . .I encourage 
our volunteers to choose their own favorites, not just what 
"I" think they should do. However, I am sure we will get all 
the warhorses done.
Q. The e-texts posted on PG are copyright free or with 
permission from their authors and publishers. How do you cope 
with the inordinately extended copyright period in the USA?
A. I just finished up years of working on an Amicus Brief for 
the Supreme Court in the hope of overturning the latest 
copyright extensions. As for coping, you just do the best you 
can with the cards you are dealt.
Q. What are the effects of such legislation on public 
literacy?
A. The US used to say we would send aid to the entire world, 
in the form of food, clothing, medical supplies, as much as we 
could afford. But now that literacy can be disseminated at no 
expense, we refuse to do it by pretty much stifling the public 
domain.
Q. PG has a mirror site in Australia where copyright law is 
less stringent.

A. Actually, they are a totally separate organization, using 
our name with permission, just as does the Gutenberg Projekt-
DE in Germany.
Q. Are such "backdoors" the solution? What about the DMCA 
(Digital Millennium Copyright Act)?
A. I am so a-political that you could call me anti-political. 
I would prefer a copyright of 10 years or so. . 
Only the biggest of the best sellers might make 10% more after 
10 years, and they don't need it.
Do we really want laws that support only the biggest and 
richest?
I love "The Bridges of Madison County", but I don't think 95 
years, or even 75 years, or even 56 years of corporations, 
family and other heirs should be supported by it. It then 
becomes the "Duchy of Madison County" and we are stuck with 
generations of "Dukes of Madison County."
What we will end up with under these copyright laws is a 
"landed gentry of the information age" who just keep 
inheriting ...
Copyright should expire soon enough that the authors, if they 
want to keep getting paid, have to come back to work again.
After all, there is no other job in the world in which one 
piece of work can keep paying off for 95 years.
By the way, do you realize that Ted Turner made millions, 
probably hundreds of millions, from the copyright extension of 
just "Gone With The Wind", not counting the hundreds of other 
movies he owns. . .all from one vote of Congress. . . . .
Congress should not be allowed to write laws that create 
windfall profits for 1% of the population and take away a 
million books from all the rest.
Q. What does PG intend to do about the legislative asymmetry 
between content producers and creators - and content 
consumers? Lobby Congress? Testify? Protest? Organize 
petitions? Place "Gone with the Wind" on the Internet and wait 
for a show trial?
A. PG Australia already has done Gone With The Wind, as their 
50th e-Book, that's good enough for me at the moment.
Eldred v. Ashcroft was originally drafted as Hart V. Reno, but 
the lawyers, Lessig & co, wouldn't include one word of mine in 
the case, so I fired them.
Q. Gutenberg texts are sometimes used as freebies within a 
commercial (Monolithic, Wallnut Creek) or semi-commercial 
product (such as the Public Domain Reader). Is this 
acceptable? Why don't you charge them a license fee?
A. Walnut Creek PG CD's weren't free and they sent us nice 
donations. The commercial outfits have to pay for a license, 
the non- commercial ones usually don't. Each case is 
separately decided. While we don't do any ads on our sites, we 
don't insist that others don't.
Q. Technology is often considered the antonym of "culture". 
TV, for instance, is berated for its vulgar, low-brow, 
programming. Hollywood is often chastised for its indulgence 
in gratuitous violence and sex.
A. No one ever went broke underestimating the intelligence of 
their audience. As long as these are "commercial applications" 
that's what you will get. What else could you possibly expect? 
These are all examples of "capitalism gone awry".
By the way, I'm not anti-capitalism, I really am an Ayn Rand 
freak, figure that out. . .hee hee!
I am doing Project Gutenberg for the most selfish of reasons - 
because I want a world that has Project Gutenberg in it.
Q. E-books are equated with low-quality vanity publishing. 
Yet, PG seems to embody the conviction that technology can do 
wonders for the dissemination of culture, literacy, democracy, 
civil society and so on.
A. e-Books do wonders for the dissemination of culture, 
literacy, democracy, civil society and so on. You do realize 
that the Declaration of Independence is/was the FIRST man-made 
item in all of history that everyone can have, in as many 
copies as they want. Do you realize that a 5 gigabyte section 
of a hard drive can hold a million copies of that file, 
uncompressed?
Terabyte drive systems are already available for only around 
$2,500. Ten years from now 5T hard disk partitions will be 
able to hold a billion copies.
Q. Are you a romantic believer in the power of technology to 
bring progress?
A. Well, I'm certainly an incurable romantic, and I believe 
that technology can bring progress, but I don't know if they 
are, or have to be, related. . . .
Q. And do you see any dangers in e-books and freely available 
e-texts (e.g., hate speech)?
A. Once you start censoring, you are playing with Pandora's 
Box. Just look at what they are doing with Little Black Sambo, 
who wasn't even black, and with Uncle Remus, who was? This is 
awful. "Song of the South" was required viewing when I was in 
school and now I can't even show this generation what we were 
required to study when I was a kid. . .1984 really did arrive. 
. . .
Q. In some ways, you "compete" directly with other bastions of 
education - libraries and universities. How do you get along? 
What about other repositories of knowledge such as Project 
Bartleby? Governments?



A. Actually, we cooperate with them, not compete with them. We 
make all our files available to them and encourage them to 
make the texts available to everyone. Some of them view this 
as competition, but we don't. Some prefer to control 
distribution. . .to be a gate that they can open and close at 
will. . .We prefer the doors always to be open.
Have you ever considered why, with the hundred millions of 
dollars granted to found e-Libraries at the major universities 
some ten years ago, and undoubtedly hundreds of millions more 
donated since then, why you are doing an interview with 
someone sitting at a basement, running computer hardware and 
software that is 10 and 20 years old?
If any college, or company, much less university, city, 
county, state or country was willing to do this, you would 
have never heard of me.
Q. What has been the personal cost? It must have been 
frustrating and exhausting and elating and rewarding ... In 
retrospect: are you happy with it? Would you have done it 
again?
A. I can't think of anything more rewarding to do as a career 
than Project Gutenberg. It is something that will reach more 
people than any other project in all of history. It is as 
powerful as The Bomb, but everyone can benefit from it. And it 
doesn't make a decent weapon. It doesn't cost anyone anything 
and it is the very first, though obviously primitive, example 
of The Neo-Industrial Revolution, when everyone can have 
everything - though they are sure to pass a law against it.
I said this in 1971, in the very first week of PG, that by the 
end of my lifetime you would be able to carry every word in 
the Library of Congress in one hand - but they will pass a law 
against it. I realized they would never let us have that much 
access to so much information. I never heard that they passed 
the copyright extension 5 years later. It was pretty much a 
secret, just as is the current one, unless the Supreme Court 
strikes it down. Only then will it make the news.
Congress passed that copyright law together with impeachment 
proceedings of President Clinton, just to make sure it never 
made the news.
As far as the cost, the happiness, the frustration - I am a 
natural born workaholic and idealist, so I overcome the 
technical frustrations. It's the social frustrations that are 
the hardest to deal with, the people who want permanent 
copyright, even though the extensions are already bringing 
about "The Landed Gentry of the Information Age."
Q. Any thought about the future?
Precedents set by the Sonny Bono Copyright Law could well have 
an enormous unpredicted effect on computer applications of the 
future. One such application is the "printing" of solid three 
dimensional objects, often referred to as Rapid Prototyping, 
or RP. These printers have been with us since the 1980's and 
now are in a price range of the 5 megabyte hard drives on the 
first computer to house Project Gutenberg in 1971. If you 
count the inflation factor, they obviously are much more 
affordable.
In addition to cost reductions, these 3-D printers now can 
print on a variety of materials. The list of printable 
substances should expand over the years until we can 
eventually print out actual working items, rather than the 
models we print out today.
Given that very inexpensive printers today can print in 
millions of colors, and that color computer printers were 
pretty much non-existent 30 years ago, we should at least 
consider the possibility that printers 30 years from now might 
be able to "print" on an extremely wide variety of materials, 
and that someday we will be able to "print out" a car and 
drive it away.
This copyright law covers 95 years. Let's look back to 95 
years and see the "copyright" to what things we may want to 
print out would have just now expired:
1. The Wright-Brothers' airplane and blueprints.
2. A dozen brands of early automobiles.
3. Everything Edison invented until he was nearly 60.
Obviously there are many more.
The point here is that under current intellectual property 
law, it would be difficult to print out anything invented 
today that reached the market in two years - until 2100, a 
time when these items would no longer have any use.
When the Star Trek Replicators become a reality, will it be 
illegal to actually use them?
Will all food items be Genetically Manipulated Organisms so 
that it will be impossible to find natural foods that could be 
copied?
When I grew up in Washington state, there were plenty of wild 
blackberries, raspberries, apple trees, pear trees, plum 
trees, grapes. I never even considered buying any of these at 
a store. But today there has been a serious effort to 
discourage free food supplies, and not only in Washington, but 
also in most other states.
Last night at dinner, one of our volunteers remarked that he 
expected that by the end of his lifetime he might be eating a 
dinner of replicated food. I pointed out that by that time - 
"they" would make it very difficult to find any kind of food 
not protected against replication by intellectual property 
laws and that THAT was one of the major reasons for extending 
copyright, so that WHEN it would be possible for everyone to 
be well-read & well-fed, they will have made it illegal to do 
so.
The trend is that everything should cost something. In some 
places there are even machines that dispense a breath of fresh 
air. . .for a price.
Do we really want to create a civilization in which everything 
has a price. . .when there are machines that could copy 
anything?


The E-Books Evangelist 
Interview with Glenn Sanders
By: Sam Vaknin, Ph.D.
Also published by United Press International (UPI)

Q. Why electronic publishing? 
A. I was first introduced to electronic publishing on the 
Internet in the late 1980s and became intrigued by the power 
of this revolutionary development. Then, when Mosaic released 
the first Web browser in 1992, the Internet finally had a 
visual aspect. Suddenly, the vast Internet was transformed 
from a dimly lit warehouse for data storage and exchange, to a 
visible library and gallery for information. I was hooked. 
 
In 1994, while teaching at a university in Japan, I created 
what was probably one of the first (if not the first) 
paperless reading classes. I taught myself HTML and built 26 
Web-based reading lessons for the "comparative cultures" 
course I taught there. The reading material in each lesson 
linked to related websites and information. Instructions were 
included for the exercises, which usually included finding 
information or doing research somewhere on the Web. Students 
emailed their results to me, and I emailed feedback and grades 
to them. Students were not required to come to class, but were 
required to turn in their "class work" results to me by Friday 
evening. 
 
Since then, I have created numerous Web sites, published a 
number of electronic & print books, and hundreds of 
articles. In the late 1990's I saw the confluence of three 
factors that foretold the electronic publishing and e-book 
revolution. The first was the imminent ubiquity of the 
Internet. Next, was the growing need for mobile access to 
information, and the availability of so much data in the 
digital domain.  
Finally, I could see the day when technology would catch up 
with my vision of a portable information tablet. As of summer 
2002, I am still waiting, but technological developments are 
rapidly nearing the time, probably somewhere around 2005, when 
affordable, portable, readable, wireless reading devices will 
reach the mass markets. The company where I work, Rolltronics 
Corporation, is developing thin, flexible electronics 
technology that will enable many of these devices in the 
future. 
 
While living in Japan and working at Fujitsu, Inc., I founded 
eBookNet and began toying with the design of a next-generation 
information display device. In 1998, I founded eBookNet.com, 
which became a renowned Web site that provided news and 
community services for the e-book and e-publishing industry 
for several years. 
 
In 1999, NuvoMedia (the company that pioneered the current 
generation of electronic reading devices with its "Rocket 
eBook" in 1998) acquired eBookNet and hired me. NuvoMedia 
supported eBookNet until April 2001. 
 
A few months later, with the support of the Rolltronics 
Foundation, Wade Roush (former managing editor of eBookNet) 
and I founded the Electronic Publishing Resource Center 
(EPRC), an industry-sponsored, non-profit organization, and 
launched eBookWeb.org on the 4th of July 2001. 
I see myself as an e-book evangelist, seeking to inform and 
educate the world about electronic publishing. My vision is of 
a world where information, entertainment, and books are 
readily available to professionals, researchers, students, and 
readers everywhere. So, even though I work full time for 
Rolltronics doing business development, I continue my daily 
efforts to help build the e-Book industry through 
eBookWeb.org.  The Website now leads in providing news, 
information, resources, and community services to the e-media 
industries. 
 
Q. This has been a bad year for e-publishing. Leading brands 
vanished, industry leaders retreated, technology gurus 
bemoaned yet another missed prognosis - that e-books will 
dethrone print books. What went wrong? 
 
A. Ever since I first realized the need for portable 
information devices, my belief in the future of e-books has 
never been shaken. Despite the fact that e-book reality 
replaced hype in 2000, and 2001 brought a temporary cyclical 
economic downturn, I firmly believe and know that e-books and 
e-publishing, or more generally portable information devices, 
will play a primary role in the way that people write, create, 
design, read, learn, access news and information, communicate, 
interact, travel, enjoy art and entertainment, and experience 
their world. 
 
It is just taking longer to get there than many had hoped 
around the turn of the century. There are still several 
factors that need to come together to make e-books a 
reality. The hardware is still not there. We need affordable, 
light, thin, readable displays with battery life measured in 
days or weeks, not hours. To be truly useful and portable, the 
devices need to be wireless and perhaps with a backup cellular 
connection for remote locales. Next, there needs to be much 
more content available for distribution to these devices. 
Secure but accessible infrastructure and standards need to be 
in place for mass-market appeal. Then, adoption by libraries 
and educational institutions will spread the use of e-books at 
the grassroots level. 
 
Q. Questions of device compatibility and standards have 
plagued the industry from its inception. Will we end up with 
an oligopoly of 2-3 formats and 2-3 corresponding readers, or 
do you have a different take on the industry's future? 
 
A. We may be destined to have several formats and platforms, 
each of which is used for certain applications and types of 
content. The reason is that there are basically four major 
players, each with their own plan to dominate the e-Publishing 
market. 
 
Despite the fact that, in my opinion, Adobe's PDF is lacking 
as an e-Book format, there are hundreds of millions of 
documents in PDF in publishing companies, governments, 
corporations, and schools. These will not be replaced 
instantly, even if a unified format were agreed upon. 
 
Then there is Microsoft, the 800-pound gorilla, who is slowly 
and silently insinuating their reading platform into their 
software and Windows operating system. The interoperability of 
MS Reader software with MS Office products will make it 
possible for many millions of documents to be converted to MS 
Reader format.  
Of course, there will need to be a portable device to display 
all those e-documents. Despite the fact that many Pocket PCs 
have been sold, they don't seem to be a major factor in e-
content sales. Now the timing of Microsoft's big push for the 
MS tablet PC begins to make more sense. 
 
The Gemstar format has an established base of customers and 
actual dedicated devices, the Rocket eBook and REB1100 and 
REB1200s. Gemstar's format actually has a lot of popular 
content going for it, and their displays are much better than 
the average computer display. Therefore they are more suitable 
for portable reading. 
 
And not surprisingly, the largest sales of electronic content 
are going to the Palm Pilot compatible devices. The 
established base of many millions of "Palm OS" customers has 
been buying hundreds of thousands of e-books each year, and 
the e-content sales are growing steadily. 
 
How to unify these four goliaths? The Open eBook Forum's 
standard is good for the formatting of the original document.  
Microsoft and Gemstar adhere to the OeBF standard. But each 
company has its own way of converting and displaying the OeBF 
format in its device or software.  So what is the answer? The 
only way to rectify all of these heavyweight solutions is to 
create a unified standard for displaying electronic content 
that is the same across all platforms. Is this possible? That 
is a question better answered by the experts at the OeBF... 
 
Q. Some analysts blame the recent bloodbath on a dearth of 
good content and wrong pricing. They derisively equate e-
publishing with vanity publishing. Do you find these 
criticisms correct? 
 
A. The amount of content is growing slowly but steadily.  
There are two major problems that contribute to the relative 
dearth of titles becoming available. One is that extra 
negotiations and agreements are necessary to publish e-books, 
or to price them differently from "p-books." Another is that 
since the market still isn't there, many publishers do not 
have the resources, or haven't budgeted enough money to 
aggressively convert content. And many veteran publishers 
still produce the final version of a book in a format that is 
not easy to convert for electronic publication. 
 
As far as vanity publishing goes, that is not defined by the 
medium. Of course electronic publishing makes it easier to 
distribute "vanity-published" works. And it is easier to 
become self-published. And there are a few vanity publishers 
out there, but they usually don't last long. Still, most 
publishers and electronic publishers strive to produce top 
quality titles.  They know that this is the only long-term 
viable business model. They screen and edit the titles that 
they publish. They actively promote their authors' works. In 
this sense, a publisher's name brand will become much more 
important to customers than is presently the case. 
 
Q. Traditional print publishers treat e-books (the content, 
not the devices) as electronic facsimiles of the print 
editions. Can e-books offer a different reading experience? In 
what way are they different to print books?


 
 
A. E-books that are nothing more than electronic copies of the 
print version offer only portability and access as 
advantages.  Of course e-books can be searched and annotated.  
The vision impaired can read with large fonts. Students can 
look up words in a built-in dictionary. 
 
But, similar to popular movie DVDs that include many extras, 
e-books should really take advantage of the flexibility and 
capacity of the electronic medium. Publishers could include 
the author's notes, rough sketches, background, audio or video 
from the author or the scene of the books. Reference works 
should be electronically updateable via the Internet. Book 
club members might be able to send each other their 
annotations and comments. Readers might send feedback to the 
author and/or publisher. Fans might write and distribute 
alternate endings, or add characters or scenes. 
 
Q. E-publishing is at the nexus of sea changes in copyright 
laws. Does e-publishing encourage piracy? Have publishers gone 
overboard in an effort to preserve their intellectual property 
rights? Do you foresee new models of revenues and royalties 
and a novel definition of intellectual property? 
 
A. E-publishing does not encourage piracy, but being in 
electronic format, it certainly becomes susceptible to the 
same kind of piracy that all other kinds of e-content 
experience. A number of models, or rather experiments, are 
being tried with respect to the level of control of 
intellectual property and the associated financial model. So 
far, there has not been a clear answer as to which experiment 
yields the best results. 
One factor is that the market is still in its infancy and 
therefore is in a state of flux. The continuum runs from 
strict and limited control offered by digital rights 
management systems, to free e-content (hopefully) supported by 
either stimulating sales of print books, or advertisements. In 
the middle are publishers who provide limited security, or 
those who use no security and depend on the basic honesty of 
most people. As the market grows, we will discover which 
models work best in which situations for which types of 
content. 
 
Q. E-books were supposed to bring about disintermediation and 
foster a direct dialog between author and readership. Have 
they succeeded? What is the future of content brokers, such as 
publishers and record companies? 
 
A. Yes, there is an enhanced dialog between author and 
audience. On eBookWeb.org, we provide space for authors to 
have a personal page. These are some of the most popular pages 
on the site. On other Websites and through the publications 
themselves, authors are coming in closer digital contact with 
their readers through email or other forms of dialog. For low 
volumes of messages, this is a good thing. But top-selling 
writers could not handle email from thousands of dedicated 
fans.  Even in an electronic world, it is still true that as 
one becomes more popular, one has to become less and less 
accessible in order to conserve one's time.


 
 
Yes, it is also much easier to become self-published 
electronically. However, there is usually a huge difference 
between simply being published, and actually reaching a large 
audience and reaping significant sales of your title. The Web 
continues to grow exponentially, but our time and attention 
span remain limited. These two opposing dynamics mean that we 
are forced to narrow our attention to a relatively few 
reliable content providers, representing an ever smaller 
proportion of the total content available. 
 
How can an author be heard above the noise? Get a publisher 
who will promote your work. But before that, get an editor or 
publisher who will help you polish your work until it shines 
brightly enough to gain popularity once it secures the 
attention of your audience. The dynamics and demands of the 
free market, and the reasons for having publishing companies 
do not disappear on the Internet. In fact, they may become 
more important as the amount of content and choices continues 
to grow. 
 
One important change that I do foresee is that small, 
independent niche publishers will make a resurgence due to the 
electronic medium. This is definitely a good thing for 
readers. Independent publishers who build a reputation for 
unique, quality content, will develop a following of faithful 
customers over time. 
 
Q. Some marketing pundits believe in viral or buzz marketing. 
They advocate giving away free content to generate "buzz". 
They believe that sales will follow. Do you subscribe to this 
view? 
 
A. This relates to the question of copyright laws and which 
model is best for a particular situation. It also has to do 
with previous models on the Web. If the goal is to gain an 
audience and fame, then giving it away to hopefully millions 
of people is a good idea. The popular dynamic of the Internet 
is to build a massive audience by giving away something of 
value.  Then, one slowly begins to charge for some content or 
service, while still providing something for free, to continue 
to attract a large following. 
 
The results of the late 1990s indicate a mixed success, 
probably due in part to the origins of the Internet, where 
everything was free. The expectation was that if it was on the 
Net, it was free. The beginnings of commercialism on the Net 
in the early 1990's were met with vehement resistance from the 
"old timers" who strongly opposed the commercialization of 
their beloved network. Of course, a number of companies such 
as eBay, Amazon, and Yahoo, attracted and kept a large 
audience. But only a few are truly profitable today. 
 
If the goal is to make maximum profit from each unit of 
content that is downloaded, then one must charge money, or 
sell advertisements. Unfortunately, the revenues from 
advertising on the Net have fallen dramatically in the last 
few years. So if you put a price tag on your content, how much 
should you charge?  Most independent electronic publishers 
charge a few dollars for their titles, anywhere from $1 each 
to about $5 or $7 per e-book. These relatively low prices 
reflect the desire to attract a large pool of customers. They 
also reflect the belief common among readers that since it is 
electronic and not print content, the price should be 
lower. They feel that without the cost of printing and 
transporting books, the publisher should set a lower price... 
 
Q. As you see it, is the Internet merely another content 
distribution channel or is there more to it then this? The 
hype of synergy and collapsing barriers to entry has largely 
evaporated together with the fortunes of the likes of AOL Time 
Warner. Is the Internet a revolution - or barely an evolution? 
 
A. In the beginning, the Internet was a revolution. Email 
brought the people of our Earth closer together. The Net 
enabled telecommuting and now as much as 10% of the world 
works at home via computer and Internet. The Internet makes it 
possible for artists to publish their own books, music, videos 
and Websites. Video conferencing has enabled conversations 
without limitations of space. The Internet has made vast 
amounts of information available to students and researchers 
at the click of the mouse. The 24/7 access and ease of 
ordering products has stimulated online commerce and sales at 
retail stores. 
 
But it is not a cure-all. And, now that the Net is part of our 
everyday lives, it is subject to the same cycles of media 
hype, as well as social, emotional, and business 
factors. Things will never be the same, and the changes have 
just begun. The present generation has never known a world 
without computers.  When they reach working age, they will be 
much more inclined to use the Net for a majority of their 
reading and entertainment needs. Then, e-books will truly take 
hold and become ubiquitous. Between now and then, we have work 
to do, building the foundation of this remarkable industry.




WEB TECHNOLOGIES AND TRENDS
Bright Planet, Deep Web 
By: Sam Vaknin
 
www.allwatchers.com and www.allreaders.com are web sites in 
the sense that a file is downloaded to the user's browser when 
he or she surfs to these addresses. But that's where the 
similarity ends. These web pages are front-ends, gates to 
underlying databases. The databases contain records regarding 
the plots, themes, characters and other features of, 
respectively, movies and books. Every user-query generates a 
unique web page whose contents are determined by the query 
parameters.The number of singular pages thus capable of being 
generated is mind boggling. Search engines operate on the same 
principle - vary the search parameters slightly and  
totally new pages are generated. It is a dynamic, user-
responsive and chimerical sort of web.
 
These are good examples of what www.brightplanet.com call the 
"Deep Web" (previously inaccurately described as the "Unknown 
or Invisible Internet"). They believe that the Deep Web is 500 
times the size of the "Surface Internet" (a portion of which 
is spidered by traditional search engines). This translates to 
c. 7500 TERAbytes of data (versus 19 terabytes in the whole 
known web, excluding the databases of the search engines 
themselves) - or 550 billion documents organized in 100,000 
deep web sites. By comparison, Google, the most comprehensive 
search engine ever, stores 1.4 billion documents in its 
immense caches at www.google.com. The natural inclination  
to dismiss these pages of data as mere re-arrangements of the 
same information is wrong. Actually, this underground ocean of 
covertintelligence is often more valuable than the information 
freely available or easily accessible on the surface. Hence 
the ability of c. 5% of these databases to charge their users 
subscription and membership fees. The average deep web site 
receives 50% more traffic than a typical surface site and is 
much more linked to by other sites. Yet it is transparent to 
classic search engines and little known to the surfing public.
 
It was only a question of time before someone came up with a 
search technology to tap these depths 
(www.completeplanet.com).
 
LexiBot, in the words of its inventors, is...
 
"...the first and only search technology capable of 
identifying, retrieving, qualifying, classifying and 
organizing "deep" and "surface" content from the World Wide 
Web.  The LexiBot allows searchers to dive deep and explore 
hidden data from multiple sources simultaneously using 
directed queries. Businesses, researchers and consumers now 
have access to the most valuable and hard-to-find information 
on the Web and can retrieve it with pinpoint accuracy."
 
It places dozens of queries, in dozens of threads 
simultaneously and spiders the results (rather as a "first 
generation" search engine would do). This could prove very 
useful with massive databases such as the human genome, 
weather patterns, simulations of nuclear explosions, thematic, 
multi-featured databases, intelligent agents (e.g., shopping 
bots) and third generation search engines. It could also have 
implications on the wireless internet (for instance, in 
analysing and generating location-specific advertising) and on 
e-commerce (which amounts to the dynamic serving of web 
documents).
 
This transition from the static to the dynamic, from the given 
to the generated, from the one-dimensionally linked to the 
multi-dimensionally hyperlinked, from the deterministic 
content to the contingent, heuristically-created and uncertain 
content - is the real revolution and the future of the web. 
Search engines have lost their efficacy as gateways. Portals 
have taken over but most people now use internal links (within 
the same web site) to get from one place to another. This is 
where the deep web comes in. Databases are about internal 
links. Hitherto they existed in splendid isolation, universes 
closed but to the most persistent and knowledgeable. This may 
be about to change. The flood of quality relevant information 
this will unleash will dramatically dwarf anything that 
preceded it.





The Seamless Internet
By: Sam Vaknin

http://www.enfish.com/
 
The hype over ubiquitous (or pervasive) computing (computers 
everywhere) has masked a potentially more momentous 
development. It is the convergence of computing devices 
interfaces with web (or other) content. Years ago - after Bill 
Gates overcame his misplaced scepticism - Microsoft introduced 
their "internet-ready" applications. Its word processing 
software ("Word"), other Office applications, and the Windows 
operating system handle both "local" documents (resident on 
the user's computer) and web pages smoothly and seamlessly. 
The transition between the desktop or laptop interfaces and 
the web is today effortlessly transparent.
 
The introduction of e-book readers and MP3 players has blurred 
the anachronistic distinction between hardware and software. 
Common speech reflects this fact. When we say "e-book", we 
mean both the device and the content we access on it. As 
technologies such as digital ink and printable integrated 
circuits mature - hardware and software will have completed 
their inevitable merger.
 
This erasure of boundaries has led to the emergence of 
knowledge management solutions and personal and shared 
workspaces. The LOCATION of a document (one's own computer, a 
colleague's PDA, or a web page) has become irrelevant. The 
NATURE of the document (e-mail message, text file, video 
snippet, soundbite) is equally unimportant. The SOURCE of the 
document (its extension, which tells us on which software it 
was created and can be read) is increasingly meaningless. 
Universal languages (such as Java) allow devices and 
applications to talk to each other. What matters are 
accessibility and logical and user-friendly work-flows.
 
Enter Enfish. In its own words, it provides:
 
"...Personalized portal solution linking personal and 
corporate knowledge with relevant information from the 
Internet, ...live-in desktop environment providing co-branding 
and customization opportunities on and offline, a unique, 
private communication channel to users that can be used also 
for eBusiness solutions, ...Knowledge Management solution that 
requires no user set-up or configuration."
 
The principle is simple enough - but the experience is 
liberating (try their online flash demo). Suddenly, instead of 
juggling dozens of windows, a single interface provides the 
tortured user (that's I) with access to all his applications: 
e-mail, contacts, documents, the company's intranet or 
network, the web and OPC's (other people's computers, other 
networks, other intranets). There is only a single screen and 
it is dynamically and automatically updated to respond to the 
changing information needs of the user.
 
"The power underlying Enfish Onespace is its patented DEX 
'engine.' This technology creates a master, cross-referenced 
index of the contents of a user's email, documents and 
Internet information. 


The Enfish engine then uses this master index as a basis to 
understand what is relevant to a user, and to provide them 
with appropriate information. In this manner Enfish Onespace 
'personalizes' the Internet for each user, automatically 
connecting relevant information and services from the Internet 
with the user's desktop information.
 
As an example, by clicking on a person or company, Enfish 
Onespace automatically assembles a page that brings together 
related emails, documents, contact information, appointments, 
news and relevant news headlines from the Internet. This is 
accomplished without the user working to find and organize 
this information. By having everything in one place and in 
context, our users are more informed and better prepared to 
perform tasks such as handling a phone call or preparing for a 
business meeting. This results in ... benefits in productivity 
and efficiency."
 
It is, indeed, addictive. The inevitable advent of transparent 
computing (smart houses, smart cards, smart clothes, smart 
appliances, wireless Internet) - coupled with the single GUI 
(Graphic User Interface) approach can spell revolution in our 
habits. Information will be available to us anywhere, through 
an identical screen, communicated instantly and accurately 
from device to device, from one appliance to another and from 
one location to the next as we move. The underlying software 
and hardware will become as arcane and mysterious as are the 
ASCII and ASSEMBLY languages to the average computer user 
today. It will be a real partnership of biological and 
artificial intelligence on the move.



 
  
The Polyglottal Internet
By: Sam Vaknin
 
http://www.everymail.com/ 
The Internet started off as a purely American phenomenon and 
seemed to perpetuate the fast-emerging dominance of the 
English language. A negligible minority of web sites were in 
other languages. Software applications were chauvinistically 
ill-prepared (and still are) to deal with anything but 
English. And the vast majority of net users were residents of 
the two North-American colossi, chiefly the USA. 
All this started to change rapidly about two years ago. Early 
this year, the number of American users of the Net was 
surpassed by the swelling tide of European and Japanese ones. 
Non-English web sites are proliferating as well. The advent of 
the wireless Internet - more widespread outside the USA - is 
likely to strengthen this unmistakable trend. By 2005, certain 
analysts expect non-English speakers to make up to 70% of all 
netizens. This fragmentation of an hitherto unprecedentedly 
homogeneous market - presents both opportunities and costs. It 
is much more expensive to market in ten languages than it is 
in one. Everything - from e-mail to supply chains has to be 
re-tooled or customized. 
It is easy to translate text in cyberspace. Various automated, 
web-based, and free applications (such as Babylon or Travlang) 
cater to the needs of the casual user who doesn't mind the 
quality of the end-result. Virtually every search engine, 
portal and directory offers access to these or similar 
services. 
But straightforward translation is only one kind of solution 
to the tower of Babel that the Internet is bound to become. 
Enter WorldWalla. A while back I used their multi-lingual e-
mail application. It converted text I typed on a virtual 
keyboard to images (of characters). My addressees received the 
message in any language I selected. It was more than cool. It 
was liberating. Along the same vein, WorldWalla's software 
allows application and content developers to work in 66 
languages. In their own words: 
"WordWalla allows device manufacturers and application 
developers to meet this challenge by developing products that 
support any language. This simplifies testing and 
configuration management, accelerates time to market, lowers 
unit costs and allows companies to quickly and easily enter 
new markets and offer greater levels of personalization and 
customer satisfaction." 
GlobalVu converts text to device-independent images. 
GlobalEase Web is a "Java-based multilingual text input and 
display engine". It includes virtual keyboards, front-end 
processors, and a contextual processor and text layout engine 
for left to right and right to left language formatting. They 
have versions tailored to the specifications of mobile 
devices. 
The secret is in generating and processing images (bitmaps), 
compressing them and transmitting them. In a way, WordWalla 
generates a FACSIMILE message (the kind we receive on our fax 
machines) every time text is exchanged. It is transparent to 
both sender and receiver - and it makes a user-driven 
polyglottal Internet a reality. 



Deja Googled
By: Sam Vaknin
http://groups.google.com/ 
http://groups.google.com/googlegroups/archive_announce.html 
The Internet may have started as the fervent brainchild of 
DARPA, the US defence agency - but it quickly evolved into a 
network of computers at the service of a community. Academics 
around the world used it to communicate, compare results, 
compute, interact and flame each other. The ethos of the 
community as content-creator, source of information, fount of 
emotional sustenance, peer group, and social substitute is 
well embedded in the very fabric of the Net. Millions of 
members in free, advertising or subscription financed, mega-
sites such as Geocities, AOL, Yahoo and Tripod generate more 
bits and bytes than the rest of the Internet combined. This 
traffic emanates from discussion groups, announcement 
(mailing) lists, newsgroups, and content sites (such as 
Suite101 and Webseed). Even the occasional visitor can find 
priceless gems of knowledge and opinion in the mound of trash 
and frivolity that these parts of the web have become. 
The emergence of search engines and directories which cater 
only to this (sizeable) market segment was to be expected. By 
far the most comprehensive (and, thus, less discriminating) 
was Deja. It spidered and took in the exploding newsgroups 
(Usenet) scene with its tens of thousands of daily messages. 
When it was taken over by Google, its archives contained more 
than 500 million messages, cross-indexed  every which way and 
pertaining to every possible (and many impossible) a topic. 
Google is by far the most popular search engine yet, having 
surpassed the more veteran Northern Lights, Fast, and Alta 
Vista. Its mind defying database (more than 1.3 billion web 
pages), its caching technology (making it, in effect, one of 
the biggest libraries on earth) and its site ranking (by 
popularity and links-over) have rendered it unbeatable. Yet, 
its efforts to integrate the treasure trove that is Deja and 
adapt it to the Google search interface have hitherto been 
spectacularly unsuccessful (though it finally made it two and 
a half months after the purchase). So much so, that it gave 
birth to a protest movement. 
Bickering and bad tempered flaming (often bordering on the 
deranged, the racial, or the stalking) are the more repulsive 
aspects of the Usenet groups. But at the heart of the debate 
this time is no ordinary sadistic venting. The issue is: who 
owns content generated by the public at large on computers 
funded by tax dollars? Can a commercial enterprise own and 
monopolize the fruits of the collective effort of millions of 
individuals from all over the world? Or should such 
intellectual property remain in the public domain, perhaps 
maintained by public institutions (such as the Library of 
Congress)? Should open source movements gain access to Deja's 
source code in order to launch Deja II? And who owns the 
copyright to all these messages (theoretically, the authors)? 
Google, as Deja before it, is offering compilations of this 
content, the copyright to which it does not and cannot own. 
The very legal concept of intellectual property is at the crux 
of this virtual conflict. 
Google was, thus, compelled to offer free access to the 
CONTENT of the Deja archives to alternative (non-Google) 
archiving systems. But it remains mum on the search 
programming code and the user interface. Already one such open 
source group (called Dela News) is coalescing, although it is 
not clear who will bear the costs of the gigantic storage and 
processing such a project would require. Dela wants to have a 
physical copy of the archive deposited in trust with a dot 
org. 
This raises a host of no less fascinating subjects. The Deja 
Usenet search technology, programming code, and systems are 
inextricable and almost indistinguishable from the Usenet 
archive itself. Without these elements - structural as well as 
dynamic - there will be no archive and no way to extract 
meaningful information from the chaotic bedlam that is the 
Usenet environment. In this case, the information lies in the 
ordering and classification of raw data and not in the content 
itself. This is why the open source proponents demand that 
Google share both content and the tools to access it. Google's 
hasty and improvised unplugging of Deja in February only 
served to aggravate the die-hard fans of erstwhile Deja. 
The Usenet is not only the refuge of pedophiles and neo-Nazis. 
It includes thousands of academically rigorous and research 
inclined discussion groups which morph with intellectual 
trends and fashionable subjects. More than twenty years of 
wisdom and erudition are buried in servers all over the world. 
Scholars often visit Usenet in their pursuit of complementary 
knowledge or expert advice. The Usenet is also the 
documentation of Western intellectual history in the last 
three decades. In it invaluable. Google's decision to abandon 
the internal links between Deja messages means the 
disintegration of the hyperlinked fabric of this resource - 
unless Google comes up with an alternative (and expensive) 
solution. 
Google is offering a better, faster, more multi-layered and 
multi-faceted access to the entire archive. But its brush with 
the more abrasive side of the open source movement brought to 
the surface long suppressed issues. This may be the single 
most important contribution of this otherwise not so opportune 
transaction. 


 
  
Maps of Cyberspace
By: Sam Vaknin
 
"Cyberspace. A consensual hallucination experienced daily by 
billions of legitimate operators, in every nation, by children 
being taught mathematical concepts...A graphical 
representation of data abstracted from the banks of every 
computer in the human system. Unthinkablecomplexity. Lines of 
light ranged in the non-space of the mind, clusters and 
constellations of data. Like city lights, receding..." 
(William Gibson, "Neuromancer", 1984, page 51) 
http://www.ebookmap.net/maps.htm 
http://www.cybergeography.org/atlas/atlas.html 
At first sight, it appears to be a static, cluttered diagram 
with multicoloured, overlapping squares. Really, it is an 
extremely powerfulway of presenting the dynamics of the 
emerging e-publishing industry. R2 Consulting has constructed 
these eBook Industry Maps to "reflect the evolving business 
models among publishers, conversion houses, digital 
distribution companies, eBook vendors, online retailers, 
libraries, library vendors, authors, and many others.  These 
maps are 3-dimensionaloffering viewers both a high-level 
orientation to the eBook landscape and an in-depth look at 
multiple eBook models and the partnerships that have formed 
within each one." Pass your mouse over any of the squares and 
a virtual floodgate opens - a universe of interconnected and 
hyperlinked names, a detailed atlas of who does what to whom. 
eBookMap.net is one example of a relatively novel approach to 
databases and web indexing. The metaphor of cyber-space comes 
alive in spatial, two and three dimensional map-like 
representations of the world of knowledge in Cybergeography's 
online "Atlas". Instead of endless, static and bi-chromatic 
lists of links - Cybergeography catalogues visual,recombinant 
vistas with a stunning palette, internal dynamics and an 
intuitively conveyed sense of inter-relatedness. Hyperlinks 
are incorporated in the topography and topology of these 
almost-neural maps. 
"These maps of Cyberspaces - cybermaps - help us visualise and 
comprehend the new digital landscapes beyond our computer 
screen, in the wires of the global communications networks and 
vast online information resources. The cybermaps, like maps of 
the real-world, help us navigate the new information 
landscapes, as well being objects of aesthetic interest. They 
have been created by 'cyber-explorers' of many different 
disciplines, and from all corners of the world. Some of the 
maps ... in the Atlas of Cyberspaces ... appear familiar, 
using the cartographicconventions of real-world maps, however, 
many of the maps are much more abstract representations of 
electronic spaces, using new metrics and grids." 
Navigating these maps is like navigating an inner, familiar, 
territory. 
They come in all shapes and modes: flow charts, quasi-
geographical maps, 3-d simulator-like terrains and many 
others. The "web Stalker" is an experimental web browser which 
is equipped with mapping functions. The range of applicability 
is mind boggling. 
A (very) partial list: 
The Internet Genome Project - "open-source map of the major 
conceptual components of the Internet and how they relate to 
each other" 
Anatomy of a Linux System - Aimed to "...give viewers a 
concise and comprehensive look at the Linux universe' and at 
the heart of the poster is a gravity well graphic showing the 
core software components,surrounded by explanatory text" 
NewMedia 500 - The financial, strategic, and other inter-
relationshipsand interactions between the leading 500 new 
(web) media firms 
Internet Industry Map - Ownership and alliances determine 
status, control, and access in the Internet industry. A 
revealing organizational chart. 
The Internet Weather Report measures Internet performance, 
latency periods and downtime based on a sample of 4000 
domains. 
Real Time Geographic Visualization of WWW Traffic - a 
stunning, 3-d representation of web usage and traffic 
statistics the world over. 
WebBrain and Map.net provide a graphic rendition of the Open 
Directory Project. The thematic structure of the ODP is 
instantly discernible. 
The WebMap is a visual, multi-category directory which 
contains 2,000,000 web sites. The user can zoom in and out of 
sub-categories and "unlock" their contents. 
Maps help write fiction, trace a user's clickpath (replete 
with clickable web sites), capture Usenet and chat 
interactions (threads), plot search results (though Alta Vista 
discontinued its mapping service and Yahoo!3D is no more), 
bookmark web destinations, and navigate through complex sites. 
Different metaphors are used as interface. Web sites are 
represented as plots of land, stars (whose brightness 
corresponds to the web site's popularity ranking), amino-acids 
in DNA-like constellations,topographical maps of the ocean 
depths, buildings in an urban landscape, or other objects in a 
pastoral setting. Virtual Reality (VR) maps allow information 
to be simultaneously browsed by teams of collaborators, 
sometimes represented as avatars in a fully immersive 
environment. In many applications, the user is expected to fly 
amongst the data items in virtual landscapes. With the advent 
of sophisticated GUI's (Graphic UserInterfaces) and VRML 
(Virtual Reality Markup Language) - these maps may well show 
us the way to a more colourful and user-friendly future. 


The Universal Intuitive Interface
By: Sam Vaknin
The history of technology is the history of interfaces - their 
successes and failures. The GUI (the Graphic User Interface) - 
which replaced cumbersome and unwieldy text-based interfaces 
(DOS) - became an integral part of the astounding success of 
the PC. 
Yet, all computer interfaces hitherto share the same growth-
stunting problems. They are:
(a) Non-transparency - the workings of the hardware and 
software (the "plumbing") show through
(b) Non-ubiquity - the interface is connected to a specific 
machine and, thus, is non-transportable
(c) Lack of friendliness (i.e., the interfaces require 
specific knowledge and specific sequences of specific 
commands).
Even the most "user-friendly" interface is way too complicated 
for the typical user. The average PC is hundreds of times more 
complicated than your average TV. Even the VCR - far less 
complex than the PC - is a challenge. How many people use the 
full range of a VCR's options? 
 
The ultimate interface, in my view, should be: 
(a) Self-assembling - it should reconstruct itself, from time 
to time, fluidly
(b) Self-recursive - it should be able to observe and analyze 
its own behavior
(c) Learning-capable - it should learn from its experience
(d) Self-modifying - it should modify itself according to its 
accumulated experience
(e) History-recording 
It must possess a "picture of the world" (a-la artificial 
intelligence) - preferably including itself, the user, and 
their cumulative interactions. 
It must regard all other "intelligent" machines in its 
"world"  (the user being only one of them) as its "clients". 
It must, therefore, be able to communicate with them in a 
natural language. 
Its universe must be seamless (e.g., the physical or even 
system location of files or hardware or software or applets or 
servers or communication lines or information and so on - will 
be irrelevant). 
It will probably be peer-orientated (no hierarchy). 
I call it "the intuitive universal interface".
The new media technologies were designed by engineers and 
programmers - not by marketing people and users. The interface 
of the future will reflect the needs, wishes, limitations, and 
skills of users. This is a revolutionary shift and a natural 
outcome of the takeover of the Internet by governments and 
bottom line orientated corporations. The interface of the 
future will seek to enhance usage and enrich the user's 
experience - not to win technological beauty contest. It is a 
welcome transition - and long overdue.


Internet Advertising - What Went Wrong?
By: Sam Vaknin

The decline in Internet advertising - though paralleled by a 
similar trend in print advertising - had more serious and 
irreversible implications. Most content dot.coms were based on 
ad-driven revenue models. Online advertising was supposed to 
amortize start-up and operational costs and lead to 
profitability even as it subsidized free access to costly 
content. 
A similar revenue model has been successfully propping up 
print periodicals for at least two centuries. But, as opposed 
to their online counterparts, print products have a few 
streams of income, not least among them paid subscriptions. 
Moreover, print media kept their costs down in good times and 
bad. Dot.coms devoured their investors' money in a self-
destructive and avaricious bacchanalia. 
But why did online advertising collapse in the first place? 
Was it ineffective?
Advertising is a multi-faceted and psychologically complex 
phenomenon. It imparts information to potential consumers, 
users, suppliers, investors, the community, or other 
stakeholders in the firm. It motivates each of these to do his 
bit: consumers to consume, investors to invest and so on. 
But this is not the main function of the advertising dollar. 
Modern economic signal theory has cast advertising in a new 
and surprising - though by no means counterintuitive - light. 
According to this theory, the role of advertising is to signal 
to the marketplace the advertiser's resilience, longevity, 
wealth, clout, and dominance. By splurging money of 
advertising, the advertiser actually informs us - the 
"eyeballs" - that it is here to stay, sufficiently affluent to 
finance its ads, stable, reliable, and dominant. 
"If firm X invested a million bucks in advertising - it must 
be worth more than a million bucks" - goes the signal. "If it 
invested so much money in promoting its products, it is not a 
fly-by-night". "If it can throw money at an ad campaign, it is 
stable and resilient".
This signal is missing in online advertising. It drowns in 
noise. The online noise to signal ratio was unacceptable to 
advertisers - so they stopped advertising. When the noise to 
signal ratio tops a certain level - ads cease to be effective. 
The readers or spectators become inured to the messages - both 
explicit and implicit. They tune off. 
The noise in online advertising stems from two sources.
A critical element in the signal is lost if the ad is not paid 
for. Only paid advertising conveys information about the 
purported health and prospects of the advertiser. Yet, the 
Internet is flooded with free advertising: free classifieds, 
free banner ads, ad exchanges. The paid ads drown in this 
ocean of free ads. There is often no way of telling a paid ad 
from a free one - without reading the fine print.
Moreover, Internet users are a "captive audience". It is easy 
to flip ad-besieged channels on TV, or turn the ad-laden leaf 
of a newspaper. It is close to impossible to avoid an ad on 
the Net. Banner ads are an integral part of the page. Pop-up 
ads pop up. Embedded ads are embedded. One needs to install 
special applications to avoid the harassment. 
This leads to desensitization and a revolt of the user. Users 
resent the intrusion, are incensed by the coercive tactics of 
advertisers, nerve wrecked by protracted download times, and 
unnerved by the content of many of the ads. This is not an 
environment conducive to clinching deals or converting to 
sales.
There is also the issue of credibility. The bulk of online 
advertising emanates from dot.coms. Even prior to the recent 
stock exchange meltdown, these were not considered paragons of 
rectitude and truth in advertising. People learned to distrust 
most of what they read in Internet ads. Scorched by scams, 
false promises, faulty products, shoddy or non-existent 
customer care, broken links, or all of the above - users 
learned to ignore Web advertising and relegate it to their 
mental dust bins.
More about credibility on the Web here:
The In-Credible Web
Will the medium ever recover? Probably not. As the Internet is 
taken over by brick-and-mortar corporations and governments, 
online fare will come to resemble the offline sort. Online ads 
will be no more than interactive renditions of their offline 
facsimiles. The revenue model will switch from advertising to 
subscriptions and "author-pays". The days of free content 
financed by advertising are over.
This does not mean that the days of free content are over as 
well. It only means that new, improved, realistic, and 
clutter-free revenue models will have to be found. There are 
some interesting developments in scholarly online publishing 
as well as in the fields of online reference and self-
publishing. But these are early days and the medium is 
dynamic. Ad-driven content was a failure. The next model may 
be a roaring success - or yet another dismal defeat.


The Economics of Spam
By: Sam Vaknin
Also published by United Press International (UPI)

Tennessee resident K. C. "Khan" Smith owes the internet 
service provider EarthLink $24 million. According to the CNN, 
last August he was slapped with a lawsuit accusing him of 
violating federal and state Racketeering Influenced and 
Corrupt Organizations (RICO) statutes, the federal Computer 
Fraud and Abuse Act of 1984, the federal Electronic 
Communications Privacy Act of 1986 and numerous other state 
laws. On July 19 - having failed to appear in court - the 
judge ruled against him. Mr. Smith is a spammer.
Brightmail, a vendor of e-mail filters and anti-spam 
applications warned that close to 5 million spam "attacks" or 
"bursts" occurred last month and that spam has mushroomed 450 
percent since June last year. PC World concurs. Between one 
seventh and one half of all e-mail messages are spam - 
unsolicited and intrusive commercial ads, mostly concerned 
with sex, scams, get rich quick schemes, financial services 
and products, and health articles of dubious provenance. The 
messages are sent from spoofed or fake e-mail addresses. Some 
spammers hack into unsecured servers - mainly in China and 
Korea - to relay their missives anonymously.
Spam is an industry. Mass e-mailers maintain lists of e-mail 
addresses, often "harvested" by spamware bots - specialized 
computer applications - from Web sites. These lists are rented 
out or sold to marketers who use bulk mail services. They come 
cheap - c. $100 for 10 million addresses. Bulk mailers provide 
servers and bandwidth, charging c. $300 per million messages 
sent. 
As spam recipients become more inured, ISP's less tolerant, 
and both more litigious - spammers multiply their efforts in 
order to maintain the same response rate. Spam works. It is 
not universally unwanted - which makes it tricky to outlaw. It 
elicits between 0.1 and 1 percent in positive follow ups, 
depending on the message. Many messages now include HTML, 
JavaScript, and ActiveX coding and thus resemble viruses. 
Jupiter Media Matrix predicted last year that the number of 
spam messages annually received by a typical Internet user is 
bound to double to 1400 and spending on legitimate e-mail 
marketing will reach $9.4 billion by 2006 - compared to $1 
billion in 2001. Forrester Research pegs the number at $4.8 
billion next year. 
More than 2.3 billion spam messages are sent daily. eMarketer 
puts the figures a lot lower at 76 billion messages this year. 
By 2006, daily spam output will soar to c. 15 billion 
missives, says Radicati Group. Jupiter projects a more modest 
268 billion annual messages by 2005. An average communication 
costs the spammer 0.00032 cents. 
PC World quotes the European Union as pegging the bandwidth 
costs of spam worldwide at $8-10 billion annually. Other 
damages include server crashes, time spent purging unwanted 
messages, lower productivity, aggravation, and increased cost 
of Internet access.
Inevitably, the spam industry gave rise to an anti-spam 
industry. According to a Radicati Group report titled "Anti-
virus, anti-spam, and content filtering market trends 2002-
2006", anti-spam revenues are projected to exceed $88 million 
this year - and more than double by 2006. List blockers, 
report and complaint generators, advocacy groups, registers of 
known spammers, and spam filters all proliferate. The Wall 
Street Journal reported in its June 25 issue about a 
resurgence of anti-spam startups financed by eager venture 
capital.
ISP's are bent on preventing abuse - reported by victims - by 
expunging the accounts of spammers. But the latter simply 
switch ISP's or sign on with free services like Hotmail and 
Yahoo! Barriers to entry are getting lower by the day as the 
costs of hardware, software, and communications plummet.
The use of e-mail and broadband connections by the general 
population is spreading. Hundreds of thousands of 
technologically-savvy operators have joined the market in the 
last two years, as the dotcom bubble burst. Still, Steve 
Linford of the UK-based Spamhaus.org insists that most spam 
emanates from c. 80 large operators.
Now, according to Jupiter Media, ISP's and portals are poised 
to begin to charge advertisers in a tier-based system, replete 
with premium services. Writing back in 1998, Bill Gates 
described a solution also espoused by Esther Dyson, chair of 
the Electronic Frontier Foundation:
"As I first described in my book "The Road Ahead" in 1995, I 
expect that eventually you'll be paid to read unsolicited e-
mail. You'll tell your e-mail program to discard all 
unsolicited messages that don't offer an amount of money that 
you'll choose. If you open a paid message and discover it's 
from a long-lost friend or somebody else who has a legitimate 
reason to contact you, you'll be able to cancel the payment. 
Otherwise, you'll be paid for your time."
Subscribers may not be appreciative of the joint ventures 
between gatekeepers and inbox clutterers. Moreover, dominant 
ISP's, such as AT&T and PSINet have recurrently been accused 
of knowingly collaborating with spammers. ISP's rely on the 
data traffic that spam generates for their revenues in an 
ever-harsher business environment. 
The Financial Times and others described how WorldCom refuses 
to ban the sale of spamware over its network, claiming that it 
does not regulate content. When "pink" (the color of canned 
spam) contracts came to light, the implicated ISP's blame the 
whole affair on rogue employees. 
PC World begs to differ:
"Ronnie Scelson, a self-described spammer who signed such a 
contract with PSInet, (says) that backbone providers are more 
than happy to do business with bulk e-mailers. 'I've signed up 
with the biggest 50 carriers two or three times,' says Scelson 
... The Louisiana-based spammer claims to send 84 million 
commercial e-mail messages a day over his three 45-megabit-
per-second DS3 circuits. "If you were getting $40,000 a month 
for each circuit," Scelson asks, "would you want to shut me 
down?"



The line between permission-based or "opt-in" e-mail marketing 
and spam is getting thinner by the day. Some list resellers 
guarantee the consensual nature of their wares. According to 
the Direct Marketing Association's guidelines, quoted by PC 
World, not responding to an unsolicited e-mail amounts to 
"opting-in" - a marketing strategy known as "opting out". Most 
experts, though, strongly urge spam victims not to respond to 
spammers, lest their e-mail address is confirmed.
But spam is crossing technological boundaries. Japan has just 
legislated against wireless SMS spam targeted at hapless 
mobile phone users. Four states in the USA as well as the 
European parliament are following suit. Expensive and slow 
connections make this kind of spam particularly resented. 
Still, according to Britain's Mobile Channel, a mobile 
advertising company quoted by "The Economist", SMS advertising 
- a novelty - attracts a 10-20 percent response rate - 
compared to direct mail's 1-3 percent.
Net identification systems - like Microsoft's Passport and the 
one proposed by Liberty Alliance - will make it even easier 
for marketers to target prospects. 
The reaction to spam can be described only as mass hysteria. 
Reporting someone as a spammer - even when he is not - has 
become a favorite pastime of vengeful, self-appointed, 
vigilante "cyber-cops". Perfectly legitimate, opt-in, email 
marketing businesses often find themselves in one or more 
black lists - their reputation and business ruined. 
In January, CMGI-owned Yesmail was awarded a temporary 
restraining order against MAPS - Mail Abuse Prevention System 
- forbidding it to place the reputable e-mail marketer on its 
Real-time Blackhole list. The case was settled out of court. 
Harris Interactive, a large online opinion polling company, 
sued not only MAPS, but ISP's who blocked its email messages 
when it found itself included in MAPS' Blackhole. Their CEO 
accused one of their competitors for the allegations that led 
to Harris' inclusion in the list.
Coupled with other pernicious phenomena, such as viruses, the 
very foundation of the Internet as a fun, relatively safe, 
mode of communication and data acquisition is at stake. 
Spammers, it emerges, have their own organizations. NOIC - the 
National Organization of Internet Commerce threatened to post 
to its Web site the e-mail addresses of millions of AOL 
members. AOL has aggressive anti-spamming policies. "AOL is 
blocking bulk email because it wants the advertising revenues 
for itself (by selling pop-up ads)" the president of NOIC, 
Damien Melle, complained to CNET.
Spam is a classic "free rider" problem. For any given 
individual, the cost of blocking a spammer far outweighs the 
benefits. It is cheaper and easier to hit the "delete" key. 
Individuals, therefore, prefer to let others do the job and 
enjoy the outcome - the public good of a spam-free Internet. 
They cannot be left out of the benefits of such an aftermath - 
public goods are, by definition, "non-excludable". Nor is a 
public good diminished by a growing number of "non-rival" 
users.
Such a situation resembles a market failure and requires 
government intervention through legislation and enforcement. 
The FTC - the US Federal Trade Commission - has taken legal 
action against more than 100 spammers for promoting scams and 
fraudulent goods and services. 
"Project Mailbox" is an anti-spam collaboration between 
American law enforcement agencies and the private sector. Non 
government organizations have entered the fray, as have 
lobbying groups, such as CAUCE - the Coalition Against 
Unsolicited Commercial E-mail.
But Congress is curiously reluctant to enact stringent laws 
against spam. Reasons cited are free speech, limits on state 
powers to regulate commerce, avoiding unfair restrictions on 
trade, and the interests of small business. The courts 
equivocate as well. In some cases - e.g., Missouri vs. 
American Blast Fax - US courts found "that the provision 
prohibiting the sending of unsolicited advertisements is 
unconstitutional". 
According to Spamlaws.com,  the 107th Congress discussed these 
laws but never enacted them: 
Unsolicited Commercial Electronic Mail Act of 2001 (H.R. 95), 
Wireless Telephone Spam Protection Act (H.R. 113), Anti-
Spamming Act of 2001 (H.R. 718), Anti-Spamming Act of 2001 
(H.R. 1017), Who Is E-Mailing Our Kids Act (H.R. 1846), 
Protect Children >From E-Mail Smut Act of 2001 (H.R.  2472), 
Netizens Protection Act of 2001 (H.R. 3146), "CAN SPAM" Act of 
2001 (S. 630).
Anti-spam laws fared no better in the 106th Congress. Some of 
the states have picked up the slack. Arkansas, California, 
Colorado, Connecticut, Delaware, Idaho, Illinois, Iowa, 
Kansas, Louisiana, Maryland, Minnesota, Missouri, Nevada, 
North Carolina, Oklahoma, Pennsylvania, Rhode Island, South 
Dakota, Tennessee, Utah, Virginia, Washington, West Virginia, 
and Wisconsin.
The situation is no better across the pond. The European 
parliament decided last year to allow each member country to 
enact its own spam laws, thus avoiding a continent-wide 
directive and directly confronting the communications 
ministers of the union. Paradoxically, it also decided, three 
months ago, to restrict SMS spam. Confusion clearly reigns.


Don't Blink! 
Interview with Jeff Harrow
By: Sam Vaknin
Also published by United Press International (UPI) 

Jeff Harrow is the author and editor of the Web-based 
multimedia "Harrow Technology Report" journal and Webcast, 
available at www.TheHarrowGroup.com. He also co-authored the 
book "The Disappearance of Telecommunications". For more than 
seventeen years, beginning with "The Rapidly Changing Face of 
Computing," the Web's first and longest-running weekly 
multimedia technology journal, he has shared with people 
across the globe his fascination with technology and his sense 
of wonder at the innovations and trends of contemporary 
computing and the growing number of technologies that drive 
them.
Jeff Harrow has been the senior technologist for the Corporate 
Strategy Groups of both Compaq and Digital Equipment 
Corporation. He invented and implemented the first iconic 
network management prototype for DECnet networks.
He now works with businesses and industry groups to help them 
better understand the strategic implications of our 
contemporary and future computing environments. 
 
Q. You introduce people to innovation and technological trends 
- but do you have any hands on experience as an innovator or a 
trendsetter?
 
A. I have many patents issued and on file in the areas of 
network management and user interface technology, I am 
commercial pilot, and technology is both my vocation and my 
passion.  I bring these and other technological interests 
together to help people "look beyond the comfortable and 
obvious," so that they don't become road-kill by the side of 
the Information Highway.
 
Q. If you had to identify the five technologies with the 
maximal economic impact in the next two decades - what would 
they be?
 
A) The continuation and expansion of "Moore's Law" as it 
relates to our ability to create ever-smaller, faster, more-
capable semiconductors and nano-scale "machines."  The 
exponential growth of our capabilities in these areas will 
drive many of the other high-impact technologies mentioned 
below. 
 
B) "Nanotechnology."  As we increasingly learn to "build 
things 'upwards" from individual molecules and atoms, rather 
than by "etching things down" as we do today when building our 
semiconductors, we're learning how to create things on the 
same scale and in the same manner as Nature has done for 
billions of years.  As we perfect these techniques, entire 
industries, such as pharmaceuticals and even manufacturing 
will be radically changed. 
 
C) "Bandwidth."  For most of the hundred years of the age of 
electronics, individuals and businesses were able to 'reach 
out and touch' each other at a distance via the telephone, 
which extended their voice. This dramatically changed how 
business was conducted, but was limited to those areas where 
voice could make a difference.  
 
Similarly, now that most business operations and knowledge 
work are conducted in the digital domain via computers, and 
because we now have a global data communications network (the 
Internet) which does not restrict the type of data shared 
(voice, documents, real-time collaboration, videoconferencing, 
video-on-demand, print-on-demand, and even the creation of 
physical 3D prototype elements at a distance from 
insubstantial CAD files), business is changing yet again. 
 
Knowledge workers can now work where they wish to, rather than 
be subject to the old restrictions of physical proximity, 
which can change the concept of cities and suburbs.  Virtual 
teams can spring up and dissipate as needed without regard to 
geography or time zones.  Indeed, as bandwidth continues to 
increase in availability and plummet in cost, entire 
industries, such as the "call center," are finding a global 
marketplace that could not have existed before.  
 
Example: U.S. firms whose "800 numbers" are actually answered 
by American-sounding representatives who are working in India, 
and U.S. firms who are outsourcing "back office" operations to 
other countries with well-educated but lower-paid workforces. 
 
Individuals can now afford Internet data connections that just 
a few years ago were the expensive province of large 
corporations (e.g., cable modem and DSL service). As these 
technologies improve, and as fiber is eventually extended "to 
the curb," many industries, some not yet invented, will find 
ways to profitably consume this new resource.  We always find 
innovative ways to consume available resources. 
 
D) "Combinational Sciences."  More than any one or two 
individual technologies, I believe that the combination and 
resulting synergy of multiple technologies will have the most 
dramatic and far-reaching effects on our societies. For 
example, completing the human genome could not have taken 
place at all, much less years earlier than expected, without 
Moore's Law of computing.  
 
And now the second stage of what will be a biological and 
medical revolution, "Proteomics", will be further driven by 
advances in computing.  But in a synergistic way, computing 
may actually be driven by advances in biology which are making 
it possible, as scientists learn more about DNA and other 
organic molecules, to use them as the basis for certain types 
of computing! 
 
Other examples of "combination sciences" that synergistically 
build on one another include:
 
- Materials science and computing. For instance: carbon 
nanotubes, in some ways the results of our abilities to work 
at the molecular level due to computing research, are far 
stronger than steel and may lead to new materials with 
exceptional qualities.



 
- Medicine, biology, and materials science. For example, the 
use of transgenic goats to produce specialized "building 
materials" such as large quantities of spider silk in their 
milk, as is being done by Nexia Biotechnologies. 
 
- "Molecular Manufacturing."  As offshoots of much of the 
above research, scientists are learning how to coerce 
molecules to automatically form the structures they need, 
rather than by having to painstakingly push or prod these tiny 
building blocks into the correct places.   
 
The bottom line is that the real power of the next decades 
will be in the combination and synergy of previously separate 
fields.  And this will impact not only industries, but the 
education process as well, as it becomes apparent that people 
with broad, "cross-field" knowledge will be the ones to 
recognize the new synergistic opportunities and benefit from 
them.
 
2. Users and the public at large are apprehensive about the 
all-pervasiveness of modern applications of science and 
engineering.  People cite security and privacy concerns with 
regards to the Internet, for example. Do you believe a Luddite 
backlash is in the cards?
 
There are some very good reasons to be concerned and cautious 
about the implementation of the various technologies that are 
changing our world.  Just as with most technologies in the 
past (arrows, gunpowder, dynamite, the telephone, and more), 
they can be used for both good and ill.  And with today's 
pell-mell rush to make all of our business and personal data 
"digital," it's no wonder that issues related to privacy, 
security and more weigh on peoples' minds.   
 
As in the past, some people will choose to wall themselves off 
from these technological changes (invasions?). Yet, in the 
context of our evolving societies, the benefits of these 
technologies, as with electricity and the telephone before 
them, will outweigh the dangers for many if not most people.  
 
That said, however, it behooves us all to watch and 
participate in how these technologies are applied, and in what 
laws and safeguards are put in place, so that the end result 
is, quite literally, something that we can live with. 
 
3. Previous predictions of convergence have flunked. The 
fabled Home Entertainment Center has yet to materialize, for 
instance. What types of convergence do you deem practical and 
what will be their impact - social and economic?
 
Much of the most important and far-reaching "convergences" 
will be at the scientific and industrial levels, although 
these will trickle down to consumers and businesses in a 
myriad ways.  "The fabled Home Entertainment Center" has 
indeed not yet arrived, but not because it's technologically 
impossible - more because consumers have not been shown 
compelling reasons and results.  However, we have seen a vast 
amount of this "convergence" in different ways.  Consider the 
extent of entertainment now provided through PCs and video 
game consoles, or the relatively new class of PDA+cell phone, 
or the pocket MP3 player, or the in-car DVD, ...
 
4. Dot.coms have bombed. Now nano-technology is touted as the 
basis for a "New Economy". Are we in for the bursting of yet 
another bubble?
 
Unrealistic expectations are rarely met over the long term.  
Many people felt that the dot.com era was unrealistic, yet the 
allure of the magically rising stock prices fueled the 
eventual conflagration.  The same could happen with 
nanotechnology, but perhaps we have learned to combine our 
excitement of "the next big thing" with reasonable and 
rational expectations and business practices.  The "science" 
will come at its own pace -- how we finance that, and profit 
from it, could well benefit from the dot.bomb lessons of the 
past.  Just as with science, there's no pot of gold at the end 
of the economic rainbow. 
 
5. Moore's Law and Metcalf's Law delineate an exponential 
growth in memory, processing speed, storage, and other 
computer capacities. Where is it all going? What is the end 
point? Why do we need so much computing power on our desktops? 
What drives what - technology the cycle-consuming applications 
or vice versa?
 
There are always "bottlenecks."  Taking computers as an 
example, at any point in time we may have been stymied by not 
having enough processing power, or memory, or disk space, or 
bandwidth, or even ideas of how to consume all of the 
resources that happened to exist at a given moment.  
 
But because each of these (and many more) technologies advance 
along their individual curves, the mix of our overall 
technological capabilities keeps expanding, and this continues 
to open incredible new opportunities for those who are willing 
to color outside the lines. 
 
For example, at a particular moment in time, a college student 
wrote a program and distributed it over the Internet, and 
changed the economics and business model for the entire music 
distribution industry (Napster).  This could not have happened 
without the computing power, storage, and bandwidth that 
happened to come together at that time.  
 
Similarly, as these basic computing and communications 
capabilities have continued to grow in capacity, other 
brilliant minds used the new capabilities to create the DivX 
compression algorithm (which allows "good enough" movies to be 
stored and distributed online) and file-format-independent 
peer-to-peer networks (such as Kazaa), which are beginning to 
change the video industry in the same manner!
 
The point is that in a circular fashion, technology drives 
innovation, while innovation also enables and drives 
technology, but it's all sparked and fueled by the innovative 
minds of individuals.  Technology remains open-ended. For 
example, as we have approached certain "limits" in how we 
build semiconductors, or in how we store magnetic information, 
we have ALWAYS found ways "through" or "around" them.  And I 
see no indication that this will slow down. 



 
6. The battle rages between commercial interests and champions 
of the ethos of free content and open source software. How do 
you envisage the field ten years from now? 
 
The free content of the Internet, financed in part by the 
dot.com era of easy money, was probably necessary to bootstrap 
the early Internet into demonstrating its new potential and 
value to people and businesses.  But while it's tempting to 
subscribe to slogans such as "information wants to be free," 
the longer-term reality is that if individuals and businesses 
are not compensated for the information that they present, 
there will eventually be little information available.
 
This is not to say that advertising or traditional 
"subscriptions," or even the still struggling system of 
"micropayments" for each tidbit, are the roads to success. 
Innovation will also play a dramatic role as numerous 
techniques are tried and refined.  But overall, people are 
willing to pay for value, and the next decade will find a 
continuing series of experiments in how the information 
marketplace and its consumers come together. 
 
7. Adapting to rapid technological change is disorientating. 
Toffler called it a "future shock". Can you compare people's 
reactions to new technologies today - to their reactions, say, 
20 years ago?
 
It's all a matter of 'rate of change.'  At the beginning of 
the industrial revolution, the parents in the farms could not 
understand the changes that their children brought home with 
them from the cities, where the pace of innovation far 
exceeded the generations-long rural change process. 
 
Twenty years ago, at the time of the birth of the PC, most 
people in industrialized nations accommodated dramatically 
more change each year than early industrial-age farmer would 
have seen in his or her lifetime. Yet both probably felt about 
the same amount of "future shock," because it's relative The 
"twenty years ago" person had become accustomed to that year's 
results of the exponential growth of technology, and so was 
"prepared" for that then-current rate of change. 
 
Similarly, today, school children happily take the most 
sophisticated of computing technologies in-stride, while many 
of their parents still flounder at setting the clock on the 
VCR - because the kids simply know no other rate of change.  
It's in the perception.
 
That said, given that so many technological changes are 
exponential in nature, it's increasingly difficult for people 
to be comfortable with the amount of change that will occur in 
their own lifetime.  Today's schoolchildren will see more 
technological change in the next twenty years than I have seen 
in my lifetime to date; it will be fascinating to see how they 
(and I) cope. 



 
8. What's your take on e-books? Why didn't they take off? Is 
there a more general lesson here?
 
The E-books of the past few years have been an imperfect 
solution looking for a problem.  
 
There's certainly value in the concept of an E-book, a self-
contained electronic "document" whose content can change at  a 
whim either from internal information or from the world at 
large.  Travelers could carry an entire library with them and 
never run out of reading material.  Textbooks could reside in 
the E-book and save the backs of backpack-touting students.  
Industrial manuals could always be on-hand (in-hand!) and up 
to date.  And more.  
 
Indeed, for certain categories, such as for industrial 
manuals, the E-book has already proven valuable.  But when it 
comes to the general case, consumers found that the 
restrictions of the first E-books outweighed their benefits.  
They were expensive.  They were fragile.  Their battery life 
was very limited.  They were not as comfortable to hold or to 
read from as a traditional book. There were several 
incompatible standards and formats, meaning that content was 
available only from limited outlets, and only a fraction of 
the content that was available in traditional books was 
available in E-book form.  Very restrictive.
 
The lesson is that (most) people won't usually buy technology 
for technology's sake.  On the other hand, use a technology to 
significantly improve the right elements of a product or 
service, or its price, and stand back. 
 
9. What are the engines of innovation? what drives people to 
innovate, to invent, to think outside the box and to lead 
others to adopt their vision?
 
"People" are the engines of innovation.  The desire to look 
over the horizon, to connect the dots in new ways, and to 
color outside the lines is what drives human progress in its 
myriad dimensions.  People want to do things more easily, 
become more profitable, or simply 'do something new,' and 
these are the seeds of innovation.
 
Today, the building blocks that people innovate with can be 
far more complex than those in the past. You can create a more 
interesting innovation out of an integrated circuit that 
contains 42-million transistors today - a Pentium 4 - than you 
could out of a few single discrete transistors 30 years ago.  
 
Or today's building blocks can be far more basic (such as 
using Atomic Force Microscopes to push individual atoms around 
into just the right structure.)  These differences in scale 
determine, in part, why today's innovations seem more 
dramatic.  
 
But at its heart, innovation is a human concept, and it takes 
good ideas and persuasion to convince people to adopt the 
resulting changes.  Machines don't (yet) innovate.  And they 
may never do so, unless they develop that spark of self-
awareness that (so far) uniquely characterizes living things.  



 
Even if we get to the point where we convince our computers to 
write their own programs, at this point it does not seem that 
they will go beyond the goals that we set for them.  They may 
be able to try superhuman numbers of combinations before 
arriving at just the right one to address a defined problem, 
but they won't go beyond the problem.  Not the machines we 
know  today, at any rate.
 
On the other hand, some people, such as National Medal of 
Technology recipient Ray Kurzweil, believe that the 
exponential increase in the capabilities of our machines - 
which some estimate will reach the complexity of the human 
brain within a few decades - may result in those machines 
becoming self-aware.  
 
Don't Blink!


The Case of the Compressed Image
By: Sam Vaknin, Ph.D.
Also published by United Press International (UPI)

Also Read:
The Disruptive Engine - Innovation and the Capitalist Dream
 
Forgent Networks from Texas wants to collect a royalty every 
time someone compresses an image using the JPEG algorithm. It 
urges third parties to negotiate with it separate licensing 
agreements. It bases its claim on a 17 year old patent it 
acquired in 1997 when VTel, from which Forgent was spun-off, 
purchased the San-Jose based Compression Labs. 
The patent pertains to a crucial element in the popular 
compression method. The JPEG committee of ISO - the 
International Standards Organization - threatens to withdraw 
the standard altogether. This would impact thousands of 
software and hardware products.
This is only the latest in a serious of spats. Unisys has 
spent the better part of the last 15 years trying to enforce a 
patent it owns for a compression technique used in two other 
popular imaging standards, GIF and TIFF. BT Group sued 
Prodigy, a unit of SBC Communications, in a US federal court, 
for infringement of its patent of the hypertext link, or 
hyperlink - a ubiquitous and critical element of the Web. Dell 
Computer has agreed with the FTC to refrain from enforcing a 
graphics patent having failed to disclose it to the standards 
committee in its deliberations of the VL-bus graphics 
standard.
"Wired" reported yesterday that the Munich Upper Court 
declared "deep linking" - posting links to specific pages 
within a Web site - in violation the European Union "Database 
Directive". The directive copyrights the "selection and 
arrangement" of a database - even if the content itself is not 
owned by the database creator. It explicitly prohibits 
hyperlinking to the database contents as "unfair extraction". 
If upheld, this would cripple most search engines. Similar 
rulings - based on national laws - were handed down in other 
countries, the latest being Denmark. 
Amazon sued Barnes and Noble - and has since settled out of 
court in March - for emulating its patented "one click 
purchasing" business process. A Web browser command to 
purchase an item generates a "cookie" - a text file replete 
with the buyer's essential details which is then lodged in 
Amazon's server. This allows the transaction to be completed 
without a further confirmation step.



A clever trick, no doubt. But even Jeff Bezos, Amazon's 
legendary founder, expressed doubts regarding the wisdom of 
the US Patent Office in granting his company the patent. In an 
open letter to Amazon's customers, he called for a rethinking 
of the whole system of protection of intellectual property in 
the Internet age.
In a recently published discourse of innovation and property 
rights, titled "The Free-Market Innovation Machine", William 
Baumol of Princeton University claims that only capitalism 
guarantees growth through a steady flow of innovation. 
According to popular lore, capitalism makes sure that 
innovators are rewarded for their time and skills since 
property rights are enshrined in enforceable contracts. 
Reality is different, as Baumol himself notes. Innovators tend 
to maximize their returns by sharing their technology and 
licensing it to more efficient and profitable manufacturers. 
This rational division of labor is hampered by the 
increasingly more stringent and expansive intellectual 
property laws that afflict many rich countries nowadays. These 
statutes tend to protect the interests of middlemen - 
manufacturers, distributors, marketers - rather than the 
claims of inventors and innovators. 
Moreover, the very nature of "intellectual property" is in 
flux. Business processes and methods, plants, genetic 
material, strains of animals, minor changes to existing 
technologies - are all patentable. Trademarks and copyright 
now cover contents, brand names, and modes of expression and 
presentation. Nothing is safe from these encroaching juridical 
initiatives. Intellectual property rights have been 
transformed into a myriad pernicious monopolies which threaten 
to stifle innovation and competition.
Intellectual property - patents, content libraries, 
copyrighted material, trademarks, rights of all kinds - are 
sometimes the sole assets - and the only hope for survival - 
of cash-strapped and otherwise dysfunctional or bankrupt 
firms. Both managers and court-appointed receivers strive to 
monetize these properties and patent-portfolios by either 
selling them or enforcing the rights against infringing third 
parties. 
Fighting a patent battle in court is prohibitively expensive 
and the outcome uncertain. Potential defendants succumb to 
extortionate demands rather than endure the Kafkaesque 
process. The costs are passed on to the consumer. Sony, for 
instance already paid Forgent an undisclosed amount in May. 
According to Forgent's 10-Q form, filed on June 17, 2002, yet 
another, unidentified "prestigious international" company, 
parted with $15 million in April. 
In commentaries written in 1999-2000 by Harvard law professor, 
Lawrence Lessig, for "The Industry Standard", he observed:
"There is growing skepticism among academics about whether 
such state-imposed monopolies help a rapidly evolving market 
such as the Internet. What is "novel," "nonobvious" or 
"useful" is hard enough to know in a relatively stable field. 
In a transforming market, it's nearly impossible..."
The very concept of intellectual property is being radically 
transformed by the onslaught of new technologies.
The myth of intellectual property postulates that 
entrepreneurs assume the risks associated with publishing 
books, recording records, and inventing only because - and 
where - the rights to intellectual property are well defined 
and enforced. In the absence of such rights, creative people 
are unlikely to make their works accessible to the public. 
Ultimately, it is the public which pays the price of piracy 
and other violations of intellectual property rights, goes the 
refrain. 
This is untrue. In the USA only few authors actually live by 
their pen. Even fewer musicians, not to mention actors, eke 
out subsistence level income from their craft.  Those who do 
can no longer be considered merely creative people. Madonna, 
Michael Jackson, Schwarzenegger and Grisham are businessmen at 
least as much as they are artists. 
Intellectual property is a relatively new notion. In the near 
past, no one considered knowledge or the fruits of creativity 
(artwork, designs) as 'patentable', or as someone's 
'property'. The artist was but a mere channel through which 
divine grace flowed. Texts, discoveries, inventions, works of 
art and music, designs - all belonged to the community and 
could be replicated freely. True, the chosen ones, the 
conduits, were revered. But they were rarely financially 
rewarded. 
Well into the 19th century, artists and innovators were 
commissioned - and salaried - to produce their works of art 
and contrivances. The advent of the Industrial Revolution - 
and the imagery of the romantic lone inventor toiling on his 
brainchild in a basement or, later, a garage -  gave rise to 
the patent. The more massive the markets became, the more 
sophisticated the sales and marketing techniques, the bigger 
the financial stakes - the larger loomed the issue of 
intellectual property. 
Intellectual property rights are less about the intellect and 
more about property. In every single year of the last decade, 
the global turnover in intellectual property has outweighed 
the total industrial production of the world. These markets 
being global, the monopolists of intellectual products fight 
unfair competition globally. A pirate in Skopje is in direct 
rivalry with Bill Gates, depriving Microsoft of present and 
future revenue, challenging its monopolistic status as well as 
jeopardizing its competition-deterring image. 
The Open Source Movement weakens the classic model of property 
rights by presenting an alternative, viable, vibrant, model 
which does not involve over-pricing and anti-competitive 
predatory practices. The current model of property rights 
encourages monopolistic behavior, non-collaborative, 
exclusionary innovation (as opposed, for instance, to Linux), 
and litigiousness. The Open Source movement exposes the myths 
underlying current property rights philosophy and is thus 
subversive.
But the inane expansion of intellectual property rights may 
merely be a final spasm, threatened by the ubiquity of the 
Internet as they are. Free scholarly online publications 
nibble at the heels of their pricey and anticompetitive 
offline counterparts. Electronic publishing poses a threat - 
however distant - to print publishing. Napster-like peer to 
peer networks undermine the foundations of the music and film 
industries. Open source software is encroaching on the turf of 
proprietary applications. It is very easy and cheap to publish 
and distribute content on the Internet, the barriers to entry 
are virtually nil. 
As processors grow speedier, storage larger, applications 
multi-featured, broadband access all-pervasive, and the 
Internet goes wireless - individuals are increasingly able to 
emulate much larger scale organizations successfully. A single 
person, working from home, with less than $2000 worth of 
equipment - can publish a Webzine, author software, write 
music, shoot digital films, design products, or communicate 
with millions and his work will be indistinguishable from the 
offerings of the most endowed corporations and institutions. 
Obviously, no individual can yet match the capital assets, the 
marketing clout, the market positioning, the global branding, 
the sales organization, and the distribution network of the 
likes of Sony, or Microsoft. In an age of information glut, it 
is still the marketing, the media campaign, the distribution, 
and the sales that determine the economic outcome. 
This advantage, however, is also being eroded, albeit 
glacially. 
The Internet is essentially a free marketing and - in the case 
of digital goods - distribution channel. It directly reaches 
200 million people all over the world. Even with a minimum 
investment, the likelihood of being seen by surprisingly large 
numbers of consumers is high. Various business models are 
emerging or reasserting themselves - from ad sponsored content 
to packaged open source software. 
Many creative people - artists, authors, innovators - are 
repelled by the commercialization of their intellect and muse. 
They seek - and find - alternatives to the behemoths of 
manufacturing, marketing and distribution that today control 
the bulk of intellectual property. Many of them go freelance. 
Indie music labels, independent cinema, print on demand 
publishing - are omens of things to come.
This inexorably leads to disintermediation - the removal of 
middlemen between producer or creator and consumer. The 
Internet enables niche marketing and restores the balance 
between the creative genius and the commercial exploiters of 
his product. This is a return to pre-industrial times when 
artisans ruled the economic scene. 
Work mobility increases in this landscape of shifting 
allegiances, head hunting, remote collaboration, contract and 
agency work, and similar labour market trends. Intellectual 
property is likely to become as atomized as labor and to 
revert to its true owners - the inspired folks. They, in turn, 
will negotiate licensing deals directly with their end users 
and customers. 
Capital, design, engineering, and labor intensive goods - 
computer chips, cruise missiles, and passenger cars - will 
still necessitate the coordination of a massive workforce in 
multiple locations. But even here, in the old industrial 
landscape, the intellectual contribution to the collective 
effort will likely be outsourced to roving freelancers who 
will maintain an ownership stake in their designs or 
inventions.
This intimate relationship between creative person and 
consumer is the way it has always been. We may yet look back 
on the 20th century and note with amazement the transient and 
aberrant phase of intermediation - the Sony's, Microsoft's, 
and Forgent's of this world.
 


THE INTERNET AND THE DIGITAL DIVIDE
 
The Internet - A Medium or a Message?
By: Sam Vaknin
The State of the Net  
An Interim Report about the Future of the Internet 
 
Who are the participants who constitute the Internet? 
*	Users - connected to the net and interacting with it 
*	The communications lines and the communications equipment 
*	The intermediaries (e.g. the suppliers of on-line 
information or access providers). 
*	Hardware manufacturers 
*	Software authors and manufacturers (browsers, site 
development tools, specific applications, smart agents, 
search engines and others). 
*	The "Hitchhikers" (search engines, smart agents, 
Artificial Intelligence - AI - tools and more) 
*	Content producers and providers 
*	Suppliers of financial wherewithal (currently - corporate 
and institutional cash gradually being replaced by 
advertising money) 
The fate of each of these components - separately and in 
solidarity - will determine the fate of the Internet. 
The first phase of the Internet's history was dominated by 
computer wizards. Thus, any attempt at predicting its future 
dealt mainly with its hardware and software components. 
Media experts, sociologists, psychologists, advertising and 
marketing executives were left out of the collective effort to 
determine the future face of the Internet. 
As far as content is concerned, the Internet cannot be 
currently defined as a medium. It does not function as one - 
rather it is a very disordered library, mostly incorporating 
the writings of non-distinguished megalomaniacs. It is the 
ultimate Narcissistic experience. The forceful entry of 
publishing houses and content aggregators is changing this 
dismal landscape, though. 
Ever since the invention of television there hasn't been 
anything as begging to become a medium as the Internet. 
Three analogies spring to mind when contemplating the Internet 
in its current state: 
*	A chaotic library 
*	A neural network or the latter day equivalent of previous 
networks (telegraph, telephony, railways) 
*	A new continent 
These metaphors prove to be very useful (even business-wise). 
They permit us to define the commercial opportunities embedded 
in the Internet. 
Yet, they fail to assist us in predicting its future in its 
transformation into a medium. 
How does an invention become a medium? What happens to it when 
it does become one? What is the thin line separating the 
initial functioning of the invention from its transformation 
into a new medium? In other words: when can we tell that some 
technological advance gave birth to a new medium? 
This work also deals with the image of the Internet once 
transformed into a medium. 
The Internet has the most unusual attributes in the history of 
media. 
It has no central structure or organization. It is hardware 
and software independent. It (almost) cannot be subjected to 
legislation or to regulation. Consider the example of 
downloading music from the internet - is it tantamount to an 
act of recording music (a violation of copyright laws)? This 
has been the crux of the legal battle between Diamond 
Multimedia (the manufacturers of the Rio MP3 device), MP3.com 
and Napster and the recording industry in America. 
The Internet's data transfer channels are not linear - they 
are random. Most of its "broadcast" cannot be "received" at 
all. It allows for the narrowest of narrowcasting through the 
use of e-mail mailing lists, discussion groups, message 
boards, private radio stations, and chats. And this is but a 
small portion of an impressive list of oddities. These 
idiosyncrasies will also shape the nature of the Internet as a 
medium. Growing out of bizarre roots - it is bound to yield 
strange fruit as a medium. 
So what business opportunities does the Internet represent? 
I believe that they are to be found in two broad categories: 
*	Software and hardware related to the Internet's future as 
a medium 
*	Content creation, management and licencing 
The Map of Terra Internetica 
 
The Users 
How many Internet users are there? How many of them have 
access to the Web (World Wide Web - WWW) and use it? There are 
no unequivocal statistics. Those who presume to give the 
answers (including the ISOC - the Internet SOCiety) - rely on 
very partial and biased resources. Others just bluff. 
Yet, everyone seems to agree that there are, at least, 100 
million active participants in North America (the Nielsen and 
Commerce-Net reports). 
The future is, inevitably, even more vague than the present. 
Authoritative consultancy firms predict 66 million active 
users in 10 years time. IBM envisages 700 million users. MCI 
is more modest with 300 million. At the end of 1999 there were 
130 million registered (though not necessarily active) users. 
The Internet - an Elitist and Chauvinistic Medium 
The average user of the Internet is young (30), with an 
academic background and high income. The percentage of the 
educated and the well-to-do among the users of the Web is 
three times as high as their proportion in the population. 
This is fast changing only because their children are joining 
them (6 million already had access to the Internet at the end 
of 1996 - and were joined by another 24 million by the end of 
the decade). This may change only due to presidential 
initiatives to bridge the "digital divide" (from Al Gore's in 
the USA to Mahatir Mohammed's in Malaysia), corporate largesse 
and institutional involvement (e.g., Open Society in Eastern 
Europe, Microsoft in the USA). These efforts will spread the 
benefits of this all-powerful tool among the less privileged. 
A bit less than 50% of all users are men but they are 
responsible for 60% of the activity in the net (as measured by 
traffic). 
Women seem to limit themselves to electronic mail (e-mail) and 
to electronic shopping of goods and services, though this is 
changing fast. Men prefer information, either due to career 
requirements or because knowledge is power. 
Most of the users are of the "experiencer" variety. They are 
leaders of social change and innovative. This breed inhabits 
universities, fashionable neighbourhoods and trendy vocations. 
This is why some wonder if the Internet is not just another 
fad, albeit an incredibly resilient and promising one. 
Most users have home access to the Internet - yet, they still 
prefer to access it from work, at their employer's expense, 
though this preference is slight and being eroded. Most users 
are, therefore, exploitative in nature. Still, we must not 
forget that there are 37 million households of the self-
employed and this possibly distorts the statistical picture 
somewhat. 
The Internet - A Western Phenomenon 
Not African, not Asian (with the exception of Israel and 
Japan), not Russian , nor a Third World phenomenon. It belongs 
squarely to the wealthy, sated world. It is the indulgence of 
those who have everything and whose greatest concern is their 
choice of nightly entertainment. Between 50-60% of all 
Internet users live in the USA, 5-10% in Canada. The Internet 
is catching on in Europe (mainly in Germany and in 
Scandinavia) and, in its mobile form (i-mode) in Japan. The 
Internet lost to the French Minitel because the latter 
provides more locally relevant content and because of high 
costs of communications and hardware. 
Communications 
Most computer owners still possess a 28,800 bps modem. This is 
much like driving a bicycle on a German Autobahn. The 56,600 
bps is gradually replacing its slower predecessor (48% of 
computers with modems) - but even this is hardly sufficient. 
To begin to enjoy video and audio (especially the former) - 
data transfer rates need to be 50 times faster. 
Half the households in the USA have at least 2 telephones and 
one of them is usually dedicated to data processing (faxes or 
fax-modems). 
The ISDN could constitute the mid-term solution. This data 
transfer network is fairly speedy and covers 70% of the 
territory of the USA. It is growing by 100% annually and its 
sales topped 10 billion USD in 1995/6. 
Unfortunately, it is quite clear that ISDN is not THE answer. 
It is too slow, too user-unfriendly, has a bad interface with 
other network types, it requires special hardware. There is no 
point in investing in temporary solutions when the right 
solution is staring the Internet in the face, though it is not 
implemented due to political circumstances. 
A cable modem is 80 times speedier than the ISDN and 700 times 
faster than a 14,400 bps modem. However, it does have problems 
in accommodating a two-way data transfer. There is also need 
to connect the fibre optic infrastructure which characterizes 
cable companies to the old copper coaxial infrastructure which 
characterizes telephony. Cable users engage specially 
customized LANs (Ethernet) and the hardware is expensive 
(though equipment prices are forecast to collapse as demand 
increases). Cable companies simply did not invest in 
developing the technology. The law (prior to the 1996 
Communications Act) forbade them to do anything that was not 
one way transfer of video via cables. Now, with the more 
liberal regulative environment, it is a mere question of time 
until the technology is found. 
Actually, most consumers single out bad customer relations as 
their biggest problem with the cable companies - rather than 
technology. 
Experiments conducted with cable modems led to a doubling of 
usage time (from an average of 24 to 47 hours per month per 
user) which was wholly attributable to the increased speed. 
This comes close to a cultural revolution in the allocation of 
leisure time. Numerically speaking: 7 million households in 
the USA are fitted with a two-way data transfer cable modems. 
This is a small number and it is anyone's guess if it 
constitutes a critical mass. Sales of such modems amount to 
1.3 billion USD annually. 
50% of all cable subscribers also have a PC at home. To me it 
seems that the merging of the two technologies is inevitable. 
Other technological solutions - such as DSL, ADSL, and the 
more promising satellite broadband - are being developed and 
implemented, albeit slowly and inefficiently. Coverage is 
sporadic and frustrating waiting periods are measured in 
months. 
Hardware and Software 
Most Internet users (82%) work with the Windows operating 
system. About 11% own a Macintosh (much stronger graphically 
and more user-friendly). Only 7% continue to work on UNIX 
based systems (which, historically, fathered the Internet) - 
and this number is fast declining. A strong entrant is the 
free source LINUX operating system. 



Virtually all users surf through a browsing software. A fast 
dwindling minority (26%) use Netscape's products (mainly 
Navigator and Communicator) and the majority use Microsoft's 
Explorer (more than 60% of the market). Browsers are now free 
products and can be downloaded from the Internet. As late as 
1997, it was predicted by major Internet consultancy firms 
that browser sales will top $4 billion by the year 2000. Such 
misguided predictions ignored the basic ethos of the Internet: 
free products, free content, free access.
Browsers are in for a great transformation. Most of them are 
likely to have 3-D, advanced audio, telephony / voice / video 
mail (v-mail), instant messaging, e-mail, and video 
conferencing capabilities integrated into the same browsing 
session. They will become self-customizing, intelligent, 
Internet interfaces. They will memorize the history of usage 
and user preferences and adapt themselves accordingly. They 
will allow content-specificity: unidentifiable smart agents 
will scour the Internet, make recommendations, compare prices, 
order goods and services and customize contents in line with 
self-adjusting user profiles. 
Two important technological developments must be considered: 
PDAs (Personal Digital Assistants) - the ultimate personal 
(and office) communicators, easy to carry, they provide 
Internet (access) Everywhere, independent of suppliers and 
providers and of physical infrastructure (in an aeroplane, in 
the field, in a cinema). 
The second trend: wireless data transfer and wireless e-mail, 
whether through pagers, cellular phones, or through more 
sophisticated apparatus and hybrids such as smart phones. 
Geotech's products are an excellent example: e-mail, faxes, 
telephone calls and a connection to the Internet and to other, 
public and corporate, or proprietary, databases - all provided 
by the same gadget. This is the embodiment of the electronic, 
physically detached, office. Wearable computing should be 
considered a part of this "ubiquitous or pervasive computing" 
wave. 
We have no way of gauging - or intelligently guessing - the 
part of the mobile Internet in the total future Internet 
market but it is likely to outweigh the "fixed" part. Wireless 
internet meshes well with the trend of pervasive computing and 
the intelligent home and office. Household gadgets such as 
microwave ovens, refrigerators and so on will connect to the 
internet via a wireless interface to cull data, download 
information, order goods and services, report their condition 
and perform basic maintenance functions. Location specific 
services (navigation, shopping recommendations, special 
discounts, deals and sales, emergency services) depend on the 
technological confluence between GPS (satellite-based 
geolocation technology) and wireless Internet.
Suppliers and Intermediaries 
"Parasitic" intermediaries occupy each stage in the Internet's 
food chain. 
Access to the Internet is still provided by "dumb pipes" - the 
Internet Service Providers (ISP) 
Content is still the preserve of content suppliers and so on. 
Some of these intermediaries are doomed to gradually fade or 
to suffer a substantial diminishing of their share of the 
market. Even "walled gardens" of content (such as AOL) are at 
risk.
By way of comparison, even today, ISPs have four times as many 
subscribers (worldwide) as AOL. Admittedly, this adversely 
affects the quality of the Internet - the infrastructure 
maintained by the phone companies is slow and often succumbs 
to bottlenecks. The unequivocal intention of the telephony 
giants to become major players in the Internet market should 
also be taken into account. The phone companies will, thus, 
play a dual role: they will provide access to their 
infrastructure to their competitors (sometimes, within a real 
or actual monopoly) - and they will compete with their 
clients. The same can be said about the cable companies. 
Controlling the last mile to the user's abode is the next big 
business of the Internet. Companies such as AOL are 
disadvantaged by these trends. It is imperative for AOL to 
obtain equal access to the cable company's backbone and 
infrastructure if it wants to survive. Hence its merger with 
Time Warner. 
No wonder that many of the ISPs judge this intrusion on their 
turf by the phone and cable companies to constitute unfair 
competition. Yet, one should not forget that the barriers to 
entry are very low in the ISP market. It takes a minimal 
investment to become an ISP. 200 modems (which cost 200 USD 
each) are enough to satisfy the needs of 2000 average users 
who generate an income of 500,000 USD per annum to the ISP. 
Routers are equally as cheap nowadays. This is a nice return 
on the ISP's capital, undoubtedly. 
The Hitchhikers 
The Web houses the equivalent of 100 billion pages. Search 
Engine applications are used to locate specific information in 
this impressive, constantly proliferating library. They will 
be replaced, in the near future, by "Knowledge Structures" - 
gigantic encyclopaedias, whose text will contain references 
(hyperlinks) to other, relevant, sites. The far future will 
witness the emergence of the "Intelligent Archives" and the 
"Personal Newspapers" (read further for detailed 
explanations). Some software applications will summarize 
content, others will index and automatically reference and 
hyperlink texts (virtual bibliographies). An average user will 
have an on-going interest in 500 sites. Special software will 
be needed to manage address books ("bookmarks", "favourites") 
and contents ("Intelligent Addressbooks"). The phenomenon of 
search engines dedicated to search a number of search engines 
simultaneously will grow ("Hyper- or meta- engines"). Meta-
engines will work in the background and download hyperlinks 
and advertising (the latter is essential to secure the 
financial interest of site developers and owners). Statistical 
software which tracks ("how long was what done"), monitors 
("what did they do while in the site") and counts ("how many") 
visitors to sites already exists. Some of these applications 
have back-office facilities (accounting, follow-up, 
collections, even tele-marketing). They all provide time 
trails and some allow for auditing. 



This is but a small fragment of the rapidly developing net-
scape: people and enterprises who make a living off the 
Internet craze rather than off the Internet itself. Everyone 
knows that there is more money in lecturing about how to make 
money on the Internet - than in the Internet itself. This 
maxim still holds true despite the 32 billion US dollars in E-
commerce in 1998. Business to Consumer (B2C) sales grow less 
vigorously than Business to Business (B2B) sales and are 
likely to suffer another blow with the advent of Peer to Peer 
(P2P) computer networks. The latter allow PCs to act as 
servers and thus enable the swapping of computer files asmong 
connected users (with or without a central directory). 
Content Suppliers 
This is the underprivileged sector of the Internet. They all 
lose money (even e-tailers which offer basic, standardized 
goods - books, CDs - with the exception, until September 11, 
of sites connected to tourism). No one thanks them for content 
produced with the investment of a lot of effort and a lot of 
money. A really qualitative, fully commerce enabled site costs 
up to 5,000,000 USD, excluding site maintenance and customer 
and visitor services. Content providers are constantly 
criticized for lack of creativity or for too much creativity. 
More and more is asked of them. They are exploited by 
intermediaries, hitchhikers and other parasites. This is all 
an off-shoot of the ethos of the Internet as a free content 
area. 
More than 100 million men and women constantly access the Web 
- but this number stands to grow (the median prediction: 300 
million). Yet, while the Web is used by 35% of those with 
access to the Internet - e-mail is used by more than 60%. E-
mail is by far the most common function ("killer app") and 
specialized applications (Eudora, Internet Mail, Microsoft 
Exchange) - free or ad sponsored - keep it accessible to all 
and user-friendly. 
Most of the users like to surf (browse, visit sites) the net 
without reason or goal in mind. This makes it difficult to 
apply traditional marketing techniques. 
What is the meaning of "targeted audiences" or "market shares" 
in this context? 
If a surfer visits sites which deal with aberrant sex and 
nuclear physics in the same session - what to make of it? 
The public and legislative backlash against the gathering of 
surfers' data by Internet ad agencies and other web sites - 
has led to growing ignorance regarding the profile of Internet 
users, their demography, habits, preferences and dislikes. 
People like the very act of surfing. They want to be 
entertained, then they use the Internet as a working tool, 
mostly in the service of their employer, who, usually foots 
the bill. Users love free downloads (mainly software). 
"Free" is a key word on the Internet: it used to belong to the 
US Government and to a bunch of universities. Users like 
information, with emphasis on news and data about new 
products. But they do not like to shop on the net - yet. Only 
38% of all surfers made a purchase during 1998. 
67% of them adore virtual sex. 50% of the sites most often 
visited are porn sites (this is reminiscent of the early days 
of the Video Cassette Recorder - VCR). People dedicate the 
same amount of time to watching video cassettes or television 
as they do to surfing the net. The Internet seems to 
cannibalize television. 
Sex is followed by music, sports, health, television, 
computers, cinema, politics, pets and cooking sites. People 
are drawn to interactive games. The Internet will shortly 
enable people to gamble, if not hampered by legislation. 10 
billion USD in gambling money are predicted to pass through 
the net. This makes sense: nothing like a computer to provide 
immediate (monetary and psychological) rewards. 
Commerce on the net is another favourite. The Internet is a 
perfect medium for the sale of software and other digital 
products (e-books). The problem of data security is on its way 
to being solved with the SET (or other) world standard. 
As early as 1995, the Internet had more than 100 virtual 
shopping malls visited by 2.5 million shoppers (and probably 
double this number in 1996). 
The predictions for 1999 were between 1-5 billion USD of net 
shopping (plus 2 billion USD through on-line information 
providers, such as CompuServe and AOL) - proved woefully 
inaccurate. The actual number in 1998 was 7 times the 
prediction for 1999. 
It is also widely believed that circa 20% of the family budget 
will pass through the Internet as e-money and this amounts to 
150 billion USD. 
The Internet will become a giant inter-bank clearing system 
and varied ATM type banking and investment services will be 
provided through it. Basically, everything can be done through 
the Internet: looking for a job, for instance. 
Yet, the Internet will never replace human interaction. People 
are likely to prefer personal banking, window shopping and the 
social experience of the shopping mall to Internet banking and 
e-commerce, or m-commerce. 
Some sites already sport classified ads. This is not a bad way 
to defray expenses, though most classified ads are free (it is 
the advertising they attract that matters). 
Another developing trend is website-rating and critique. It 
will be treated the way today's printed editions are. It will 
have a limited influence on the consumption decisions of some 
users. Browsers already sport buttons labelled "What's New" 
and "What's Hot". Most Search Engines recommend specific 
sites. Users are cautious. Studies discovered that no user, no 
matter how heavy, has consistently re-visited more than 200 
sites, a minuscule number. The 10 most popular web sites 
(Yahoo!, MSN, etc.) attracted more than 50% of all Internet 
traffic. Site recommendation services often produce random - 
at times, wrong - selections for their user. There are also 
concerns regarding privacy issues. The backlah against 
Amazon's "readers' circles" is an example. 



Web Critics, who work today mainly for the printed press, will 
publish their wares on the net and will link to intelligent 
software which will hyperlink, recommend and refer. Some web 
critics will be identified with specific applications - 
really, expert systems which will incorporate their knowledge 
and experience. 
The Money 
Where will the capital needed to finance all these 
developments come from? 
Again, there are two schools: 
One says that sites will be financed through advertising - and 
so will search engines and other applications accessed by 
users. 
Certain ASPs (Application Service Providers which rent out 
access to application software which resides on their servers) 
are considering this model. 
The second version is simpler and allows for the existence of 
non-commercial content. 
It proposes to collect negligible sums (cents or fractions of 
cents) from every user for every visit ("micro-payments") or a 
subscription fee. These accumulated cents or subscription fees 
will enable the owners of old sites to update and to maintain 
them and encourage entrepreneurs to develop new ones. Certain 
content aggregators (especially of digital textbooks) have 
adopted this model (Questia, Fathom). 
The adherents of the first school pointed at the 5 million USD 
invested in advertising during 1995 and to the 60 million or 
so invested during 1996. 
Its opponents point exactly at the same numbers: ridiculously 
small when contrasted with more conventional advertising 
modes. The potential of advertising on the net is limited to 
1.5 billion USD annually in 1998, thundered the pessimists 
(many thought that even half that would be very nice). The 
actual figure was double the prediction but still woefully 
small and inadequate to support the Internet's content 
development. 
Compare these figures to the sale of Internet software ($4 
billion), Internet hardware ($3 billion), Internet access 
provision ($4.2 billion) in 1995. 
Hembrecht and Quist estimated that Internet related industries 
scooped up 23.2 billion USD annually (A report released in 
mid-1996). 
And what follows advertising is hardly more enocuraging.
The consumer interacts and the product is delivered to him. 
This - the delivery phase - is a slow and enervating epilogue 
to the exciting affair of ordering through the net at the 
speed of light. Too many consumers still complain that they do 
not receive what they ordered, or that delivery is late and 
products defective. 
The solution may lie in the integration of advertising and 
content. Pointcast, for instance, integrated advertising into 
its news broadcasts, continuously streamed to the user's 
screen, even when inactive (they provided a downloadable 
active screen saver and ticker in a "push technology"). 
Downloading of digital music, video and text (e-books) will 
lead to immediate gratification of the consumer and will 
increase the efficacy of advertising. 
Whatever the case may be, a uniform, agreed upon system of 
rating as a basis for charging advertisers, is sorely needed. 
There is also the question of what does the advertiser pay 
for? 
Many advertisers (Procter and Gamble, for instance) refuse to 
pay according to the number of hits or impressions (=entries, 
visits to a site). They agree to pay only according to the 
number of the times that their advertisement was hit (page 
views).
This different basis for calculation is likely to upset all 
revenue scenarios. 
Very few sites of important, respectable newspapers are on a 
subscription basis. Dow Jones (Wall Street Journal) and The 
Economist, to mention but two. 
Will this become the prevailing trend?
The Internet as a Metaphor 
 
Three metaphors come to mind when considering the Internet 
"philosophically". 
The Internet as a Chaotic Library 
1. The Problem of Cataloguing
The Internet is an assortment of billions of pages containing 
information. Some of them are visible and others are generated 
from hidden databases by users' requests ("Invisible 
Internet"). 
The Internet displays no discernible order, classification, or 
categorization. As opposed to "classical" libraries, no one 
has invented a cataloguing standard (remember Dewey?). This is 
so needed that it is amazing that it has not been invented 
yet. Some sites indeed apply the Dewey Decimal Syatem 
(Suite101). Others default to a directory structure (Open 
Directory, Yahoo!, Look Smart and others). 
Had such a standard existed (an agreed upon numerical 
cataloguing method) - each site would have self-classified. 
Sites would have an interest to do so to increase their 
penetration rates and their visibility. This, naturally, would 
have eliminated the need for today's clunky, incomplete and 
(highly) inefficient search engines. 
A site whose number starts with 900 will be immediately 
identified as dealing with history and multiple classification 
will be encouraged to allow finer cross-sections to emerge. An 
example of such an emerging technology of "self 
classification" and "self-publication" (though limited to 
scholarly resources) is the "Academic Resource Channel" by 
Scindex. 
Users will not be required to remember reams of numbers. 
Future browsers will be akin to catalogues, very much like the 
applications used in modern day libraries. Compare this utopia 
to the current dystopy. Users struggle with reams of 
irrelevant material to finally reach a partial and 
disappointing destination. At the same time, there likely are 
web sites which exactly match the poor user's needs. Yet, what 
currently determines the chances of a happy encounter between 
user and content - are the whims of the specific search engine 
used and things like meta-tags, headlines, a fee paid, or the 
right opening sentences. 
2. Screen versus Page
The computer screen, because of physical limitations (size, 
the fact that it has to be scrolled) fails to effectively 
compete with the printed page. The latter is still the most 
ingenious medium yet invented for the storage and release of 
textual information. Granted: a computer screen is better at 
highlighting discrete units of information. So, this draws the 
batlle lines: structures (printed pages) versus units 
(screen), the continuous and easily reversible versus the 
discrete. 
The solution is an efficient way to translate computer screens 
to printed matter. It is hard to believe, but no such thing 
exists. Computer screens are still hostile to off-line 
printing. In other words: if a user copies information from 
the Internet to his Word Processor (or vice versa, for that 
matter) - he ends up with a fragmented, garbage-filled and 
non-aesthetic document. 
Very few site developers try to do something about it - even 
fewer succeed. 
3. The Internet and the CD-ROM
One of the biggest mistakes of content suppliers is that they 
do not mix contents or have a "static-dynamic interaction". 
The Internet can now easily interact with other media 
(especially with audio CDs and with CD-ROMs) - even as the 
user surfs. 
Examples abound: 
A shopping catalogue can be distributed on a CD-ROM by mail. 
The Internet Site will allow the user to order a product 
previously selected from the catalogue, while off-line. The 
catalogue could also be updated through the site (as is done 
with CD-ROM encyclopedias). 
The advantages of the CD-ROM are clear: very fast access time 
(dozens of times faster than the access to a site using a dial 
up connection) and a data storage capacity tens of times 
bigger than the average website. 
Another example: a CD-ROM can be distributed, containing 
hundreds of advertisements. The consumer will select the ad 
that he wants to see and will connect to the Internet to view 
a relevant video. 
He could then also have an interactive chat (or a conference) 
with a salesperson, receive information about the company, 
about the ad, about the advertising agency which created the 
ad - and so on. 
CD-ROM based encyclopedias (such as the Britannica, Encarta, 
Grolier) already contain hyperlinks which carry the user to 
sites selected by an Editorial Board. 
But CD-ROMs are probably a doomed medium. This industry chose 
to emphasize the wrong things. Storage capacity increased 
exponentially and, within a year, desktops with 80 Gb hard 
disks will be common. Moreover, the Network Computer - the 
stripped down version of the personal computer - will put at 
the disposal of the average user terabytes in storage capacity 
and the processing power of a supercomputer. What separates 
computer users from this utopia is the communication 
bandwidth. With the introduction of radio, statellite, ADSL 
broadband services, cable modems and compression methods - 
video (on demand), audio and data will be available speedily 
and plentifully. 
The CD-ROM, on the other hand, is not mobile. It requires 
installation and the utilization of sophisticated hardware and 
software. This is no user friendly push technology. It is 
nerd-oriented. As a result, CD-ROMs are not an immediate 
medium. There is a long time lapse between the moment they are 
purchased and the moment the first data become accessible to 
the user. Compare this to a book or a magazine. Data in these 
oldest of media is instantly available to the user and allows 
for easy and accurate "back" and "forward" functions. 
Perhaps the biggest mistake of CD-ROM manufacturers has been 
their inability to offer an integrated hardware and software 
package. CD-ROMs are not compact. A Walkman is a compact 
hardware-cum-software package. It is easily transportable, it 
is thin, it contains numerous, user-friendly, sophisticated 
functions, it provides immediate access to data. So does the 
discman or the MP3-man. This cannot be said of the CD-ROM. By 
tying its future to the obsolete concept of stand-alone, 
expensive, inefficient and technologically unreliable personal 
computers - CD-ROMs have sentenced themselves to oblivion 
(with the possible exception of reference material). 
4. On-line Reference Libraries
These already exist. A visit to the on-line Encyclopaedia 
Britannica exemplifies some of the tremendous, mind boggling 
possibilities: 
Each entry is hyperlinked to sites on the Internet which deal 
with the same subject matter. The sites are carefully screened 
(though more detailed descriptions of each site should be 
available - they could be prepared either by the staff of the 
encyclopaedia or by the site owner). Links are available to 
data in various forms, including audio and video. Everything 
can be copied to the hard disk or to CD-ROMs. 



This is a new conception of a knowledge centre - not just an 
assortment of material. It is modular, can be added on and 
subtracted from. It can be linked to a voice Q&A centre. 
Queries by subscribers can be answered by e-mail, by fax, 
posted on the site, hard copies can be sent by post. This 
"Trivial Pursuit" service could be very popular - there is 
considerable appetite for "Just in Time Information". The 
Library of Congress - together with a few other libraries - is 
in the process of making just such a service available to the 
public (CDRS - Collaborative Digital Reference Service). 
5. The Feedback Option
Hard to believe, but very few sites encourage their guests to 
express an opinion about the site, its contents and its 
aesthetics. This indicates an ossified mode of thinking about 
the most dynamic mass medium ever created, the only 
interactive mass medium yet. Each site must absolutely contain 
feedback and rating questionnaires. It has the side benefit of 
creating a database of the visitors to the site. 
Moreover, each site can easily become a "knowledge centre". 
Let us consider a site dedicated to advertising and marketing: 
It can contain feedback questionnaires (what do you think 
about the site, suggestions for improvement, mailto and leave 
message facilities, etc.) 
It can contain rating questionnaires (rate these ads, these TV 
or radio shows, these advertising campaigns). 
It can allocate some space to clients to create their home 
pages in (these home pages could lead to their sites, to other 
sites, to other sections of the host site - and, in any case, 
will serve as a display of the creative talent of the site 
owners). This will give the site owners a picture of the 
distribution of the areas of interest of the visitors to the 
site. 
The site can include statistical, tracking and counter 
software. 
Such a site can refer to hundreds of useful shareware 
applications (which deal with different aspects of advertising 
and marketing, for instance). Developers of applications will 
be able to use the site to promote their products. Other 
practical applications could also be referred to from - or 
reside on - the site (browsers, games, search engines). 
And all this can be organized in a portal structure (for 
instance, by adopting the open software of the Open Directory 
Project).
6. Internet Derived CD-ROMS
The Internet is an enormous reservoir of freely available, 
public domain, information. 
With a minimal investment, this information can be gathered 
into coherent, theme oriented, cheap CD-ROMs. Each such CD-ROM 
can contain: 
   Addresses of web sites specific to the subject matter 
*	The first pages of each of these sites 
*	Hyperlinks to each of the sites 
*	A browser 
*	Access to all the important search engines 
*	Recommended search strings (it is extremely difficult to 
formulate a successful search in the Internet, it takes 
expertise. "Ready-made searches" will be a hit in the 
future, as the number of sites grows) 
*	A dictionary of professional terms, a speller and a 
thesaurus 
*	A list of general reference sites 
*	Shareware specific to the field 
7. Publishing
The Internet is the world's largest "publisher", by far. It 
"publishes" FAQs (Frequent Answers and Questions regarding 
almost every technical matter in the world), e-zines 
(electronic versions of magazines, not a very profitable 
pursuit), the electronic versions of dailies (together with 
on-line news and information services), reference and other e-
books, monographs, articles and minutes of discussions 
("threads"), among other types of material. 
Publishing an e-zine has a few advantages: it promotes the 
sales of the printed edition, it helps to sign on subscribers 
and it leads to the sale of advertising space. The electronic 
archive function (see next section) saves the need to file 
back issues, the space required to do so and the irritating 
search for data items. 
The future trend is a combined subscription: electronic 
(mainly for the archival value and the ability to hyperlink to 
additional information) and printed (easier to browse current 
issue). 
The electronic daily presents other advantages: 
It allows for immediate feedback and for flowing, almost real-
time, communication between writers and readers. The 
electronic version, therefore, acquires a gyroscopic function: 
a navigation instrument, always indicating deviations from the 
"right" course. The content can be instantly updated and 
immediacy has its premium (remember the Lewinsky affair?). 
Strangely, this (conventional) field was the first to develop 
a "virtual reality" facet. There are virtual "magazine 
stalls". They look exactly like the real thing and the user 
can buy a paper using his mouse. 
Specialty hand held devices already allow for downloading and 
storage of vast quantities of data (up to 4000 print pages). 
The user gains access to libraries containing hundreds of 
texts, adapted to be downloaded, stored and read by the 
specific device. Again, a convergence of standards is to be 
expected in this field as well (the final contenders will 
probably be Adobe's PDF against Microsoft's MS-Reader). 



Broadly, e-books are treated either as: 
Continuation of print books (p-books) by other means  
or as  
A whole new publishing universe. 
Since p-books are a more convenient medium then e-books - they 
will prevail in any straightforward "medium replacement" or 
"medium displacement" battle. 
In other words, if publishers will persist in the simple and 
straightforward conversion of p-books to e-books - then e-
books are doomed. They are simply inferior to the price, 
comfort, tactile delights, browseability and scanability of p-
books. 
But e-books - being digital - open up a vista of hitherto 
neglected possibilities. These will only be enhanced and 
enriched by the introduction of e-paper and e-ink. Among them: 
*	Hyperlinks within the e-book and without it - to web 
content, reference works, etc. 
*	Embedded instant shopping and ordering links 
*	Divergent, user-interactive, decision driven plotlines 
*	Interaction with other e-books (using a wireless 
standard) - collaborative authoring 
*	Interaction with other e-books - gaming and community 
activities 
*	Automatically or periodically updated content 
*	Multimedia 
*	Database, Favourites and History Maintenance (reading 
habits, shopping habits, interaction with other readers, 
plot related decisions and much more) 
*	Automatic and embedded audio conversion and translation 
capabilities 
*	Full wireless piconetworking and scatternetworking 
capabilities 
The technology is still not fully there. Wars rage in both the 
wireless and the ebook realms. Platforms compete. Standards 
clash. Gurus debate. But convergence is inevitable and with it 
the e-book of the future. 
8. The Archive Function
The Internet is also the world's biggest cemetery: tens of 
thousands of deadbeat sites, still accessible - the "Ghost 
Sites" of this electronic frontier. 
This, in a way, is collective memory. One of the Internet's 
main functions will be to preserve and transfer knowledge 
through time. It is called "memory" in biology - and "archive" 
in library science. The history of the Internet is being 
documented by search engines (Google) and specialized services 
(Alexa) alike.



 
 
The Internet as a Collective Brain 
  
Drawing a comparison from the development of a human baby - 
the human race has just commenced to develop its neural 
system. 
The Internet fulfils all the functions of the Nervous System 
in the body and is, both functionally and structurally, pretty 
similar. It is decentralized, redundant (each part can serve 
as functional backup in case of malfunction). It hosts 
information which is accessible in a few ways, it contains a 
memory function, it is multimodal (multimedia - textual, 
visual, audio and animation). 
I believe that the comparison is not superficial and that 
studying the functions of the brain (from infancy to 
adulthood) - amounts to perusing the future of the Net itself. 
1. The Collective Computer
To carry the metaphor of "a collective brain" further, we 
would expect the processing of information to take place in 
the Internet, rather than inside the end-user's hardware (the 
same way that information is processed in the brain, not in 
the eyes). Desktops will receive the results and communicate 
with the Net to receive additional clarifications and 
instructions and to convey information gathered from their 
environment (mostly, from the user). 
This is part fo the philosophy of the JAVA programming 
language. It deals with applets - small bits of software - and 
links different computer platforms by means of software. 
Put differently: 
Future servers will contain not only information (as they do 
today) - but also software applications. The user of an 
application will not be forced to buy it. He will not be 
driven into hardware-related expenditures to accommodate the 
ever growing size of applications. He will not find himself 
wasting his scarce memory and computing resources on passive 
storage. Instead, he will use a browser to call a central 
computer. This computer will contain the needed software, 
broken to its elements (=applets, small applications). Anytime 
the user wishes to use one of the functions of the 
application, he will siphon it off the central computer. When 
finished - he will "return" it. Processing speeds and response 
times will be such that the user will not feel at all that it 
is not with his own software that he is working (the question 
of ownership will be very blurred in such a world). This 
technology is available and it provoked a heated debated about 
the future shape of the computing industry as a whole 
(desktops - really power packs - or network computers, a 
little more than dumb terminals). Applications are already 
offered to corporate users by ASPs (Application Service 
Providers). 



In the last few years, scientists put the combined power of 
the computers linked to the internet at any given moment to 
perform astounding feats of distributed parallel processing. 
Millions of PCs connected to the net co-process signals from 
outer space, meteorological data and solve complex equations. 
This is a prime example of a collective brain in action. 
2. The Intranet - a Logical Extension of the Collective 
Computer
LANs (Local Area Networks) are no longer a rarity in corporate 
offices. WANs (wide Area Networks) are used to connect 
geographically dispersed organs of the same legal entity 
(branches of a bank, daughter companies, a sales force). Many 
LANs are wireless. 
The intranet / extranet and wireless LANs will be the winners. 
They will gradually eliminate both fixed line LANs and WANs. 
The Internet offers equal, platform-independent, location-
independent and time of day - independent access to all the 
members of an organization.Sophisticated firewall security 
application protects the privacy and confidentiality of the 
intranet from all but the most determined and savvy hackers. 
The Intranet is an inter-organizational communication network, 
constructed on the platform of the Internet and which enjoys 
all its advantages. The extranet is open to clients and 
suppliers as well. 
The company's server can be accessed by anyone authorized, 
from anywhere, at any time (with local - rather than 
international - communication costs). The user can leave 
messages (internal e-mail or v-mail), access information - 
proprietary or public - from it and to participate in "virtual 
teamwork" (see next chapter). 
By the year 2002, a standard intranet interface will emerge. 
This will be facilitated by the opening up of the TCP/IP 
communication architecture and its availability to PCs. A 
billion USD will go just to finance intranet servers - or, at 
least, this is the median forecast. 
The development of measures to safeguard server routed inter-
organizational communication (firewalls) is the solution to 
one of two obstacles to the institution of the Intranet. The 
second problem is the limited bandwidth which does not permit 
the efficient transfer of audio (not to mention video). 
It is difficult to conduct video conferencing through the 
Internet. Even the voices of discussants who use internet 
phones come out (slightly) distorted. 
All this did not prevent 95% of the Fortune 1000 from 
installing intranet. 82% of the rest intend to install one by 
the end of this year. Medium to big size American firms have 
50-100 intranet terminals per every internet one. 
At the end of 1997, there were 10 web servers per every other 
type of server in organizations. The sale of intranet related 
software was projected to multiply by 16 (to 8 billion USD) by 
the year 1999. 
One of the greatest advantages of the intranet is the ability 
to transfer documents between the various parts of an 
organization. Consider Visa: it pushed 2 million documents per 
day internally in 1996. 
An organization equipped with an intranet can (while protected 
by firewalls) give its clients or suppliers access to non-
classified correspondence. This notion has its  charm. 
Consider a newspaper: it can give access to all the materials 
which were discarded by the editors. Some news are fit to 
print - yet are discarded because of  space limitations. 
Still, someone is bound to be interested. It costs the 
newspaper close to nothing (the material is, normally, already 
computer-resident) - and it might even generate added 
circulation and income. It can be even conceived as an 
"underground, non-commercial, alternative" newspaper for a 
wholly different readership. 
The above is but one example of the possible use of the 
intranet to communicate with the organization's consumer base. 
3. Mail and Chat
The Internet (its e-mail possibilities) is eroding traditional 
mail. The market share of the post office in conveying 
messages by regular mail has dwindled from 77% to 62% (1995). 
E-mail has expanded to capture 36% (up from 19%). 
90% of customers with on-line access use e-mail from time to 
time and 60% work with it regularly. More than 2 billion 
messages traverse the internet daily. 
E-mail applications are available as freeware and are included 
in all browsers. Thus, the Internet has completely assimilated 
what used to be a separate service, to the extent that many 
people make the mistake of thinking that e-mail is a feature 
of the Internet. Microsoft continues to incorporate previously 
independent applications in its browsers - a behaviour which 
led to the 1999 anti-trust lawsuit against it. 
The internet will do to phone calls what it has done to mail. 
Already there are applications (Intel's, Vocaltec's, 
Net2Phone) which enable the user to conduct a phone 
conversation through his computer. The voice quality has 
improved. The discussants can cut into each others words, 
argue and listen to tonal nuances. Today, the parties (two or 
more) engaging in the conversation must possess the same 
software and the same (computer) hardware. In the very near 
future, computer-to-regular phone applications will eliminate 
this requirement. And, again, simultaneous multi-modality: the 
user can talk over the phone, see his party, send e-mail, 
receive messages and transfer documents - without obstructing 
the flow of the conversation. 
The cost of transferring voice will become so negligible that 
free voice traffic is conceivable in 3-5 years. Data traffic 
will overtake voice traffic by a wide margin. 
This beats regular phones. 



The next phase will probably involve virtual reality. Each of 
the parties will be represented by an "avatar", a 3-D figurine 
generated by the application (or the user's likeness mapped 
into the software and superimposed on the the avatar). These 
figurines will be multi-dimensional: they will possess their 
own communication patterns, special habits, history, 
preferences - in short: their own "personality". 
Thus, they will be able to maintain an "identity" and a 
consistent pattern of communication which they will develop 
over time. 
Such a figure could host a site, accept, welcome and guide 
visitors, all the time bearing their preferences in its 
electronic "mind". It could narrate the news, like "Ananova" 
does. Visiting sites in the future is bound to be a much more 
pleasant affair. 
4. E-cash
In 1996, the four corporate giants (Visa, MasterCard, Netscape 
and Microsoft) agreed on a standard for effecting secure 
payments through the Internet: SET. Internet commerce is 
supposed to mushroom by a factor of 50 to 25 billion USD. Site 
owners will be able to collect rent from passing visitors - or 
fees for services provided within the site. Amazon instituted 
an honour system to collect donations from visitors. Dedicated 
visitors will not be deterred by such trifles. 
5. The Virtual Organization
The Internet allows simultaneous communication between an 
almost unlimited number of users. This is coupled with the 
efficient transfer of multimedia (video included) files. 
This opens up a vista of mind boggling opportunities which are 
the real core of the Internet revolution: the virtual 
collaborative ("Follow the Sun") modes. 
Examples: 
A group of musicians will be able to compose music or play it 
- while spatially and temporally separated; 
Advertising agencies will be able to co-produce ad campaigns 
in a real time interactive mode; 
Cinema and TV films will be produced from disparate 
geographical spots through the teamwork of people who never 
meet, except through the net. 
These examples illustrate the concept of the "virtual 
community". Locations in space and time will no longer hinder 
a collaboration in a team: be it scientific, artistic, 
cultural, or for the provision of services (a virtual law firm 
or accounting office, a virtual consultancy network). 
Two on going developments are the virtual mall and the virtual 
catalogue. 
There are well over 300 active virtual malls in the Internet. 
They were frequented by 32.5 million shoppers, who shopped in 
them for goods and services in 1998. The intranet can also be 
thought of as a "virtual organization", or a "virtual 
business". 
The virtual mall is a computer "space" (pages) in the 
internet, wherein "shops" are located. These shops offer their 
wares using visual, audio and textual means. The visitor 
passes a gate into the store and looks through its offering, 
until he reaches a buying decision. Then he engages in a 
feedback process: he pays (with a credit card), buys the 
product and waits for it to arrive by mail. The manufacturers 
of digital products (intellectual property such as e-books or 
software) have begun selling their merchandise on-line, as 
file downloads. 
Yet, slow communications and limited bandwidth - constrain the 
growth potential of this mode of sale. Once solved - 
intellectual property will be sold directly from the net, on-
line. Until such time, the intervention of the Post Office is 
still required. So, then virtual mall is nothing but a 
glorified computerized mail catalogue or Buying Channel, the 
only difference being the exceptionally varied inventory. 
Websites which started as "specialty stores" are fast 
transforming themselves into multi-purpose virtual malls. 
Amazon.com, for instance, has bought into a virtual pharmacy 
and into other virtual businesses. It is now selling music, 
video, electronics and many other products. It started as a 
bookstore. 
This contrasts with a much more creative idea: the virtual 
catalogue. It is a form of narrowcasting (as opposed to 
broadcasting): a surgically accurate targeting of potential 
consumer audiences. Each group of profiled consumers (no 
matter how small) is fitted with their own - digitally 
generated - catalogue. This is updated daily: the variety of 
wares on offer (adjusted to reflect inventory levels, consumer 
preferences and goods in transit) - and prices (sales, 
discounts, package deals) change in real time. 
The user will enter the site and there delineate his 
consumption profile and his preferences. A customized 
catalogue will be immediately generated for him. 
From then on, the history of his purchases, preferences and 
responses to feedback questionnaires will be accumulated and 
added to a database. 
Each catalogue generated for him will come replete with order 
forms. Once the user concluded his purchases, his profile will 
be updated. 
There is no technological obstacles to implementing this 
vision today - only administrative and legal ones. Big retail 
stores are not up to processing the flood of data expected to 
arrive. They also remain highly sceptical regarding the 
feasibility of the new medium. And privacy issues prevent data 
mining or the effective collection and usage of personal data. 
The virtual catalogue is a private case of a new internet off-
shoot: the "smart (shopping) agents". These are AI 
applications with "long memories". 
They draw detailed profiles of consumers and users and then 
suggest purchases and refer to the appropriate sites, 
catalogues, or virtual malls. 
They also provide price comparisons and the new generation 
(NetBot) cannot be blocked or fooled by using differing 
product categories. 
In the future, these agents will refer also to real life 
retail chains and issue a map of the branch or store closest 
to an address specified by the user (the default being his 
residence). This technology can be seen in action in a few 
music sites on the web and is likely to be dominant with 
wireless internet appliances. The owner of an internet enabled 
(third generation) mobile phone is likely to be the target of 
geographically-specific marketing campaigns, ads and special 
offers pertaining to his current location (as reported by his 
GPS - satellite Geographic Positioning System). 
6. Internet News
Internet news are advantaged. They can be frequently and 
dynamically updated (unlike static print news) and be always 
accessible (similar to print news), immediate and fresh. 
The future will witness a form of interactive news. A special 
"corner" in the site will be open to updates posted by the 
public (the equivalent of press releases). This will provide 
readers with a glimpse into the making of the news, the raw 
material news are made of. The same technology will be applied 
to interactive TVs. Content will be downloaded from the 
internet and be displayed as an overlay on the TV screen or in 
a square in a special location. The contents downloaded will 
be directly connected to the TV programming. Thus, the 
biography and track record of a football player will be 
displayed during a football match and the history of a country 
when it gets news coverage. 
Terra Internetica - Internet, an Unknown Continent 
  
This is an unconventional way to look at the Internet. Laymen 
and experts alike talk about "sites" and "advertising space". 
Yet, the Internet was never compared to a new continent whose 
surface is infinite. 
The Internet will have its own real estate developers and 
construction companies. The real life equivalents derive their 
profits from the scarcity of the resource that they exploit - 
the Internet counterparts will derive their profits from the 
tenants (the content). 
Two examples: 
A few companies bought "Internet Space" (pages, domain names, 
portals), developed it and make commercial use of it by: 
*	renting it out 
*	constructing infrastructure and selling it 
*	providing an intelligent gateway, entry point to the rest 
of the internet 
*	or selling advertising space which subsidizes the tenants 
(Yahoo!-Geocities, Tripod and others). 
*	Cybersquatting (purchasing specific domain names 
identical to brand names in the "real" world) and then 
selling the domain name to an interested party 
Internet Space can be easily purchased or created. The 
investment is low and getting lower with the introduction of 
competition in the field of domain registration services and 
the increase in the number of top domains. 
Then, infrastructure can be erected - for a shopping mall, for 
free home pages, for a portal, or for another purpose. It is 
precisely this infrastructure that the developer can later 
sell, lease, franchise, or rent out. 
At the beginning, only members of the fringes and the avant-
garde (inventors, risk assuming entrepreneurs, gamblers) 
invest in a new invention. The invention of a new 
communications technology is mostly accompanied by devastating 
silence. 
No one knows to say what are the optimal uses of the invention 
(in other words, what is its future). Many - mostly members of 
the scientific and business elites - argue that there is no 
real need for the invention and that it substitutes a new and 
untried way for old and tried modes of doing the same thing 
(so why assume the risk?) 
These criticisms are usually founded: 
To start with, there is, indeed, no need for the new medium. A 
new medium invents itself - and the need for it. It also 
generates its own market to satisfy this newly found need. 
Two prime examples are the personal computer and the compact 
disc. 
When the PC was invented, its uses were completely unclear. 
Its performance was lacking, its abilities limited, it was 
horribly user unfriendly. 
It suffered from faulty design, absent user comfort and ease 
of use and required considerable professional knowledge to 
operate. The worst part was that this knowledge was unique to 
the new invention (not portable). 
It reduced labour mobility and limited one's professional 
horizons. There were many gripes among those assigned to tame 
the new beast. 
The PC was thought of, at the beginning, as a sophisticated 
gaming machine, an electronic baby-sitter. As the presence of 
a keyboard was detected and as the professional horizon 
cleared it was thought of in terms of a glorified typewriter 
or spreadsheet. It was used mainly as a word processor (and 
its existence justified solely on these grounds). The 
spreadsheet was the first real application and it demonstrated 
the advantages inherent to this new machine (mainly 
flexibility and speed). Still, it was more (speed) of the 
same. A quicker ruler or pen and paper. What was the 
difference between this and a hand held calculator (some of 
them already had computing, memory and programming features)? 
The PC was recognized as a medium only 30 years after it was 
invented with the introduction of multimedia software. All 
this time, the computer continued to spin off markets and 
secondary markets, needs and professional specialities. The 
talk as always was centred on how to improve on existing 
markets and solutions. 
The Internet is the computer's first important breakthrough. 
Hitherto the computer was only quantitatively different - the 
multimedia and the Internet have made it qualitatively 
superior, actually, sui generis, unique. 
This, precisely, is the ghost haunting the Internet: 
It has been invented, is maintained and is operated by 
computer professionals. For decades these people have been 
conditioned to think in Olympic terms: more, stronger, higher. 
Not: new, unprecedented, non-existent. To improve - not to 
invent. They stumbled across the Internet - it invented itself 
despite its own creators. 
Computer professionals (hardware and software experts alike) - 
are linear thinkers. The Internet is non linear and modular. 
It is still the age of hackers. There is still a lot to be 
done in improving technological prowess and powers. But their 
control of the contents is waning and they are being gradually 
replaced by communicators, creative people, advertising 
executives, psychologists and the totally unpredictable masses 
who flock to flaunt their home pages. 
These all are attuned to the user, his mental needs and his 
information and entertainment preferences. 
The compact disc is a different tale. It was intentionally 
invented to improve upon an existing technology (basically, 
Edison's Gramophone). Market-wise, this was a major gamble: 
the improvement was, at first, debatable (many said that the 
sound quality of the first generation of compact discs was 
inferior to that of its contemporaneous record players). 
Consumers had to be convinced to change both software and 
hardware and to dish out thousands of dollars just to listen 
to what the manufacturers claimed was better quality Bach. A 
better argument was the longer life of the software (though 
contrasted with the limited life expectancy of the consumer, 
some of the first sales pitches sounded absolutely morbid). 
The computer suffered from unclear positioning. The compact 
disc was very clear as to its main functions - but had a rough 
time convincing the consumers. 
Every medium is first controlled by the technical people. 
Gutenberg was a printer - not a publisher. Yet, he is the 
world's most famous publisher. The technical cadre is joined 
by dubious or small-scale entrepreneurs and, together, they 
establish ventures with no clear vision, market-oriented 
thinking, or orderly plan of action. The legislator is also 
dumbfounded and does not grasp what is happening - thus, there 
is no legislation to regulate the use of the medium. Witness 
the initial confusion concerning copyrighted software and the 
copyrights of ROM embedded software. Abuse or under-
utilization of resources grow. Recall the sale of radio 
frequencies to the first cellular phone operators in the West 
- a situation which repeats itself in Eastern and Central 
Europe nowadays. 
But then more complex transactions - exactly as in real estate 
in "real life" - begin to emerge. 
This distinction is important. While in real life it is 
possible to sell an undeveloped plot of land - no one will buy 
"pages". The supply of these is unlimited - their scarcity 
(and, therefore, their virtual price) is zero. 
The second example involves the utilization of a site - rather 
than its mere availability. 
A developer could open a site wherein first time authors will 
be able to publish their first manuscript - for a fee. 
Evidently, such a fee will be a fraction of what it would take 
to publish a "real life" book. The author could collect money 
for any downloading of his book - and split it with the site 
developer. The potential buyers will be provided with access 
to the contents and to a chapter of the books. This is 
currently being done by a few fledgling firms but a full scale 
publishing industry has not yet developed. 
The Life of a Medium 
  
The internet is simply the latest in a series of networks 
which revolutionized our lives. A century before the internet, 
the telegraph, the railways, the radio and the telephone have 
been similarly heralded as "global" and transforming. 
Every medium of communications goes through the same 
evolutionary cycle: 
Anarchy 
The Public Phase 
At this stage, the medium and the resources attached to it are 
very cheap, accessible, under no regulatory constraints. The 
public sector steps in: higher education institutions, 
religious institutions, government, not for profit 
organizations, non governmental organizations (NGOs), trade 
unions, etc. Bedevilled by limited financial resources, they 
regard the new medium as a cost effective way of disseminating 
their messages. 
The Internet was not exempt from this phase which ended only a 
few years ago. It started with a complete computer anarchy 
manifested in ad hoc networks, local networks, networks of 
organizations (mainly universities and organs of the 
government such as DARPA, a part of the defence establishment, 
in the USA). Non commercial entities jumped on the bandwagon 
and started sewing these networks together (an activity fully 
subsidized by government funds). The result was a globe 
encompassing network of academic institutions. The American 
Pentagon established the network of all networks, the ARPANET. 
Other government departments joined the fray, headed by the 
National Science Foundation (NSF) which withdrew only lately 
from the Internet. 
The Internet (with a different name) became semi-public 
property - with access granted to the chosen few. 
Radio took precisely this course. Radio transmissions started 
in the USA in 1920. Those were anarchic broadcasts with no 
discernible regularity. Non commercial organizations and not 
for profit organizations began their own broadcasts and even 
created radio broadcasting infrastructure (albeit of the cheap 
and local kind) dedicated to their audiences. Trade unions, 
certain educational institutions and religious groups 
commenced "public radio" broadcasts. 



The Commercial Phase 
When the users (e.g., listeners in the case of the radio, or 
owners of PCs and modems in the example of the Internet) reach 
a critical mass - the business sector is alerted. In the name 
of capitalist ideology (another religion, really) it demands 
"privatization" of the medium. This harps on very sensitive 
strings in every Western soul: the efficient allocation of 
resources which is the result of competition, corruption and 
inefficiency naturally associated with the public sector 
("Other People's Money" - OPM), the ulterior motives of 
members of the ruling political echelons (the infamous 
American Paranoia), a lack of variety and of catering to the 
tastes and interests of certain audiences, the equation 
private enterprise = democracy and more. 
The end result is the same: the private sector takes over the 
medium from "below" (makes offers to the owners or operators 
of the medium - that they cannot possibly refuse) - or from 
"above" (successful lobbying in the corridors of power leads 
to the appropriate legislation and the medium is 
"privatized"). 
Every privatization - especially that of a medium - provokes 
public opposition. There are (usually founded) suspicions that 
the interests of the public were compromised and sacrificed on 
the altar of commercialization and rating. Fears of 
monopolization and cartelization of the medium are evoked - 
and justified, in due time. Otherwise, there is fear of the 
concentration of control of the medium in a few hands. All 
these things do happen - but the pace is so slow that the 
initial fears are forgotten and public attention reverts to 
fresher issues. 
A new Communications Act was legislated in the USA in 1934. It 
was meant to transform radio frequencies into a national 
resource to be sold to the private sector which will use it to 
transmit radio signals to receivers. In other words: the radio 
was passed on to private and commercial hands. Public radio 
was doomed to be marginalized. 
The American administration withdrew from its last major 
involvement in the Internet in April 1995, when the NSF ceased 
to finance some of the networks and, thus, privatized its 
hitherto heavy involvement in the net. 
A new Communications Act was legislated in 1996. It permitted 
"organized anarchy". It allowed media operators to invade each 
other's territories. 
Phone companies will be allowed to transmit video and cable 
companies will be allowed to transmit telephony, for instance. 
This is all phased over a long period of time - still, it is a 
revolution whose magnitude is difficult to gauge and whose 
consequences defy imagination. It carries an equally momentous 
price tag - official censorship. "Voluntary censorship", to be 
sure, somewhat toothless standardization and enforcement 
authorities, to be sure - still, a censorship with its own 
institutions to boot. The private sector reacted by 
threatening litigation - but, beneath the surface it is caving 
in to pressure and temptation, constructing its own censorship 
codes both in the cable and in the internet media.


 
Institutionalization 
This phase is the next in the Internet's history, though, it 
seems, unbeknownst to it. 
It is characterized by enhanced activities of legislation. 
Legislators, on all levels, discover the medium and lurch at 
it passionately. Resources which were considered "free", 
suddenly are transformed to "national treasures not to be 
dispensed with cheaply, casually and with frivolity". 
It is conceivable that certain parts of the Internet will be 
"nationalized" (for instance, in the form of a licensing 
requirement) and tendered to the private sector. Legislation 
will be enacted which will deal with permitted and disallowed 
content (obscenity? incitement? racial or gender bias?) 
No medium in the USA (not to mention the wide world) has 
eschewed such legislation. There are sure to be demands to 
allocate time (or space, or software, or content, or hardware) 
to "minorities", to "public affairs", to "community business". 
This is a tax that the business sector will have to pay to 
fend off the eager legislator and his nuisance value. 
All this is bound to lead to a monopolization of hosts and 
servers. The important broadcast channels will diminish in 
number and be subjected to severe content restrictions. Sites 
which will not succumb to these requirements - will be deleted 
or neutralized. Content guidelines (euphemism for censorship) 
exist, even as we write, in all major content providers 
(CompuServe, AOL, Geocities, Tripod, Prodigy). 
The Bloodbath 
This is the phase of consolidation. The number of players is 
severely reduced. The number of browser types will be limited 
to 2-3 (Netscape, Microsoft and which else?). Networks will 
merge to form privately owned mega-networks. Servers will 
merge to form hyper-servers run on supercomputers in "server 
farms". The number of ISPs will be considerably cut. 
50 companies ruled the greater part of the media markets in 
the USA in 1983. The number in 1995 was 18. At the end of the 
century they will number 6. 
This is the stage when companies - fighting for financial 
survival - strive to acquire as many users/listeners/viewers 
as possible. The programming is shallowed to the lowest (and 
widest) common denominator. Shallow programming dominates as 
long as the bloodbath proceeds. 



From Rags to Riches 
Tough competition produces four processes: 
1. A Major Drop in Hardware Prices
This happens in every medium but it doubly applies to a 
computer-dependent medium, such as the Internet. 
Computer technology seems to abide by "Moore's Law" which says 
that the number of transistors which can be put on a chip 
doubles itself every 18 months. As a result of this 
miniaturization, computing power quadruples every 18 months 
and an exponential series ensues. Organic-biological-DNA 
computers, quantum computers, chaos computers - prompted by 
vast profits and spawned by inventive genius will ensure the 
longevity and continued applicability of Moore's Law. 
The Internet is also subject to "Metcalf's Law". 
It says that when we connect N computers to a network - we get 
an increase of N to the second power in its computing / 
processing power. And these N computers are more powerful 
every year, according to Moore's Law. 
The growth of computing powers in networks is a multiple of 
the effects of the two laws. More and more computers with ever 
increasing computing power get connected and create an 
exponential 16 times growth in the network's computing power 
every 18 months. 
2. Free Availability of Software and Connection
This is prevalent in the Net where even potentially commercial 
software can be downloaded for free. In many countries 
television viewers still pay for television broadcasts - but 
in the USA and many other countries in the West, the basic 
package of television channels comes free of charge. 
As users / consumers form a habit of using (or consuming) the 
software - it is commercialized and begins to carry a price 
tag. This is what happened with the advent of cable 
television: contents are sold for subscription and usage (Pay 
Per View - PPV) fees. 
Gradually, this is what will happen to most of the sites and 
software on the Net. Those which survive will begin to collect 
usage fees, access fees, subscription fees, downloading fees 
and other, appropriately named, fees. These fees are bound to 
be low - but it is the principle that counts. Even a few cents 
per transaction will accumulate to hefty sums with the traffic 
which will characterize the Net (or, at least its more popular 
locales). 
Adverising revenues will allow ISPs to offer free 
communication and storage volume. Gradually, connect time 
charges imposed by the phone companies will be eroded by tough 
competition from the likes of the cable companies. Accessing 
the internet might well be free of all charges in 10 years 
time. 
3. Increased User Friendliness
As long as the computer is less user friendly and less 
reliable (predictable) than television - less of a black box - 
its potential (and its future) is limited. Television attracts 
3.5 billion users daily. The Internet will attract - under the 
most exuberant scenario - less than one tenth of this number 
of people. The only reasons for this disparity are (the lack 
of) user friendliness and reliability. Even browsers, among 
the most user friendly applications ever - are not 
sufficiently so. The user still needs to know how to use a 
keyboard and must possess some basic acquaintance with the 
operating system. 
The more mature the medium, the more friendly it becomes. 
Finally, it will be operated using speech or common language. 
There will be room left for user "hunches" and built in 
flexible responses. 
4. Social Taxes
Sooner or later, the business sector has to mollify the God of 
public opinion by offerings of political and social nature. 
The Internet is an affluent, educated, yuppie medium. It 
necessitates a control of the English language, live interest 
in information and its various uses (scientific, commercial, 
other), a lot of resources (free time, money to invest in 
hardware, software and connect time). It empowers - and thus 
deepens the divide between the haves and have-nots, the 
knowing and the ignorant, the computer illiterate. 
In short: the Internet is an elitist medium. Publicly, this is 
an unhealthy posture. "Internetophobia" is already 
discernible. People (and politicians) talk about how unsafe 
the Internet is and about its possible uses for racial, sexist 
and pornographic purposes. The wider public is in a state of 
awe. 
So, site builders and owners will do well to begin to improve 
their image: provide free access to schools and community 
centres, bankroll internet literacy classes, freely distribute 
contents and software to educational institutions, collaborate 
with researchers and social scientists and engineers. 
In short: encourage the view that the Internet is a medium 
catering to the needs of the community and the 
underprivileged, a mostly altruist endeavour. This also 
happens to make good business sense by educating a future 
generation of users. He who visited a site when a student, 
free of charge - will pay to do so when made an executive. 
Such a user will also pass on the information within and 
without his organization. This is called media exposure. 
The future will, no doubt, witness public Internet terminals, 
subsidized ISP accounts, free Internet classes and an 
alternative "non-commercial, public" approach to the Net. 



 
 
The Internet: Medium or Chaos? 
  
There has never been a medium like the Internet. The way it 
has formed, the way it was (not) managed, its hardware-
software-communications specifications - are all unique. 
No Government 
The Internet has no central (or even decentralized) structure. 
In reality, it hardly has a structure at all. It is a 
collection of 16 million computers (end 1996) connected 
through thousands of networks. There are organizations which 
purport to set Internet standards (like the aforementioned 
ISOC, or the domain setting ICANN) - but they are all 
voluntary organizations, with no binding legal, enforcement, 
or adjudication powers. The result is often mayhem. 
Many erroneously call the Internet the first democratic 
medium. Yet, it hardly qualifies as a medium and by no stretch 
of terminology is it democratic. Democracy has institutions, 
hierarchies, order. The Internet has none of these things. 
There are some vague understandings as to what is and is not 
allowed. This is a "code of honour" (more reminiscent of the 
Sicilian Mob than of the British Parliament, let's say). 
Violations are punished by excommunication (of the violating 
site or person). 
The Internet has culture - but no education. Freedom of Speech 
is entrenched. Members of this virtual community react 
adversely to ideas of censorship, even when applied to hard 
core porno. In 1999, hackers hacked major government sites 
following an FBI initiative against hacking-related crimes. 
Government initiatives (in the USA, in France, the lawsuit 
against the General Manager of AOL in Germany) are acutely 
criticized. In the meantime, the spirit of the Internet 
prevails: the small man's medium. What seems to be emerging, 
though, is self censorship by content providers (such as AOL 
and CompuServe). 
Independence 
The Internet is not dependent upon a given hardware or 
software. True, it is accessible only through computers and 
there are dominant browsers. 
But the Internet accommodates any digital (bit transfer) 
platform. Internet will be incorporated in the future into 
portable computers, palmtops, PDAs, mobile phones, cable 
television, telephones (with voice interface), home appliances 
and even wrist watches. It will be accessible to all, 
regardless of hardware and software. 
The situation is, obviously, different with other media. There 
is standard hardware (the television set, the radio receiver, 
the digital print equipment). Data transfer modes are 
standardized as well. The only variable is the contents - and 
even this is standardized in an age of American cultural 
imperialism. Today, one can see the same television programs 
all over the globe, regardless of cultural or geographical 
differences. 
Here is a reasonable prognosis for the Internet: 
It will "broadcast" (it is, of course, a PULL medium, not a 
PUSH medium - see next chapter) to many kinds of hardware. Its 
functions will be controlled by 2-5 very common software 
applications. But it will differ from television in that 
contents will continue to be decentralized: every point on the 
Net is a potential producer of content at low cost. This is 
the equivalent of producing a talk show using a single home 
video camera. And the contents will remain varied. 
Naturally, marketing content (sites) will remain an expensive 
art. Sites will also be richer or poorer, in accordance with 
the investment made in them. 
Non Linearity and Functional Modularity 
The Internet is the first medium in human history that is non-
linear and totally modular. 
A television program is broadcast from a transmitter, through 
the airwaves to a receiver (=the television set). The viewer 
sits opposite this receiver and passively watches. This is an 
entirely linear process. The Internet is different: 
When communicating through the Internet, there is no way to 
predict how the information will reach its destination. The 
routing of information through the network is completely 
random, very much like the principle governing the telephony 
system (but on a global scale). The latter is not a point-to-
point linear network. Rather, it is a network of networks. Our 
voice is transmitted back and forth inside a gigantic maze of 
copper wires and optic fibres. It seeps through any available 
wire - until it reaches its destination. 
It is the same with the Internet. 
Information is divided to packets. An address is attached to 
each packet and - using the TCP/IP data transfer protocol - is 
dispatched to roam this worldwide labyrinth. But the path from 
one neighbourhood of London to another may traverse Japan. 
The really ingenious thing about the Internet is that each 
computer (each receiver or end user) indeed burdens the system 
by imposing on it its information needs (as is the case with 
other media) - but it also assists in the task of pushing 
information packets on to their destinations. It seems that 
this contribution to the system outweighs the burdens imposed 
upon it. 
The network has a growth potential which is always bigger than 
the number of its users. It is as though television sets 
assisted in passing the signals received by them to other 
television sets. Every computer which is a member of the 
network is both a message (content) and a medium (active 
information channel), both a transmitter and a receiver. If 
30% of all computers on the Net were to crash - there will be 
no operational impact (there is enormous built in redundancy). 
Obviously, some contents will no longer be available 
(information channels will be affected). 
The interactivity of this medium is a guarantee against the 
monopolization of contents. Anyone with a thousand dollars can 
launch his/her own (reasonably sophisticated) site, accessible 
to all other Internet users. Space is available through home 
page providers. 
The name of the game is no longer the production - it is the 
creative content (design), the content itself and, above all, 
the marketing of the site. 
The Internet is an infinite and unlimited resource. This goes 
against the grain of the most basic economic concept (of 
scarcity). Each computer that joins the Internet strengthens 
it exponentially - and tens of thousands join daily. The 
Internet infrastructure (maybe with the exception of 
communication backbones) can accommodate an annual growth of 
100% to the year 2020. It is the user who decides whether to 
increase the Internet's infrastructure by connecting his 
computer to it. By comparison: it is as though it were 
possible to produce and to broadcast radio programmes from 
every radio receiver. Each computer is a combination of studio 
and transmitter (on the Internet). 
In reality, there is no other interactive medium except the 
Internet. Cable TV does not allow two-way data transfer (from 
user to cable operator). If the user wants to buy a product - 
he has to phone. Interactive television is an abject failure 
(the Sony and TCI experiments were terminated). This all is 
notwithstanding the combining of the Internet with satellite 
capabilities (VSAT) or with the revenant digital television. 
The television screen is inferior when compared to the 
computer screen. Only the Internet is there as a true two-way 
possibility. The technological problems that besieged it are 
slowly dissipating. 
The Internet allows for one-dimensional and bi - dimensional 
interactivity. 
One-dimensional interactivity: fill in and dispatch a form, 
send and receive messages (through e-mail or v-mail). 
Two-dimensional interactivity: to talk to someone while both 
parties work on an application, to see your conversant, to 
talk to him and to transfer documents to him for his perusal 
as the conversation continues apace. 
This is no longer science fiction. In less than five years 
this will be as common as the telephone - and it will have a 
profound effect on the traditional services provided by the 
phone companies. Internet phones, Internet videophones - they 
will be serious competitors and the phone companies are likely 
to react once they begin to feel the heat. This will happen 
when the Internet will acquire black box features. Phone 
companies, software giants and cable TV operators are likely 
to end up owning big chunks of the lucrative future market of 
the Net. 
The Solitary Medium 
The Internet is NOT a popular medium. It is the medium of 
affluent executives who fully master the English language, as 
part of a wider general education. 
Alternatively, it is the medium of academia (students, 
lecturers), or of children of the former, well-to-do group. In 
any case, it is not the medium of the "wide public". It is 
also a highly individualistic medium. 
The Internet was an initiative of the DOD (Department of 
Defence in the USA). It was later "requisitioned" by the 
National science Fund (NSF) in the USA. This continuous 
involvement of the administration came to an end in 1995 when 
the medium was "privatized". 
This "privatization" was a recognition of the civilian roots 
of the Internet. It was - and is still being - formed by 
millions of information-intoxicated users. They formed 
networks to exchange bits and pieces of mutual interest. Thus, 
as opposed to all other media, the Internet was not invented, 
nor was its market. The inventors of the telephone, the 
telegraph, the radio, the television and the compact disc - 
all invented previously non-existent markets for their 
products. It took time, effort and money to convince consumers 
that they needed these "gadgets". 
By contrast, the Internet was invented by its own consumers 
and so was the market for it. Only when the latter was fully 
forged did producers and businessmen join in. Microsoft began 
to hesitantly test the internet waters only in 1995! 
On Line Memories 
The Internet is the only medium with online memory, very much 
like the human brain. The memories of these two - the Net and 
the Brain - are immediately accessible. In both, it is stored 
in sites and in both, it does not grow old or is eliminated. 
It is possible to find sites which commemorate events the same 
way that the human mind registers them. This is Net Memory. 
The history of a site can be reviewed. The Library of Congress 
stores the consecutive development phases of sites. The 
Internet is an amazing combination of data processing 
software, data, a record of all the activities which took 
place in connection with the data and the memory of these 
records. Only the human brain is recalled by these capacities: 
one language serves all these functions, the language of the 
neurones. 
There is a much clearer distinction even in computers (not to 
mention more conventional media, such as television). 
Raw English - the Language of Raw Materials 
The following - apparently trivial - observation is critical: 
All the other media provide us with processed, censored, 
"clean" content. 
The Internet is a medium of raw materials, partly well 
organized (the rough equivalent of a newspaper) - and partly 
still in raw form, yesterday's supper. 
This is a result of the immediate and absolute access afforded 
each user: access to programming and site publishing tools - 
as well as access to computer space on servers. This leads to 
varying degrees of quality of contents and content providers 
and this, in turn, prevents monopolization and cartelization 
of the information supply channels. 



The users of the Internet are still undecided: do they prefer 
drafts or newspapers. They frequent well designed sites. There 
are even design competitions and awards. But they display a 
preference for sites that are constantly updated (i.e. closer 
in their nature to a raw material - rather than to a finished 
product). They prefer sites from which they can download 
material to quietly process at home, alone, on their PCs, at 
their leisure. 
Even the concept of "interactivity" points at a preference for 
raw materials with which one can interact. For what is 
interactivity if not the active involvement of the user in the 
creation of content? 
The Internet users love to be involved, to feel the power in 
their fingertips, they are all addicted to one form of power 
or another. 
Similarly, a car completely automatically driven and navigated 
is not likely to sell well. Part of the experience of driving 
- the sensation of power ("power stirring") - is critical to 
the purchase decision. 
It is not in vain that the metaphor for using the Internet is 
"surfing" (and not, let's say, browsing). 
The problem is that the Internet is still predominantly an 
English language medium (though it is fast changing). It 
discriminates against those whose mother tongue is different. 
All software applications work best in English. Otherwise they 
have to be adapted and fitted with special fonts (Hebrew, 
Arabic, Japanese, Russian and Chinese - each present a 
different set of problems to overcome). This situation might 
change with the attainment of a critical mass of users (some 
say, 2 million per non-Anglophone country). 
Comprehensive (Virtual) Reality 
This is the first (though, probably, not the last) medium 
which allows the user to conduct his whole life within its 
boundaries. 
Television presents a clear division: there is a passive 
viewer. His task is to absorb information and subject it to 
minimal processing. The Internet embodies a complete and 
comprehensive (virtual) reality, a full fledged alternative to 
real life. 
The illusion is still in its infancy - and yet already 
powerful. 
The user can talk to others, see them, listen to music, see 
video, purchase goods and services, play games (alone or with 
others scattered around the globe), converse with colleagues, 
or with users with the same hobbies and areas of interest, to 
play music together (separated by time and space). 
And all this is very primitive. In ten years time, the 
Internet will offer its users the option of video conferencing 
(possibly, three dimensional, holographic). The participants' 
figures will be projected on big screens. Documents will be 
exchanged, personal notes, spreadsheets, secret counteroffers. 
Virtual Reality games will become reality in less time. 
Special end-user equipment will make the player believe that 
he, actually, is part of the game (while still in his room). 
The player will be able to select an image borrowed from a 
database and it will represent him, seen by all the other 
players. Everyone will, thus, end up invading everyone else's 
private space - without encroaching on his privacy! 
The Internet will be the medium of choice for phone and 
videophone communication (including conferencing). 
Many mundane activities will be done through Internet: 
banking, shopping for standard items, etc. 
The above are examples to the Internet's power and ability to 
replace our reality in due time. A world out there will 
continue to exist - but, more and more we will interact with 
it through the enchanted interface of the Net. 
 
A Brave New Net
  
The future of a medium in the making is difficult to predict. 
Suffice it to mention the ridiculous prognoses which 
accompanied the PC (it is nothing but a gaming gadget, it is a 
replacement for the electric typewriter, will be used only by 
business). The telephone also had its share of ludicrous 
statements: no one - claimed the "experts" would like to avoid 
eye contact while talking. Or television: only the Nazi regime 
seemed to have fully grasped its potential (in the Berlin 1936 
Olympics). And Bill Gates thought that the internet has a very 
limited future as late as 1995!!! 
Still, this medium has a few characteristics which 
differentiate it from all its predecessors. Were these traits 
to be continuously and creatively exploited - a few statements 
can be made about the future of the Net with relative 
assurance. 
Time and Space Independence 
This is the first medium in history which does not require the 
simultaneous presence of people in space-time in order to 
facilitate the transfer of information. Television requires 
the existence of studio technicians, narrators and others in 
the transmitting side - and the availability of a viewer in 
the receiving side. The phone is dependent on the existence of 
two or more parties simultaneously. 
With time, tools to bridge the time gap between transmitter 
and receiver were developed. The answering machine and the 
video cassette recorder both accumulate information sent by a 
transmitter - and release it to a receiver in a different 
space and time. But they are discrete, their storage volume is 
limited and they do not allow for interaction with the 
transmitter. 
The Internet does not have these handicaps. 
It facilitates the formation of "virtual organizations / 
institutions / businesses/ communities". These are groups of 
users that communicate in different points in space and time, 
united by a common goal or interest. 
A few examples: 
The Virtual Advertising Agency 
A budget executive from the USA will manage the account of a 
hi-tech firm based in Sydney. He will work with technical 
experts from Israel and with a French graphics office. They 
will all file their work (through the intranet) in the Net, to 
be studied by the other members of this virtual group. These 
will enter the right site after clearing a firewall security 
software. They will all be engaged in flexiwork (flexible 
working times) and work from their homes or offices, as they 
please. Obviously, they will all abide by a general schedule. 
They will exchange audio files (the jingle, for instance), 
graphics, video, colour photographs and text. They will 
comment on each other's work and make suggestions using e-
mail. The client will witness the whole creative process and 
will be able to contribute to it. There is no technological 
obstacle preventing the participation of the client's clients, 
as well. 
Virtual Rock'n'Roll 
It is difficult to imagine that "virtual performances will 
replace real life ones. 
The mass rock concert has its own inimitable sounds, palette 
and smells. But a virtual production of a record is on the 
cards and it is tens of percents cheaper than a normal 
production. Again, the participants will interact through the 
Intranet. They will swap notes, play their own instruments, 
make comments by e-mail, play together using an appropriate 
software. If one of them is grabbed by inspiration in the 
middle of (his) night, he will be able to preserve and pass on 
his ideas through the Net. The creative process will be aided 
by novel applications which enable the simultaneous transfer 
of sound over the Net. The processes which are already 
digitized (the mix, for one) will pose no problem to a 
digitized medium. Other applications will let the users listen 
to the final versions and even ask the public for his preview 
opinion. 
Thus, even creative processes which are perceived as demanding 
human presence - will no longer do so with the advent of the 
Net. 
Perhaps it is easier to understand a Virtual Law Firm or 
Virtual Accountants Office. 
In the extreme, such a firm will not have physical offices, at 
all. The only address will be an e-mail address. Dozens of 
lawyers from all over the world with hundreds of specialities 
will be partners in such an office. Such an office will be 
truly multinational and multidisciplinary. It will be fast and 
effective because its members will electronically swap 
information (precedents, decrees, laws, opinions, research and 
plain ideas or professional experience). 



It will be able to service clients in every corner of the 
globe. It will involve the transfer of audio files 
(NetPhones), text, graphics and video (crucial in certain 
types of litigation). Today, such information is sent by post 
and messenger services. Whenever different types of 
information are to be analysed - a physical meeting is a must. 
Otherwise, each type of information has to be transferred 
separately, using unique equipment for each one. 
Simultaneity and interactivity - this will be the name of the 
game in the Internet. The professional term is "Coopetition" 
(cooperation between potential competitors, using the 
Internet). 
Other possibilities: a virtual production of a movie, a 
virtual research and development team, a virtual sales force. 
The harbingers of the virtual university, the virtual 
classroom and the virtual (or distance) medical centre are 
here. 
The Internet - Mother of all Media 
The Internet is the technological solution to the mythological 
"home entertainment centre" debate. 
It is almost universally agreed that, in the future, a typical 
home will have one apparatus which will give it access to all 
types of information. Even the most daring did not talk about 
simultaneous access to all the types of information or about 
full interactivity. 
The Internet will offer exactly this: access to every 
conceivable type of information simultaneously , the ability 
to process them at the same time and full interactivity. The 
future image of this home centre is fairly clear - it is the 
timing that is not. It is all dependent on the availability of 
a wide (information) band - through which it will be possible 
to transfer big amounts of data at high speeds, using the same 
communications line. Fast modems were coupled with optic 
fibres and with faulty planning and vision of future needs. 
The cable television industry, for instance, is totally 
technologically unprepared for the age of interactivity. This 
is only partly the result of unwise, restrictive, legislation 
which prohibits data vendors from stepping on each others' 
toes. Phone companies were not permitted to provide Internet 
services or to transfer video through their wires - and cable 
companies were not allowed to transmit phone calls. 
It is a question of time until these fossilized remains are 
removed by the almighty hand of the market. When this happens, 
the home centre is likely to look like this: 
A central computer attached to a big screen divided to 
windows. Television is broadcast on one window. A software 
application is running on another. This could be an 
application connected to the television program (deriving data 
from it, recording it, collating it with pertinent data it 
picks out of databases). It could be an independent 
application (a computer game). 
Updates from the New York Stock exchange flash at the corner 
of the screen and an icon blinks to signal the occurrence of a 
significant economic event. 
A click of the mouse (?) and the news flash is converted to a 
voice message. Another click and your broker is on the 
InternetPhone (possibly seen in a third window on the screen). 
You talk, you send him a fax containing instructions and you 
compare notes. The fax was printed on a word processing 
application which opened up in yet another window. 
Many believe that communication with the future generation of 
computers will be voice communication. This is difficult to 
believe. It is weird to talk to a machine (especially in the 
presence of other humans). We are seriously inhibited this 
way. Moreover, voice will interrupt other people's work or 
pleasure. It is also close to impossible to develop an 
efficient voice recognition software. Not to mention mishaps 
such as accidental activation. 
The Friendly Internet 
The Internet will not escape the processes experienced by all 
other media. 
It will become easy to operate, user-friendly, in professional 
parlance. 
It requires too much specialized information. It is not 
accessible to those who lack basic hardware and (Windows) 
software concepts. 
Alas, most of the population falls into the latter category. 
Only 30 million "Windows" operating systems were sold 
worldwide at the end of 1996. Even if this constitutes 20% of 
all the copies (the rest being pirated versions) - it still 
represents less than 3% of the population of the world. And 
this, needless to say, is the world's most popular software 
(following the DOS operating system). 
The Internet must rely on something completely different. It 
must have sophisticated, transparent-to-the-user search 
engines to guide to the cavernous chaotic libraries which will 
typify it. The search engines must include complex decision 
making algorithms. They must understand common languages and 
respond in mundane speech. They will be efficient and 
incredibly fast because they will form their own search 
strategy (supplanting the user's faulty use of syntax). 
These engines, replete with smart agents will refer the user 
to additional data, to cultural products which reflect the 
user's history of preferences (or pronounced preferences 
expressed in answers to feedback questionnaires). All the 
decisions and activities of the user will be stored in the 
memory of his search engine and assist it in designing its 
decision making trees. The engine will become an electronic 
friend, advise the user, even on professional matters. 



Cease-Fire 
The cessation of hostilities between the Internet and some 
off-the-shelf software applications heralds the commencement 
of the integration between the desktop computer and the Net. 
This is a small step for the user - and a big one for 
humanity. The animosity which prevailed until recently between 
the UNIX systems and the HTML language and between most of the 
standard applications (headed by the Word Processors) - has 
officially ended with the introduction of Office 97 which 
incorporates full HTML capabilities. With the Office 2000 
products, the distinctions between a web computing environment 
and a PC computing one - have all but vanished. Browsers can 
replace operating systems, word processors can browse, 
download and upload - the PC has finally been entirely 
absorbed by its offspring, the internet. 
The Portable Document Format (PDF) enables the user to work 
the Internet off-line. In other words: text files will be 
loaded to word processors and edited off-line. The same 
applies to other types of files (audio, video). 
Downloading time will be speeded up (today, it takes so long 
to download an audio or video file that, many times, it is 
impracticable). 
This is not a trivial matter. The ability to switch between 
on-line and off-line states and to continue the work, 
uninterrupted - this ability means the integration of the PC 
in the Internet. 
There are two competing views concerning the future of 
computer hardware and both of them acknowledge the importance 
of the Internet. 
Bill Gates - Microsoft's legendary boss - says that the PC 
will continue to advance and strengthen its processing and 
computing powers. The Internet will be just another tool 
available through telecommunications, rather than through the 
ownership of hard copies of software and data. The Internet is 
perceived to be a tremendous external database, available for 
processing by tomorrow's desktops. This view is lately being 
gradually reversed in view of the incredible vitality and 
powers of the Internet. 
Gates is converging on the worldview held by Sun Microsystems. 
The future desktop will be a terminal, albeit powerful and 
with considerable processing, computing and communications 
capabilities. The name of the game will be the Internet 
itself. The terminal will access Internet databases 
(containing raw or processed data) and satisfy its information 
needs. 
This terminal - equipped with languages the likes of Java - 
will get into libraries of software applications. It will make 
use of components of different applications as the needs 
arise. When finished using the component, the terminal will 
"return" it to the virtual "shelf" until the next time it is 
needed. 
This will minimize memory resources in the desktop. 
The truth, as always, is probably somewhere in the middle. 
Tomorrow's computer will be a home entertainment centre. No 
consumer will accept total dependence on telecommunications 
and on the Net. They will all ask for processing and computing 
powers at their fingertips, a-la Bill Gates. 
But tomorrow's computer will also function as a terminal, when 
needed: when data retrieving or even when using NON standard 
software applications. Why purchase rarely used, expensive 
applications - when they are available, for a fraction of the 
cost, on the Net? 
In other words: no consumer will subjugate his frequent word 
processing needs to the whims of the local phone company, or 
to those of the site operator. That is why every desktop is 
still likely to be include a hard (or optical)-disk-resident 
word processing software. But very few will by CAD-CAM, 
animation, graphics, or publishing software which they are 
likely to use infrequently. Instead, they will access these 
applications, which will be resident in the Net, use those 
parts that are needed. This is usage tailored to the client's 
needs. This is also the integration of a desktop (not of a 
terminal) with the Net. 
Decentralized Lack of Planning 
The course adopted by content creators (producers) in the last 
few years proves the maxim that it is easy to repeat mistakes 
and difficult to derive lessons from them. Content producers 
are constantly buying channels to transfer their contents. 
This is a mistake. A careful study of the history of 
successful media (e.g., television) points to a clear pattern: 
Content producers do not grant life-long exclusivity to any 
single channel. Especially not by buying into it. They prefer 
to contract for a limited time with content providers (their 
broadcast channels). They work with all of them, sometimes 
simultaneously. 
In the future, the same content will be sold on different 
sites or networks, at different times. Sometimes it will be 
found with a provider which is a combination of cable TV 
company and phone company - at other times, it will be found 
with a provider with expertise in computer networks. Much 
content will be created locally and distributed globally - and 
vice versa. The repackaging of branded contents will be the 
name of the game in both the media firms and the firms which 
control contents distribution (=the channels). 
No exclusivity pact will survive. Networks such as CompuServe 
are doomed and have been doomed since 1993. The approach of 
decentralized access, through numerous channels, to the same 
information - will prevail. 
The Transparent Language 
The Internet will become the next battlefield between have 
countries and have-not countries. It will be a cultural war 
zone (English against French, Japanese, Chinese, Russian and 
Spanish). It will be politically charged: those wishing to 
restrict the freedom of speech (authoritarian and dictatorial 
regimes, governments, conservative politicians) against pro-
speechers. It will become a new arena of warfare and an 
integral part of actual wars. 
Different peer groups, educational and income social-economic 
strata, ethnic, sexual preference groups - will all fight in 
the eternal fields of the Internet. 
Yet, two developments are likely to pacify the scene: 
Automatic translation applications (like Accent and the Alta 
Vista translation engines) will make every bit of information 
accessible to all. The lingual (and, by extension ethnic or 
national) source of the information will be disguised. A 
feeling of a global village will permeate the medium. Being 
ignorant of the English language will no longer hinder one's 
access to the Net. Equal opportunities. 
The second trend will be the new classification methods of 
contents on the Net together with the availability of chips 
intended to filter offensive information. Obscene material 
will not be available to tender souls. anti-Semitic sites will 
be blocked to Jews and communists will be spared Evil Empire 
speeches. Filtering will be usually done using extensive and 
adaptable lists of keywords or key phrases. 
This will lead to the formation of cultural Internet Ghettos - 
but it will also considerably reduce tensions and largely 
derail populist legislative efforts aimed at curbing or 
censoring free speech. 
Public Internet - Private Internet 
The day is not far when every user will be able to define his 
areas of interest, order of priorities, preferences and 
tastes. Special applications will scour the Net for him and 
retrieve the material befitting his requirements. This 
material will be organized in any manner prescribed. 
A private newspaper comes to mind. It will have a circulation 
of one copy - the user's. It will borrow its contents from a 
few hundreds of databases and electronic versions of 
newspapers on the Net. Its headlines will reflect the main 
areas of interest of its sole subscriber. The private paper 
will contain hyperlinks to other sites in the Internet: to 
reference material, to additional information on the same 
subject. It will contain text, but also graphics, audio, video 
and photographs. It will be interactive and editable with the 
push of a button. 
Another idea: the intelligent archive. 
The user will accumulate information, derived from a variety 
of sources in an archive maintained for him on the Net. It 
will not be a classical "dead" archive. It will be active. A 
special application will search the Net daily and update the 
archive. It will contain hyperlinks to sites, to additional 
information on the Net and to alternative sources of 
information. It will have a "History" function which will 
teach the archive about the preferences and priorities of the 
user. 
The software will recommend new sites to him and subjects 
similar to his history. It will alert him to movies, TV shows 
and new musical releases - all within his cultural sphere. If 
convinced to purchase - the software will order the wares from 
the Net. It will then let him listen to the music, see the 
movie, or read the text. 



The internet will become a place of unceasing stimuli, of 
internal order and organization and of friendliness in the 
sense of personally rewarding acquaintance. Such an archive 
will be a veritable friend. It will alert the user to 
interesting news, leave messages and food for thought in his 
e-mail (or v-mail). It will send the user a fax if not 
responded to within a reasonable time. It will issue reports 
every morning. 
This, naturally, is only a private case of the archival 
potential of the Net. 
A network connecting more than 16.3 million computers (end 
1996) is also the biggest collective memory effort in history 
after the Library of Alexandria. The Internet possesses the 
combined power of all its constituents. Search engines are, 
therefore, bound to be replaced by intelligent archives which 
will form universal archives, which will store all the paths 
to the results of searches plus millions of recommended 
searches. 
Compare this to a newspaper: it is much easier to store back 
issues of a paper in the Internet than physically. Obviously, 
it is much easier to search and the amortization of such a 
copy is annulled. Such an archive will let the user search by 
word, by key phrase, by contents, search the bibliography and 
hop to other parts of the archive or to other territories in 
the Internet using hyperlinks. 
Money, Again 
We have already mentioned SET, the safety standard. This will 
facilitate credit card transactions over the Net. These are 
safe transactions even today - but there an ingrained interest 
to say otherwise. Newspapers are afraid that advertising 
budgets will migrate to the Web. Television harbours the same 
fears. More commerce on the Net - means more advertising 
dollars diverted from established media. Too many feel unhappy 
when confronted with this inevitability. They spread lies 
which feed off the ignorance about how safe paying with credit 
cards on the Net is. Safety standards will terminate this 
propaganda and transform the Internet into a commercial 
medium. 
Users will be able to buy and sell goods and services on the 
Net and get them by post. Certain things will be directly 
downloaded (software, e-books). Many banking transactions and 
EDI operations will be conducted through bank-clients 
intranets. All stock and commodity exchanges will be 
accessible and the role of brokers will be minimized. Foreign 
exchange will be easily tradable and transferable. Initial 
Public Offerings of shares, day trading of stocks and other 
activities traditionally connected with physical ("pit") 
capital markets will become a predominant feature of the 
internet. The day is not far that the likes of Merill Lynch 
will be offering full services (including advisory services) 
through the internet. The first steps towards electronic 
trading of shares (with discounted fees) have already been 
taken in mid 1999. Home banking, private newspapers, 
subscriptions to cultural events, tourism packages and airline 
tickets - are all candidates for Net-Trading. 
The Internet is here to stay. 



Commercially, it would be an extreme strategic error to ignore 
it. A lot of money will flow through it. A lot more people 
will be connected to it. A lot of information will be stored 
on it. 
It is worth being there. 
Published by "PC World" in Tel-Aviv on April 1996.  
Partially Revised: 7/00. 



 
Appendix - Ethics and the Internet 
  
The "Internet" is a very misleading term. It's like saying 
"print". Professional articles are "print" - and so are the 
sleaziest porno brochures. 
So, first, I think it would be useful to make a distinction 
between two broad categories: 
Content-related 
or 
Content-driven and Interaction-driven 
Most content driven sites maintain reasonable ethical 
standards, roughly comparable to the "real" or "non-virtual" 
media. This is because many of these sites were established by 
businesses with a "real" dimension to start with (Walt Disney, 
The Economist, etc.). These sites (at least the institutional 
ones) maintain standards of privacy, veracity, cross-checking 
of information, etc. 
Personal home pages would be a sub-category of content-driven 
sites. These cannot be seriously considered "media". They are 
representatives of the new phenomenon of extreme 
narrowcasting. They do not adhere to any ethical standards, 
with the exception of those upheld by their owners'. 
The interaction orientated sites and activities can, in turn, 
be divided to E-commerce sites (such as Amazon) which adhere 
to commercial law and to commercial ethics and to interactive 
sites. 
The latter - discussion lists, mailing lists and so on - are a 
hotbed of unethical, verbally aggressive, hostile behaviour. A 
special vocabulary developed to discuss these phenomena 
("flaming", "mail bombing" etc.). 
To summarize: 
Where the aim is to provide consumers with another venue for 
the dissemination of information or to sell products or 
services to them the standards of ethics maintained reflect 
those upheld outside the realm of the internet. Additionally, 
codified morals, the commercial law is adhered to. 
Where the aim is interaction or the dissemination of the 
personal opinions and views of site-owners - ethical standards 
are in the process of becoming. A rough set of guidelines 
coalesced into the "netiquette". It is a set of rules of 
peaceful co-existence intended to prevent flame wars and the 
eruption of interpersonal verbal abuse. Since it lacks 
effective means of enforcement - it is very often violated and 
constitutes an expression of goodwill, rather than an obliging 
code. 


The Internet in the Countries in Transition
By: Sam Vaknin

Though the countries in transition are far from being an 
homogeneous lot, there are a few denominators common to their 
Internet experience hitherto: 
1. Internet Invasion 
The penetration of the Internet in the countries in transition 
varies from country to country - but is still very low even by 
European standards, not to mention by American ones. This had 
to do with the lack of infrastructure, the prohibitive cost of 
services, an extortionist pricing structure, computer 
illiteracy and luddism (computer phobia). Societies in the 
countries in transition are inert (and most of them, 
conservative or traditionalist) - following years of central 
mis-planning. The Internet (and computers) are perceived by 
many as threatening - mainly because they are part of a 
technological upheaval which makes people redundant. 
2. The Rumour Mill 
All manner of instant messaging - mainly the earlier versions 
of IRC - played an important role in enhancing social cohesion 
and exchanging uncensored information. As in other parts of 
the world - the Internet was first used to communicate: IRC, 
MIRC e-mail and e-mail fora were - and to a large extent, are 
- all the rage. 
The IRC was (and is) used mainly to exchange political views 
and news and to engage in inter-personal interactions. The 
media in countries in transition is notoriously unreliable. 
Decades of official indoctrination and propaganda left people 
reading between (real or imaginary) lines. Rumours and gossip 
always substituted for news and the Internet was well suited 
to become a prime channel of dissemination of conspiracy 
theories, malicious libel, hearsay and eyewitness accounts. 
Instant messaging services also led to an increase in the 
number (though not necessarily in the quality) of interactions 
between the users - from dating to the provision of services, 
the Internet was enthusiastically adopted by a generation of 
alienated youth, isolated from the world by official doctrine 
and from each other by paranoia fostered by the political 
regime. The Internet exposed its users to the west, to other 
models of existence where trust and collaboration play a major 
role. It increase the quantity of interaction between them. It 
fostered a sense of identity and community. The Internet is 
not ubiquitous in the countries in transition and, therefore, 
its impact is very limited. It had no discernible effect on 
how governments work in this region. Even in the USA it is 
just starting to effect political processes and be integrated 
in them. 



The Internet encouraged entrepreneurship and aspirations of 
social mobility. Very much like mobile telephony - which 
allowed the countries in transition to skip massive 
investments in outdated technologies - the Internet was 
perceived to be a shortcut to prosperity. Its decentralized 
channels of distribution, global penetration, "rags to riches" 
ethos and dizzying rate of innovation - attracted the young 
and creative. Many decided to become software developers and 
establish local version of "Silicon Valley" or the flourishing 
software industry in India. Anti virus software was developed 
in Russia, web design services in former Yugoslavia, e-media 
in the Czech Republic and so on. But this is the reserve of a 
minuscule part of society. E-commerce, for instance, is a long 
way off (though m-commerce might be sooner in countries like 
the Czech Republic or the Baltic). 
E-commerce is the natural culmination of a process. You need 
to have a rich computer infrastructure, a functioning 
telecommunications network, cheap access to the Internet, 
computer literacy, inability to postpone gratification, a 
philosophy of consumerism and, finally, a modicum of trust 
between the players in the economy. The countries in 
transition lack all of the above. Most of them are not even 
aware that the Internet exists and what it can do for them. 
Penetration rates, number of computers per household, number 
of phone lines per household, the reliability of the 
telecommunications infrastructure and the number of Internet 
users at home (and at work)- are all dismally low. On the 
other hand, the cost of accessing the net is still 
prohibitively high. It would be a wild exaggeration to call 
the budding Internet enterprises in the countries in 
transition - "industries". There are isolated cases of 
success, that's all. They sprang in response to local demand, 
expanded internationally on rare occasions and, on the whole 
remained pretty confined to their locale. There was no 
agreement between countries and entrepreneurs who will develop 
what. It was purely haphazard. 
3. The Great Equalizer 
Very early on, the denizens of the countries in transition 
have caught on to the "great equalizer" effects of the Net. 
They used it to vent their frustrations and aggression, to 
conduct cyber-warfare, to unleash an explosion of visual 
creativity and to engage in deconstructive discourse. 
By great equalizer - I meant equalizer with the rich, 
developed countries. See the article I quoted above. The 
citizens of the countries in transition are frustrated by 
their inability to catch up with the affluence and prosperity 
of the West. They feel inferior, neglected, looked down upon, 
dictated to and, in general, put down. The Internet is 
perceived as something which can restore the balance. Only, of 
course, it cannot. It is still a rich people's medium. 
President Clinton points out the Digital Divide within America 
- such a divide exists to a much larger extent and with more 
venomous effects between the developed and developing world. 
the Internet has done nothing to bridge this gap - on the 
contrary: It enhanced the productivity and economic growth 
(this is known as "The New Economy") of rich countries (mainly 
the States) and left the have-nots in the dust. 



4. Intellectual Property 
The concept of intellectual property - foreign to the global 
Internet culture to start with - became an emblem of Western 
hegemony and monopolistic practices. Violating copyright, 
software piracy and hacking became both status symbols and a 
political declaration of sorts. But the rapid dissemination of 
programs and information (for instance, illicit copies of 
reference works) served to level the playing field. 
Piracy of material is quite prevalent in the countries in 
transition. The countries in transition are the second capital 
of piracy (after Asia). Software, films, even books - are 
copied and distributed quite freely and openly. There are 
street vendors who deal in the counterfeit products - but most 
of it is sold through stores and OEMs. 
I think that intellectual property will go the way the 
pharmaceutical industry did: Instead of fighting windmills - 
owners and distributors of intellectual property will join the 
trend. They are likely to team up with sponsors which will 
subsidize the price of intellectual property in order to make 
it affordable to the denizens of poor countries. Such sponsors 
could be either multi-lateral institutions (such as the World 
Bank) - or charities and donors. 


Leapfrogging Transition 
Technology and Development in Post-Communist Europe
Also published by United Press International (UPI) 

In many countries in transition cellular phones are more 
ubiquitous than the fixed-line kind. Teledensity is 
vanishingly low throughout swathes of Central and Eastern 
Europe (CEE). Broadband and e-commerce are distant rumors 
(ISDN is available in theory but not so in practice - DSL and 
ADSL are not available at all). Rare phone lines - especially 
in urban centers - are still being multiplexed and shared by 
4-8 subscribers, greatly reducing both quality and usability. 
Terrestrial television competes ferociously with satellite TV, 
though cable penetration is low. Internet access is 
prohibitively expensive and intermittent. Many technologies 
rely on network effects (i.e., a critical mass of users). CEE 
is far from reaching this elusive point.
When communism imploded in 1989, pundits were quick to spot 
the silver lining. The countries in transition, they said, 
could now leapfrog whole stages of development by adopting 
novel technologies and through them the expensive Western 
research they embody. The East can learn from the West's 
mistakes and, by avoiding them, achieve a competitive edge.
In his seminal book, "Leapfrogging Development - The Political 
Economy of Telecommunications Restructuring", J.P. Singh, 
examined the acceleration of development through the adoption 
of ready-made, off the shelf, technologies. His melancholy 
conclusion was that development preferences are the outcomes 
of an intricate inter-play between sectoral pressure groups 
and coalitions of interest groups - and not the result of 
progress ex machina. He distinguished three types of states - 
catalytic, near-catalytic, and dysfunctional. Though he deals 
exclusively with Asia and Latin America, his typology is 
applicable to post-Communist Europe.
I. An Overview
The Central and East European market will double itself (to 
$17 billion) by 2003, says IDC. Pyramid Research predicts a 
$60 billion communications market by 2005. "Information 
Society", ICT (Information and Communication Technologies), 
"leapfrogging", and "better online than in line" are buzzwords 
and slogans oft-used throughout the region. A horde of NGO's - 
local and international - collaborate with domestic government 
and local authorities, with foreign governments, 
multinationals, and international organizations to make the 
dream of a digital Europe come true. 
Russia pledged to attract $33 billion in investments in its 
telecommunications infrastructure and services by the year 
2010 (the "Electronic Russia" initiative). The US Commercial 
Service, in the American Embassy in Moscow, predicts an annual 
growth rate of the Russian ICT sector of 15-20 percent through 
2003. Conferences abound (an important one regarding municipal 
collaboration in constructing an information highway is to be 
held in the Czech Republic on March 26-27). 
Even devastated Armenia succeeded to export $20 million worth 
of IT goods in 2001 (its IT sector has grown by 30% last 
year). It hosts branches of Silicon Valley household names 
such as Credence, HPL, and Virage Logic. More than 4000 
professionals are employed in 200 companies. Of 60 software 
development outfits - 26 were founded with American capital. 
LEDA, a prominent local IT firm, finances IT programs at the 
Armenian State Engineering University.
All EU candidates strive to get incorporated in existing 
European networks (such as ELANET, Telecities, IDA, and ERISA) 
and new, candidate-only, initiatives (such as eEurope+). The 
EU has applied its "universal (i.e., also affordable) service" 
rule to Internet access. EU members adopted a variety of 
measures to increase Internet awareness and usage. Portugal, 
for instance, granted individuals with tax incentives coupled 
with free e-mail accounts and Web hosting services to 
encourage them to purchase PC's. The Dutch established public 
computer literacy centers for the disenfranchised (e.g., the 
unemployed) and provided them with discounted and subsidized 
hardware and connection time. 
In one of its more grandiose moments, the heads of governments 
of the EU countries have decided in Lisbon (2000) that "each 
citizen should have access to the Internet and the whole 
European Union should become computer-literate", in the words 
of the Czech conference organizers. 
This is an ambitious undertaking not only because Europe in 
general is behind the USA where Internet matters (with the 
exception of wireless Internet) are concerned - but because 
the countries which used to be behind the Iron Curtain, now 
lurch in the Digital Divide. 
According to Vasile Baltac from the Information Technology and 
Communications Association of Romania ("The Balkan and Eastern 
Europe - Digital Divide or Digital Opportunity"), Romania has 
invested $25 per capita in ICT in 1999 (compared to Greece's 
$567 and the EU's average of $1215). There were only 2.5 
Internet users per 1000 inhabitants in Romania and Bulgaria - 
compared to 56.4 in Westward-looking Slovenia.
New technologies are used mostly by the elites in CEE (as 
pointed out by Zassourski and Vartanova in "Transformation in 
the Context of Transition") - and perhaps advertently so. 
Still, Baltac fingers the managerial class as the main 
obstacle to leapfrogging (i.e., the rapid dissemination and 
assimilation of advanced technologies). They pay lip service 
to modernization but feel threatened and repelled by it. On 
the positive side, Baltac notes the annual yield of qualified 
professionals (who mostly find work in the West) and the 
emergence of telework and e-commerce. The technological vacuum 
makes the CEE countries receptive to state of the art 
technologies. GSM penetration in Romania surpassed the level 
of fixed line coverage in 1989. The number of cable TV 
subscribers in the region is projected to double (to 20 
million) by 2005. 
But the true picture is often obscured by anecdotal evidence, 
wishful thinking, phobias (e.g., the West European fear of 
mass migration from East Europe), lack of reliable statistics, 
and absence of qualified analysts and investment bankers. 
Factors like hostile terrain and climate, cross-subsidies, 
lack of real competition, corruption, red tape, moribund 
financial systems, archaic legal ones, dearth of credit card 
holders, urban-rural gaps, and English language illiteracy - 
rarely appear in neat, colorful, presentations.
Pyramid Research is bearish on broadband. "Internet access is 
and will remain for the foreseeable future a predominantly 
narrowband, dial-up affair, even in the most advanced 
countries (in Central Europe)". This despite plans by regional 
operators to offer DSL, FWA (Fixed Wireless Access), cable TV 
and leased-line broadband access (already offered in the Czech 
Republic by cable networks) and despite a regulatory welcome 
in all three CE candidates (Hungary, Poland, and the Czech 
Republic). 
Luckily, mobile telephony - the other pillar of the 
leapfrogging theory - is getting increasingly concentrated in 
the hands of fewer operators (though at least 3 per every 
major market). Pyramid projects that by 2006, 94 percent of 
Russia's cellular phone market will be in the hands of the 
five leading providers (compared to 85 percent at the end of 
2001). Mobile penetration will increase (to c. 10 percent) and 
prepaid customers will account for the vast majority of users. 
Revenues from cellular networks exceed revenues from fixed 
line networks in certain markets. SMS is booming. Second and 
third mobile operator licenses are tendered by all cash 
strapped governments in the region (though a Polish attempt to 
sell an UMTS license ended in a fiasco). Poland introduced a 
wireless local loop service. Macedonia just handed a second 
mobile operator license to the Greek OTE.
"By the end of 2005, the total number of mobile subscribers in 
CEE will exceed 50 million (compared to 30 million by end-
2001) and mobile Internet accounts will constitute 
approximately 21 percent of total mobile accounts", projects 
Pyramid. The Czech Republic will have 78 mobile users per 100 
population - and Hungary 66. In a second tier of countries - 
the likes of Bulgaria, Romania, Ukraine, and Russia - a mobile 
phone will remain a luxury and a status symbol.
Hitherto domestic operators - from the Greek OTE to the 
Russian MTS - are becoming regional. Multinationals, such as 
the British Vodafone and the French Orange - have entered the 
regional fray. Some CEE markets are as saturated (and 
customers as savvy and demanding) as many advanced Western 
European ones.  A host of value added services (VAS) is thrust 
upon the - sometimes reluctant - users, leading naturally to 
WAP (recently introduced throughout much of CEE), 2.5G, and 3G 
(wi-fi or wireless Internet) services.
Moreover, Pyramid sees an intriguing opportunity in VoIP 
(Voice over IP) telephony. It says:
"As the incumbents in the CEE markets continue to dominate 
long-distance circuit-switched telephony, VoIP offers a unique 
opportunity for new operators to gain a foothold in this 
traditional monopolistic stronghold."
Internet Telephony Service Providers (ITSP's) have sprung up 
all over the region (an Israeli firm is now planning to offer 
VoIP services in Macedonia, Kosovo, and Albania). Even 
incumbents have been offering VoIP - as early as 1998 in the 
Czech Republic. In his keynote address to The Economist CEE 
Telecommunications Conference, in December 2001, Ofer Gneezy, 
President and CEO of iBasis (a global ITSP), cited industry 
analysts projecting VoIP average annual growth rates in CEE of 
80 percent through 2006.



This, coupled with a growing number of Internet users and 
access providers (spurred on by telecoms liberalization and 
growing incomes), may revolutionize the landscape in the next 
5-10 years. Pyramid expects annual Internet adoption growth 
rates of 40 percent through 2005 (that's 30,000 new users a 
day!). Internet related revenues will reach $10 billion by 
2005 (five times today's $1.8 billion - but only one seventh 
the Internet market in Western Europe). 
Internet penetration in Central Europe will reach 15 percent 
in 2005 (from 4 percent today and 3 percent in Russia) - and 
40 percent in Western Europe (compared to 18 percent today). 
Mobile Internet accounts will constitute one third of the 
total in CEE - c. 20 million users. Harald Gruber of the 
European Investment Bank is even more optimistic, saying 
("Competition and Innovation: The Diffusion of 
Telecommunications in CEE", March 2000): "About 20 percent of 
the population will adopt mobile telecommunications".
II. The Future
Leapfrogging is not a linear function of the ubiquity of 
hardware and software. Though not a homogeneous lot, some 
lessons common to all countries in transition are already 
evident. 
Technology is a social phenomenon with social implications. It 
fosters entrepreneurship and social mobility. By allowing the 
countries in transition to skip massive investments in 
outdated technologies - the cellular phone, the Internet, 
cable TV, and the satellite came to be perceived as shortcuts 
to prosperity, the generators of the dual ethoses of "rags to 
riches", and "creative destruction" (dizzying, constant, and 
disruptive innovation). They are the future, a youthful 
promise, and a landscape of opportunities.
Software developers in CEE countries tried to establish local 
versions of "Silicon Valley", or the flourishing software 
industry in India. Russian entrepreneurs developed anti virus 
software, Yugoslavs offered web design services, electronic 
media flourished in the Czech Republic and so on. But, as hard 
reality set in, most of these talents left for Western Europe, 
the USA, Canada, and Australia - where technology firms 
snatched them eagerly. Central and Eastern Europe is a major 
net exporter of engineers, programmers, systems analysts, Web 
designers, and concepts analysts.
Internet penetration in these countries  - even in the most 
wired - is still very low by European standards, let alone 
American ones. The trauma of communism left them with decrepit 
and rarefied infrastructure, a prohibitive, extortionist, and 
skewed cost structure, computer illiteracy, inefficient 
competition, insufficient investment capital, and entrenched 
luddism (e.g., computer phobia). Foreign operators often 
exacerbate the situation. ArmenTel, the Greek owned monopoly 
in Armenia, keeps Internet access costs prohibitively high, 
ignoring court actions by the government and loud complaints 
by disgruntled customers.
The Center for Democracy and Technology (in its report 
"Bridging the Digital Divide: Internet Access in Central and 
Eastern Europe") says that, as contrasted with India (or 
Malaysia), the countries of the CEE did not invest in 
computerizing their schools, public libraries, and higher 
education institutions, or in subsidizing private computer-
training colleges. 
More crucially and less reversibly, decades of central (mis-
)planning rendered the societies of Central and Eastern Europe 
inert and dependent, apart from their traditional 
conservatism. Many - especially older mid- and high-level 
managers and engineers - feel threatened by technology. 
Technology makes people redundant. 
To a few open minded (i.e., foreign owned) firms, computer 
networking stands for decentralized channels of distribution 
and marketing as well as potential global penetration. But 
even there, only a minuscule number of businesses took 
advantage of e-commerce (though the countries of Central 
Europe and the Baltic may be the global pioneers of m-commerce 
due to their wireless networks). 
E-commerce is leapfrogging's litmus test because it represents 
the culmination and confluence of hardware, software, and 
process engineering. To have e-commerce, a country needs rich 
computer infrastructure, a functioning telecommunications 
network, and cheap access to the Internet. Its citizens need 
to be reasonably computer literate, possess both a consumerist 
mentality (e.g., inability to postpone gratification), and a 
modicum of trust between the players in the economy - and hold 
credit cards. 
Alas, the countries in transition lack all of the above to 
varying degrees. The Economist Intelligence Unit ranked Russia 
42nd (out of 60 countries) in its year 2000 "e-readiness 
survey". Other CEE countries fared little better.
Penetration and coverage rates (the number of computers and 
phone lines per household), network reliability, and the 
absolute number of Internet users - are all dismally low. 
Access fees are prohibitively high. Budding Internet 
enterprises in the countries in transition are happy 
exceptions that prove the depressing rule. They usually 
respond to erratic local demand. Few have expanded 
internationally. Even fewer engage in research and 
development. 
Technology was supposed to be the great equalizer (with the 
rich, developed countries). It did not deliver on this 
promise. Unable to catch up with Western affluence and 
prosperity, the denizens of CEE are frustrated. They feel 
inferior, neglected, looked down upon, dictated to, and, in 
general, put down. New, ever-cheaper, technologies, thought 
the locals, would surely restore the rightful balance between 
impoverished East and filthy rich West. But the Internet - and 
even technologies such as cellular telephony - belong to those 
who can effectively deploy them (i.e., consumers in developed, 
infrastructure-rich, countries). 
The news get worse.
The Internet is gradually permeated by commercial interests 
and going wireless. This convergence of content and business 
interests - means less access to the underprivileged.  The 
digital divide is growing by the day.  New technologies have 
done little to bridge this gap - on the contrary: they 
enhanced the productivity and economic growth (this is known 
as "The New Economy") of rich countries (mainly the United 
States) and left the have-nots in the dust. 



The countries in transition also lack the proper legislative 
and law enforcement infrastructure (backed by the right 
cultural background). Property rights, contracts, intellectual 
property - are all new, often indigestible, concepts, emblems 
of Western hegemony and monopolistic practices. Widespread 
copyright violation, software piracy, and hacking are both 
status symbols and political declarations of sorts. 
Admittedly, the dissemination of illicit intellectual products 
may have served to level the playing field. But now it is 
hindering entrepreneurship and holding back development.
After Asia, the countries in transition are the second largest 
centre of piracy. Software, films, even books - are copied and 
distributed quite freely and openly. There are street vendors 
who deal in the counterfeit products - but most of it is sold 
through stores and OEMs. This despite massive efforts (e.g., 
in Russia, Bulgaria, Ukraine, and, lately, in Macedonia) by 
software developers, licensed film libraries, and distributors 
- to fight these phenomena. 
Intellectual property may go the way the pharmaceutical 
industry has. Content owners and distributors may team up with 
sponsors (multilateral institutions, private charities and 
donors). The latter will subsidize intellectual property and, 
thus, make it affordable to the denizens of poor countries. 
This is already happening in scholarly publishing. 
This is very promising. But it far from leapfrogging 
development. In hindsight, leapfrogging may have been nothing 
but another of those intellectual fads whose time has gone 
before it ever came.
 


The Selfish Net - The Semantic Web
By: Sam Vaknin
A decade after the invention of the World Wide Web, Tim 
Berners-Lee is promoting the "Semantic Web". The Internet 
hitherto is a repository of digital content. It has a 
rudimentary inventory system and very crude data location 
services. As a sad result, most of the content is invisible 
and inaccessible. Moreover, the Internet manipulates strings 
of symbols, not logical or semantic propositions. In other 
words, the Net compares values but does not know the meaning 
of the values it thus manipulates. It is unable to interpret 
strings, to infer new facts, to deduce, induce, derive, or 
otherwise comprehend what it is doing. In short, it does not 
understand language. Run an ambiguous term by any search 
engine and these shortcomings become painfully evident. This 
lack of understanding of the semantic foundations of its raw 
material (data, information) prevent applications and 
databases from sharing resources and feeding each other. The 
Internet is discrete, not continuous. It resembles an 
archipelago, with users hopping from island to island in a 
frantic search for relevancy.
Even visionaries like Berners-Lee do not contemplate an 
"intelligent Web". They are simply proposing to let users, 
content creators,  and web developers assign descriptive meta-
tags ("name of hotel") to fields, or to strings of symbols 
("Hilton"). These meta-tags (arranged in semantic and 
relational "ontologies" - lists of metatags, their meanings 
and how they relate to each other) will be read by various 
applications and allow them to process the associated strings 
of symbols correctly (place the word "Hilton" in your address 
book under "hotels"). This will make information retrieval 
more efficient and reliable and the information retrieved is 
bound to be more relevant and amenable to higher level 
processing (statistics, the development of heuristic rules, 
etc.). The shift is from HTML (whose tags are concerned with 
visual appearances and content indexing) to languages such as 
the DARPA Agent Markup Language, OIL (Ontology Inference Layer 
or Ontology Interchange Language), or even XML (whose tags are 
concerned with content taxonomy, document structure, and 
semantics). This would bring the Internet closer to the 
classic library card catalogue.
Even in its current, pre-semantic, hyperlink-dependent, phase, 
the Internet brings to mind Richard Dawkins' seminal work "The 
Selfish Gene" (OUP, 1976). This would be doubly true for the 
Semantic Web.
Dawkins suggested to generalize the principle of natural 
selection to a law of the survival of the stable. "A stable 
thing is a collection of atoms which is permanent enough or 
common enough to deserve a name". He then proceeded to 
describe the emergence of "Replicators" - molecules which 
created copies of themselves. The Replicators that survived in 
the competition for scarce raw materials were characterized by 
high longevity, fecundity, and copying-fidelity. Replicators 
(now known as "genes") constructed "survival machines" 
(organisms) to shield them from the vagaries of an ever-
harsher environment.
This is very reminiscent of the Internet. The "stable things" 
are HTML coded web pages. They are replicators - they create 
copies of themselves every time their "web address" (URL) is 
clicked. The HTML coding of a web page can be thought of as 
"genetic material". It contains all the information needed to 
reproduce the page. And, exactly as in nature, the higher the 
longevity, fecundity (measured in links to the web page from 
other web sites), and copying-fidelity of the HTML code - the 
higher its chances to survive (as a web page).
Replicator molecules (DNA) and replicator HTML have one thing 
in common - they are both packaged information. In the 
appropriate context (the right biochemical "soup" in the case 
of DNA, the right software application in the case of HTML 
code) - this information generates a "survival machine" 
(organism, or a web page). 
The Semantic Web will only increase the longevity, fecundity, 
and copying-fidelity or the underlying code (in this case, OIL 
or XML instead of HTML). By facilitating many more 
interactions with many other web pages and databases - the 
underlying "replicator" code will ensure the "survival" of 
"its" web page (=its survival machine). In this analogy, the 
web page's "DNA" (its OIL or XML code) contains "single genes" 
(semantic meta-tags). The whole process of life is the 
unfolding of a kind of Semantic Web.
In a prophetic paragraph, Dawkins described the Internet:
"The first thing to grasp about a modern replicator is that it 
is highly gregarious. A survival machine is a vehicle 
containing not just one gene but many thousands. The 
manufacture of a body is a cooperative venture of such 
intricacy that it is almost impossible to disentangle the 
contribution of one gene from that of another. A given gene 
will have many different effects on quite different parts of 
the body. A given part of the body will be influenced by many 
genes and the effect of any one gene depends on interaction 
with many others...In terms of the analogy, any given page of 
the plans makes reference to many different parts of the 
building; and each page makes sense only in terms of cross-
reference to numerous other pages"
What Dawkins neglected in his important work is the concept of 
the Network. People congregate in cities, mate, and reproduce, 
thus providing genes with new "survival machines". But Dawkins 
himself suggested that the new Replicator is the "meme" - an 
idea, belief, technique, technology, work of art, or bit of 
information. Memes use human brains as "survival machines" and 
they hop from brain to brain and across time and space 
("communications") in the process of cultural (as distinct 
from biological) evolution. The Internet is a latter day meme-
hopping playground. But, more importantly, it is a Network. 
Genes move from one container to another through a linear, 
serial, tedious process which involves prolonged periods of 
one on one gene shuffling ("sex") and gestation. Memes use 
networks. Their propagation is, therefore, parallel, fast, and 
all-pervasive. The Internet is a manifestation of the growing 
predominance of memes over genes. And the Semantic Web may be 
to the Internet what Artificial Intelligence is to classic 
computing. We may be on the threshold of a self-aware Web.


END OF THE PROJECT GUTENBERG EBOOK, E-BOOKS AND E-PUBLISHING
